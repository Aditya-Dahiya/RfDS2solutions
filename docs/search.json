[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Solutions: R for Data Science (2e)",
    "section": "",
    "text": "Author: Aditya Dahiya\nThis website displays the solutions for the exercises in the book R for Data Science, 2nd Edition.\n."
  },
  {
    "objectID": "index.html#solutions-for-the-book-r-for-data-science-2nd-edition",
    "href": "index.html#solutions-for-the-book-r-for-data-science-2nd-edition",
    "title": "Solutions: R for Data Science (2e)",
    "section": "",
    "text": "Author: Aditya Dahiya\nThis website displays the solutions for the exercises in the book R for Data Science, 2nd Edition.\n."
  },
  {
    "objectID": "Chapter29.html",
    "href": "Chapter29.html",
    "title": "Chapter 29",
    "section": "",
    "text": "Create a new Quarto document using File &gt; New File &gt; Quarto Document. Read the instructions. Practice running the chunks individually. Then render the document by clicking the appropriate button and then by using the appropriate keyboard short cut. Verify that you can modify the code, re-run it, and see modified output.\n\n\nThe document has been created. Some of the chunks are as follows:\n\n\nlibrary(tidyverse)\n\n\ndiamonds |&gt; \n  filter(carat &gt; 2) |&gt;\n  ggplot(mapping = aes(x=carat, y=price,\n                       color = color)) +\n  geom_point(alpha = 0.5) + \n  #scale_y_continuous(trans = \"log\") +\n  geom_smooth(se=FALSE) + \n  scale_color_brewer(palette = 2)\n\n\n\n\n\nVerifying that the code can be modified and re-run:–\n\n\ndiamonds |&gt; \n  filter(carat &gt; 2) |&gt;\n  filter (carat &lt; 3) |&gt;\n  filter (price &gt; 10000) |&gt;\n  ggplot(mapping = aes(x=carat, y=price,\n                       color = color)) +\n  geom_point(alpha = 0.3) + \n  scale_y_continuous(trans = \"log\") +\n  geom_smooth(se=FALSE) + \n  scale_color_brewer(palette = 4) +\n  theme_minimal() +\n  labs(title = \"Plot of relation between Carat and Price\",\n       subtitle = \"For different colours of diamonds\",\n       y = \"Price (in $)\", x = \"Carat\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nCreate one new Quarto document for each of the three built-in formats: HTML, PDF and Word. Render each of the three documents. How do the outputs differ? How do the inputs differ? (You may need to install LaTeX in order to build the PDF output — RStudio will prompt you if this is necessary.)\n\nThe outputs are different in the following ways:--\n\nThe output text and code are same, but rendering to PDF takes more time than Word. HTML rendering seems to be the fastest.\nPDF and MS Word cannot incorporate interactive graphics, while HTML offers interactivity.\nThe size of the PDF document is the largest, followed by MS Word document, while the HTML uses the least disk space."
  },
  {
    "objectID": "Chapter29.html#nd-level-header",
    "href": "Chapter29.html#nd-level-header",
    "title": "Chapter 29",
    "section": "2nd Level Header",
    "text": "2nd Level Header\n\n3rd Level Header"
  },
  {
    "objectID": "Chapter29.html#lists",
    "href": "Chapter29.html#lists",
    "title": "Chapter 29",
    "section": "Lists",
    "text": "Lists\n\nBulleted list item 1\nItem 2\n\nItem 2a\nItem 2b\n\n\n\nNumbered list item 1\nItem 2. The numbers are incremented automatically in the output.\n\n\nLinks and Images\nhttps://example.com/\nlinked phrases\n\n\n\nCredits: Nick Tierney's (mostly) rstats blog\n\n\nTables\n\nMy First Table in Quarto\n\n\nFirst Header\nSecond Header\n\n\n\n\nContent Cell 1.1\nContent Cell 2.1\n\n\nContent Cell 1.2\nContent Cell 2.2\n\n\nContent Cell 1.3\nContent Cell 2.3\n\n\n\n\nUsing the visual editor, insert a code chunk using the Insert menu and then the insert anything tool.\nHere, I am inserting a code chunk using simple the “/” key, and then selecting R-code option:--\n\nprint(\"Hello World\")\n\n[1] \"Hello World\"\n\n\nUsing the visual editor, figure out how to:\n\nAdd a footnote.\nYou can add a foot note by selecting the menu Insert –&gt; Footnote; or, by using Ctrl+Shift+7. Here is an example.1\nAdd a horizontal rule.\nYou can add a foot note by selecting the menu Insert –&gt; Horizontal Rule. Here is an example:--\n\nAdd a block quote.\nYou can add a foot note by selecting the menu Format –&gt; Block quote. Here is how we add a block quote:-\n\nA block quote is a long quote formatted as a separate “block” of text. Instead of using quotation marks, you place the quote on a new line, and indent the entire quote to mark it apart from your own words2\n\n\nIn the visual editor, go to Insert &gt; Citation and insert a citation to the paper titled Welcome to the Tidyverse using its DOI (digital object identifier), which is 10.21105/joss.01686. Render the document and observe how the reference shows up in the document. What change do you observe in the YAML of your document?\n\nLet us first add some text from the paper, so that we can use a citation:---\n\nAt a high level, the tidyverse is a language for solving data science challenges with R code. Its primary goal is to facilitate a conversation between a human and a computer about data. Less abstractly, the tidyverse is a collection of R packages that share a high-level design philosophy and low-level grammar and data structures, so that learning one package makes it easier to learn the next. (Wickham et al. 2019)\n\nOnce we render the document, the citation shows up in the very end of the HTML webpage rendered. It is displayed just above the footnotes. The YAML header of the document, when viewed in the “Source” displays an additional line bibliography: references.bib."
  },
  {
    "objectID": "Chapter29.html#footnotes",
    "href": "Chapter29.html#footnotes",
    "title": "Chapter 29",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is a sample footnote to answer the Question 3(a) of the 29.3.1 Exercises within the Chapter 29 “Quarto” of the Book “R for Data Science, 2nd Edition” by Wickham, Cetinkaya-Rundel & Grolemund.↩︎\nSource: Scribbr.com. What is a block quote?↩︎\nI use Ctrl + Shift + 7 to create a footnote here.↩︎\nThis is a test footnote I wrote in the Source Editor↩︎"
  },
  {
    "objectID": "solutions.html",
    "href": "solutions.html",
    "title": "Solutions",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nChapter 10\n\n\nLayers\n\n\nAug 22, 2023\n\n\n\n\nChapter 11\n\n\nExploratory data analysis\n\n\nAug 28, 2023\n\n\n\n\nChapter 12\n\n\nCommunication\n\n\nAug 28, 2023\n\n\n\n\nChapter 13\n\n\nLogical Vectors\n\n\nSep 3, 2023\n\n\n\n\nChapter 14\n\n\nNumbers\n\n\nSep 8, 2023\n\n\n\n\nChapter 15\n\n\nStrings\n\n\nSep 10, 2023\n\n\n\n\nChapter 2\n\n\nData Visualization\n\n\nJul 29, 2023\n\n\n\n\nChapter 21\n\n\nSpreadsheets\n\n\nAug 15, 2023\n\n\n\n\nChapter 29\n\n\nQuarto\n\n\nJul 25, 2023\n\n\n\n\nChapter 3\n\n\nWorkflow: Basics\n\n\nJul 29, 2023\n\n\n\n\nChapter 30\n\n\nQuarto Formats\n\n\nJul 25, 2023\n\n\n\n\nChapter 4\n\n\nData Transformation\n\n\nJul 29, 2023\n\n\n\n\nChapter 5\n\n\nWorkflow: code style\n\n\nAug 2, 2023\n\n\n\n\nChapter 6\n\n\nData tidying\n\n\nAug 5, 2023\n\n\n\n\nChapter 7\n\n\nWorkflow: scripts and projects\n\n\nAug 8, 2023\n\n\n\n\nChapter 8\n\n\nData Import\n\n\nAug 10, 2023\n\n\n\n\nChapter 9\n\n\nWorkflow: getting help\n\n\nAug 13, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to the quirky realm of Aditya Dahiya, your friendly neighborhood Indian Administrative Service (IAS) officer, currently working as the Director and Special Secretary in the Government of Haryana, India. Amidst the daily avalanche of files and meetings that could rival a paper mountain, Aditya somehow manages to sneak in quality time with his true loves: data visualization and health financing data. Think of him as your data-wrangling, file-taming, bureaucracy-battling guide with a slightly skewed sense of humor. Need more bureaucratic banter or data insights? Connect with Aditya on LinkedIn or shoot him an email – he promises it won’t be as formal as a government memo!"
  },
  {
    "objectID": "Ch29-6-3-1-3.html",
    "href": "Ch29-6-3-1-3.html",
    "title": "Chapter 29: Exercise 29.6.3.3",
    "section": "",
    "text": "3. Change the size of the figure with the following chunk options, one at a time, render your document, and describe how the figure changes.\n\nHow the figure changes with `fig-width: 6`\n\n\nlibrary(tidyverse)\ndiamonds |&gt;\n  filter(carat &lt;= 2.5) |&gt;\n      ggplot(aes(x = carat)) + \n        geom_freqpoly(binwidth = 0.01) +\n        theme_light() +\n        labs(x=\"Diamond Carat Size\", y = \"Number of Diamonds\")\n\n\n\n\nFigure 1: Plot with width fixed at 10\n\n\n\n\n\nHow the figure changes with `fig-height: 3`\n\n\ndiamonds |&gt;\n  filter(carat &lt;= 2.5) |&gt;\n      ggplot(aes(x = carat)) + \n        geom_freqpoly(binwidth = 0.01) +\n        theme_light() +\n        labs(x=\"Diamond Carat Size\", y = \"Number of Diamonds\")\n\n\n\n\nFigure 2: Plot with height fixed at 10\n\n\n\n\n\nHow the figure changes with `out-width: \"100%\"`\n\n\ndiamonds |&gt;\n  filter(carat &lt;= 2.5) |&gt;\n      ggplot(aes(x = carat)) + \n        geom_freqpoly(binwidth = 0.01) +\n        theme_light() +\n        labs(x=\"Diamond Carat Size\", y = \"Number of Diamonds\")\n\n\n\n\nFigure 3: Plot with output width at 100%\n\n\n\n\n\nHow the figure changes with `out-width: \"20%\"`\n\n\ndiamonds |&gt;\n  filter(carat &lt;= 2.5) |&gt;\n      ggplot(aes(x = carat)) + \n        geom_freqpoly(binwidth = 0.01) +\n        theme_light() +\n        labs(x=\"Diamond Carat Size\", y = \"Number of Diamonds\")\n\n\n\n\nFigure 4: Plot with output width at 20%"
  },
  {
    "objectID": "Chapter2.html",
    "href": "Chapter2.html",
    "title": "Chapter 2",
    "section": "",
    "text": "library(tidyverse)\nlibrary(palmerpenguins)\npenguins = penguins\n\n\n2.2.5 Exercises\n\nHow many rows are in penguins? How many columns?\n\nThe number of rows in penguins data-set is 344 and the number of columns is 8\n\nWhat does the bill_depth_mm variable in the penguins data frame describe? Read the help for ?penguins to find out.\n\nFirst, we find out the names of the variables in the penguins data frame in Table 1.\n\nnames(penguins) |&gt;\n  t() |&gt;\n  as_tibble() |&gt;\n  gt::gt()\n\n\n\n\n\nTable 1:  List of variables in the penguins dataset \n  \n    \n    \n      V1\n      V2\n      V3\n      V4\n      V5\n      V6\n      V7\n      V8\n    \n  \n  \n    species\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n  \n  \n  \n\n\n\n\n# Finding the details of the variables.\n# ?penguins\n\nThe variable name bill_depth_mm depicts “a number denoting bill depth (millimeters)”.[Gorman, Williams, and Fraser (2014)](Horst, Hill, and Gorman 2020)\n\nMake a scatter-plot of bill_depth_mm vs. bill_length_mm. That is, make a scatter-plot with bill_depth_mm on the y-axis and bill_length_mm on the x-axis. Describe the relationship between these two variables.\n\nThe scatter-plot is depicted below.\n\npenguins |&gt;\n  ggplot(mapping = aes(x = bill_length_mm,\n                       y = bill_depth_mm,\n                       col = species)) +\n  geom_point() +\n  geom_smooth(se = FALSE,\n              method = \"lm\") +\n  theme_classic() +\n  labs(x = \"Bill Length (mm)\", y = \"Bill Depth (mm)\")\n\n\n\n\nFigure 1: Scatterplot of relation between Bill Length and Bill Depth\n\n\n\n\nWe now test the correlations, and create a beautiful table using gt (Iannone et al. 2023)and gtExtras packages.(Mock 2022)\n\n# Checking the correlation between the two variables\ntest1 = function(x) {cor.test(x$bill_length_mm, x$bill_depth_mm)$estimate}\n\n# An empty data-frame to collect results\ndf = tibble(Penguins = NA,\n            Correlation = NA,\n            .rows = 4)\n# Finding Correlation by each penguin variety\nfor (y in 1:3) {\n  c = penguins |&gt;\n      filter(species == unique(penguins$species)[y]) |&gt;\n      test1() |&gt;\n      format(digits = 2)\n  df[y,2] = c\n  df[y,1] = unique(penguins$species)[y]\n  }\n# Converting the nature of 1st column from factor to character\ndf$Penguins = as.character(df$Penguins)  \n# Storing the overall correlation\ndf[4,1] = \"Overall\"\ndf[4,2] = penguins |&gt; test1() |&gt; format(digits = 2)\n\n# Displaying the result\ngt::gt(df) |&gt;\n  gt::tab_header(title = \"Correlation Coefficitents\",\n                 subtitle = \"Between Bill Length & Bill Depth amongst   \n                 different penguins\") |&gt;\n  gtExtras::gt_theme_538() |&gt;\n  gtExtras::gt_highlight_rows(rows = 4, fill = \"#d4cecd\")\n\n\n\n\n\nTable 2:  Correlation Table amongst different types of penguins \n  \n    \n      Correlation Coefficitents\n    \n    \n      Between Bill Length & Bill Depth amongst   \n                 different penguins\n    \n    \n      Penguins\n      Correlation\n    \n  \n  \n    Adelie\n0.39\n    Gentoo\n0.64\n    Chinstrap\n0.65\n    Overall\n-0.24\n  \n  \n  \n\n\n\n\n\nThus, we see that the relation is not apparent on a simple scatter plot, but if we plot a different colour for each species, we observe that there is positive correlation between Bill Length and Bill Depth, in all three species. The strongest correlation is amongst Gentoo and Chinstrap penguins.\n\nWhat happens if you make a scatter-plot of species vs. bill_depth_mm? What might be a better choice of geom?\nIf we make a scatter-plot of species vs. bill_depth_mm, the following happens:-\n\npenguins |&gt;\n  ggplot(mapping = aes(x = species,\n                       y = bill_depth_mm)) +\n  geom_point() +\n  theme_bw() +\n  labs(x = \"Species\", y = \"Bill Depth (mm)\")\n\n\n\n\nFigure 2: Scatter plot of species vs. Bill Depth\n\n\n\n\nThis produces an awkward scatter-plot, since the x-axis variable is discrete, and not continuous. A better choice of geom might be a box-plot, which is a good way to present the relationship between a continuous (Bill Depth) and a categorical (species) variable. which shows that the average Bill Depth (in mm) is lower in Gentoo penguins compared to the other two.\n\npenguins |&gt;\n  ggplot(mapping = aes(x = species,\n                       y = bill_depth_mm)) +\n  geom_boxplot() +\n  theme_bw() +\n  labs(x = \"Species\", y = \"Bill Depth (mm)\")\n\n\n\n\nFigure 3: Box-plot of species vs. Bill Depth\n\n\n\n\nWhy does the following give an error and how would you fix it?\nggplot(data = penguins) +    geom_point()\nThe above code will give an error, because we have only given the data to the ggplot call, but not specified the mapping aesthetics, i.e., the x-axis and y-axis for the scatter plot called by the geom_point() . We can fix the error as follows in Figure 4 :---\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nFigure 4: Corrected code to display the plot\n\n\n\n\nWhat does the na.rm argument do in geom_point()? What is the default value of the argument? Create a scatterplot where you successfully use this argument set to TRUE.\nWithin the function geom_point() the na.rm argument can do one of the two things. If it is set to FALSE , as it is by default, then the missing values are removed but the following warning message is displayed:–\nWarning message: \nRemoved 2 rows containing missing values (`geom_point()`)\nBut, if it is set to na.rm = TRUE, then the missing values are silently removed. Here’s the code with na.rm = TRUE to produce Figure 5 :---\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm)) +\n  geom_point(na.rm = TRUE)\n\n\n\n\nFigure 5: Corrected code to display the plot with na.rm = TRUE\n\n\n\n\nAdd the following caption to the plot you made in the previous exercise: “Data come from the palmerpenguins package.” Hint: Take a look at the documentation for labs().\nThe caption is added here with the labs function with ggplot function below in (fi?)\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm)) +\n  geom_point(na.rm = TRUE) +\n  labs(caption = \"Data come from the palmerpenguins package.\")\n\n\n\n\nFigure 6: Plot with a caption added in ggplot call itself\n\n\n\n\nRecreate the following visualization. What aesthetic should bill_depth_mm be mapped to? And should it be mapped at the global level or at the geom level?\n\npenguins |&gt;\n  ggplot(mapping = aes(x = flipper_length_mm,\n                       y = body_mass_g)) + \n  geom_point(mapping = aes(color = bill_depth_mm)) + \n  geom_smooth()\n\n\n\n\nFigure 7: Recreated figure using the ggplot2 code\n\n\n\n\nThe code above recreates the Figure 7. The aesthetic should bill_depth_mm should be mapped the aesthetic colo in the geom_point() function level. It should not be done at the global level, because then it will even be an aesthetic for geom_smooth resulting in multiple smoother lines fitted for each level of bill_depth_mm , and possible result in an error because bill_depth_mm is not a categorical variable or a factor variable with certain distinct categories or levels.\nLuckily, ggplot2 recognizes this error and still produces the same plot by droppin the color aesthetic, i.e., The following aesthetics were dropped during statistical transformation: colour. So, ggplot2 is trying to guess our intentions, and it works, but the code not correct. The wrong code is tested at Figure 8.\n\npenguins |&gt;\n  ggplot(mapping = aes(x = flipper_length_mm,\n                       y = body_mass_g,\n                       color = bill_depth_mm)) + \n  geom_point() + \n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: The following aesthetics were dropped during statistical transformation: colour\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nFigure 8: The Wrong Code - Recreated figure is the same - but the code is fundamentally flawed\n\n\n\n\nRun this code in your head and predict what the output will look like. Then, run the code in R and check your predictions.\n\nggplot(data = penguins,\n       mapping = aes(x = flipper_length_mm, \n                     y = body_mass_g, \n                     color = island)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\nOn visual inspection, I believe this code should create a scatter plot of penguins flipper lengths (on x-axis) vs. body mass (on y-axis), with the dots coloured by islands on the penguins. Further, a smoother line if fitted to show the relationship, with a separate smoother line for each island type. Thus, since we know there are three types of islands, we expect three smoother lines fitted to the plot, without the display of standard error intervals.\nNow, let us check our predictions with the code in the Figure 9 :--\n\n\n\n\n\nFigure 9: Plot generated from running the Code of Question 9\n\n\n\n\nWill these two graphs look different? Why/why not?\n\n# Code 1\nggplot(data = penguins,\n       mapping = aes(x = flipper_length_mm, \n                     y = body_mass_g)) +\n  geom_point() +\n  geom_smooth()\n\n# Code 2\nggplot() +\n  geom_point(data = penguins,\n             mapping = aes(x = flipper_length_mm, \n                           y = body_mass_g)\n  ) +\n  geom_smooth(data = penguins,\n              mapping = aes(x = flipper_length_mm, \n                            y = body_mass_g))\n\nYes, these two graphs should look the same. Since, the data and the aesthetics mapped are the same in both. Only difference is that the second code has redundancy.\nHere’s the visual confirmation for both codes in Figure 10.\n\n\n\n\n\nFigure 10: Comparison of the two plots produced by the codes in Question 10\n\n\n\n\n\n\n\n2.4.3 Exercises\n\nMake a bar plot of species of penguins, where you assign species to the y aesthetic. How is this plot different?\nWhen we assign species to the y-axis, we get a horizontal bar plot, instead of the vertical bar plot given in the textbook. The results are compared in Figure 11 .\n\np1 = penguins |&gt;\n      ggplot(aes(x = species)) +\n      geom_bar() +\n      labs(caption = \"Species on x-axis\")\np2 = penguins |&gt;\n      ggplot(aes(y = species)) +\n      geom_bar() +\n      labs(caption = \"Species on y-axis\")\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n\n\n\nFigure 11: Change in figure when species is assigned to y-axis\n\n\n\n\nHow are the following two plots different? Which aesthetic, color or fill, is more useful for changing the color of bars?\nThe output of the two plots is in Figure 12 .\n\ngridExtra::grid.arrange(\n\nggplot(penguins, aes(x = species)) + geom_bar(color = \"red\") +\n  labs(caption = \"Color = Red\"),\n\nggplot(penguins, aes(x = species)) + geom_bar(fill = \"red\") +\n  labs(caption = \"Fill = Red\"),\n\nncol = 2)\n\n\n\n\nFigure 12: The two plots produced by the code given, with red color vs. red fill\n\n\n\n\nThe two plots are different in where the colour red appears. As a color aesthetic, it appears only on the borders. But, as a fill aesthetic, it fills the entire bar(s).\nThus, the aesthetic fill is more useful in changing the colour of the bars.\nWhat does the bins argument in geom_histogram() do?\nThe bins argument tell the number of bins (i.e. number of bars) in the histogram to be plotted. The default value is 30. However, if the binwidth is also specified, then the binwidth argument over-rides the bins argument.\nMake a histogram of the carat variable in the diamonds dataset that is available when you load the tidyverse package. Experiment with different binwidths. What bin-width reveals the most interesting patterns?\n\ng1 = ggplot(diamonds, aes(x=carat)) + \n  geom_histogram(fill = \"white\", color = \"black\") + \n  theme_classic() + labs(x = NULL, y = NULL, \n                         subtitle = \"Default Bindwidth\")\ng2 = ggplot(diamonds, aes(x=carat)) + \n    geom_histogram(fill = \"white\", color = \"black\", \n                   binwidth = 0.1) + \n    theme_classic() + \n    labs(x = NULL, y = NULL, \n         subtitle = \"Bindwidth = 0.1\")\ng3 = ggplot(diamonds, aes(x=carat)) + \n    geom_histogram(fill = \"white\", color = \"black\", \n                   binwidth = 0.2) + \n    theme_classic() + \n    labs(x = NULL, y = NULL, \n         subtitle = \"Bindwidth = 0.2\")  \ng4 = ggplot(diamonds, aes(x=carat)) + \n    geom_histogram(fill = \"white\", color = \"black\", \n                   binwidth = 0.3) + \n    theme_classic() + \n    labs(x = NULL, y = NULL, \n         subtitle = \"Bindwidth = 0.3\")\ng5 = ggplot(diamonds, aes(x=carat)) + \n    geom_histogram(fill = \"white\", color = \"black\", \n                   binwidth = 0.5) + \n    theme_classic() + \n    labs(x = NULL, y = NULL, \n         subtitle = \"Bindwidth = 0.5\")\ng6 = ggplot(diamonds, aes(x=carat)) + \n    geom_histogram(fill = \"white\", color = \"black\", \n                   binwidth = 1) + \n    theme_classic() + \n    labs(x = NULL, y = NULL, \n         subtitle = \"Bindwidth = 1\")\n\ngridExtra::grid.arrange(g1, g2, g3, g4, g5, g6,\n                        ncol = 3, nrow = 2)\n\n\n\n\nFigure 13: Histogram with different bin-widths tried out to select the most relevant one\n\n\n\n\nThus, we see that best binwidth is either the default binwidth chosen by ggplot2 or the bind-width of 0.2 per bin, since it reveals the most interesting patterns.\n\n\n\n2.5.5 Exercises\n\nThe mpg data frame that is bundled with the ggplot2 package contains 234 observations collected by the US Environmental Protection Agency on 38 car models. Which variables in mpg are categorical? Which variables are numerical? (Hint: Type ?mpg to read the documentation for the dataset.) How can you see this information when you run mpg?\nThe code below displays the summary fo the mpg data-set. The following variables are categorical: manufacturer (manufacturer name), model (model name), trans (type of transmission), drv (the type of drive train: front, rear or 4-wheel), fl (fuel type), and class (type of car). The numerical variables are displ (engine displacement, in litres), year (year of manufacture) , cyl (number of cylinders) , cty (city miles per gallon) and hwy (highway miles per gallon). We can see these in the square parenthesis the column titled Variable in the output of the code below .\n\n# Visualize summary of the data frame\nmpg |&gt;\n  summarytools::dfSummary(plain.ascii  = FALSE, \n                          style = \"grid\", \n                          graph.magnif = 0.75, \n                          valid.col = FALSE,\n                          na.col = FALSE,\n                          headings = FALSE) |&gt;\n  view()\n\nIf we simply run mpg , we can still see this information in the R console output, by the terms &lt;chr&gt; (for categorical variables) ; and, &lt;dbl&gt; or &lt;int&gt; (for numerical variables)\n\nmpg\n## # A tibble: 234 × 11\n##    manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n##    &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n##  1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n##  2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n##  3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n##  4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n##  5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n##  6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n##  7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n##  8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n##  9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n## 10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n## # ℹ 224 more rows\n\nMake a scatterplot of hwy vs. displ using the mpg data frame. Next, map a third, numerical variable to color, then size, then both color and size, then shape. How do these aesthetics behave differently for categorical vs. numerical variables?\n\ng1 = mpg |&gt;\n      ggplot(aes(x = hwy, y = displ)) +\n      geom_point() +\n      theme_minimal() +\n      labs(caption = \"Original Plot\")\n# Using numerical variable 'cty' to map to colour, size\ng2 = mpg |&gt;\n      ggplot(aes(x = hwy, y = displ, color = cty)) +\n      geom_point() +\n      theme_minimal()+\n      labs(caption = \"cty mapped to color\")\ng3 = mpg |&gt;\n      ggplot(aes(x = hwy, y = displ, size = cty)) +\n      geom_point(alpha = 0.5) +\n      theme_minimal()+\n      labs(caption = \"cty mapped to size\")\ng4 = mpg |&gt;\n      ggplot(aes(x = hwy, y = displ, \n                 color = cty, size = cty)) +\n      geom_point() +\n      theme_minimal()+\n      labs(caption = \"cty mapped to size and color\")\ngridExtra::grid.arrange(g1, g2, g3, g4, ncol = 2)\n\n\n\n\nFigure 14: Scatterplots of different kinds for different aesthetic mappings\n\n\n\n\nSo, we see that we can map a numerical variable to color or size aesthetics, and ggplot2 will itself make a scale and display the output with a legend. However, numerical variables (i.e., continuous variables) don’t map to shape aesthetic, as there cannot be any continuum amongst shapes. Accordingly, when mapped to shape, the code throws an error as below:---\n\n# Using numerical variable 'cty' to map to size aesthetic \n  mpg |&gt;\n      ggplot(aes(x = hwy, \n                 y = displ, \n                 shape = cty)) +\n      geom_point()\n\nError in `geom_point()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error in `scale_f()`:\n! A continuous variable cannot be mapped to the shape aesthetic\nℹ choose a different aesthetic or use `scale_shape_binned()`\n\n\nThus, the shape aesthetic works only with categorical variables, whereas color works with both numerical and categorical variables; and, by definition size aesthetic should be used only with numerical variables (it can work with categorical variables, but then the sizes are assigned arbitrarily to different categories).\nIn the scatter-plot of hwy vs. displ, what happens if you map a third variable to linewidth?\n\nmpg |&gt;\n      ggplot(aes(x = hwy, y = displ,\n                 linewidth = cty)) +\n      geom_point() +\n      theme_minimal()\n\n\n\n\nFigure 15: Experiment with mapping line width to a third variable\n\n\n\n\nAs we see, nothing changes with addition of the linewidth argument in Figure 15 . This is because the linewidth argument “scales the width of lines and polygon strokes.” in ggplot2 documentation. Since we are only plotting point geoms, and no lines, the argument is useless and not used to produce the output.\nWhat happens if you map the same variable to multiple aesthetics?\nWe can map the same variable to multiple aesthetics, and the output will display its variations in all such aesthetics. But it is redundant, and make plot cluttery with too much visual input.\nFor example, Figure 16 shows a poorly understandable plot where class of the vehicle is mapped to size, shape and color. It works, but there’s too much information redundancy.\n\nmpg |&gt;\n  ggplot(aes(x=hwy, y = cty,\n             size = class,\n             color = class,\n             shape = class)) +\n  geom_point(alpha = 0.5) +\n  theme_classic()\n\n\n\n\nFigure 16: A messy plot with Mutliple aesthetics defined by the same variable\n\n\n\n\nMake a scatterplot of bill_depth_mm vs. bill_length_mm and color the points by species. What does adding coloring by species reveal about the relationship between these two variables? What about faceting by species?\nThe Figure 17 shows the importance of coloring or faceting by species. This allows us to detect a fairly strong positive correlation which was not apparent in the simple scatter plot. This, perhaps, can be called an example of negative confounding (Mehio-Sibai et al. 2005) of the relation between bill depth and bill length by the species type.\n\np1 = penguins |&gt;\n  ggplot(mapping = aes(x = bill_length_mm,\n                       y = bill_depth_mm)) +\n  geom_point() +\n  geom_smooth(se = FALSE,\n              method = \"lm\") +\n  theme_classic() +\n  labs(x = \"Bill Length (mm)\", y = \"Bill Depth (mm)\",\n       subtitle = \"No relation is apparent\")\n\np2 = penguins |&gt;\n  ggplot(mapping = aes(x = bill_length_mm,\n                       y = bill_depth_mm,\n                       col = species)) +\n  geom_point() +\n  geom_smooth(se = FALSE,\n              method = \"lm\") +\n  theme_classic() +\n  labs(x = \"Bill Length (mm)\", y = \"Bill Depth (mm)\",\n       subtitle = \"Colouring  by species reveals relations\")\n\np3 = penguins |&gt;\n  ggplot(mapping = aes(x = bill_length_mm,\n                       y = bill_depth_mm)) +\n  geom_point() +\n  geom_smooth(se = FALSE,\n              method = \"lm\") +\n  facet_wrap(~species) +\n  theme_classic() +\n  labs(x = \"Bill Length (mm)\", y = \"Bill Depth (mm)\",\n       subtitle = \"Faceting also reveals the relations\")\n\nlay = rbind(c(1,1,2,2,2),\n            c(3,3,3,3,3))\ngridExtra::grid.arrange(p1, p2, p3, layout_matrix = lay)\n\n\n\n\nFigure 17: Adding color by species reveals a strong relationship\n\n\n\n\nWhy does the following yield two separate legends? How would you fix it to combine the two legends?\n\n\nggplot(data = penguins,   \n       mapping = aes(x = bill_length_mm, \n                     y = bill_depth_mm,      \n                     color = species, \n                     shape = species)) +   \n  geom_point() +   \n  labs(color = \"Species\")\n\nThis code presents a plot with two legends because in the last line, we have forced ggplot2 to name out “Color” legend as the string “Species”. Thus, ggplot2 differentiates between “species” and “Species”.\nWe can correct this issue in either of the following two ways:--\n\nggplot(data = penguins,   \n       mapping = aes(x = bill_length_mm, \n                     y = bill_depth_mm,      \n                     color = species, \n                     shape = species)) +   \n  geom_point()\n\nor,\n\nggplot(data = penguins,   \n       mapping = aes(x = bill_length_mm, \n                     y = bill_depth_mm,      \n                     color = species, \n                     shape = species)) +   \n  geom_point()\n\n\nCreate the two following stacked bar plots. Which question can you answer with the first one? Which question can you answer with the second one?\nThe plots are produced in Figure 18 .\n\ng1 = ggplot(penguins, aes(x = island, \n                     fill = species)) +   \n  geom_bar(position = \"fill\") +\n  labs(subtitle = \"Sub-figure A\")\n\ng2 = ggplot(penguins, aes(x = species, \n                     fill = island)) +   \n  geom_bar(position = \"fill\") +\n  labs(subtitle = \"Sub-figure B\")\n\ngridExtra::grid.arrange(g1, g2, ncol = 2)\n\n\n\n\nFigure 18: The two stacked bar plots produced by the code\n\n\n\n\nThe Sub-Figure A answers the question, that “On each of the three islands, what proportion of penguins belong to which species?”\nThe Sub-Figure B answers the question reg. distribution of the population of each species of penguins, that is, “For each of the penguin species’, what proportion of each species total population is found on which island?”\n\n\n\n2.6.1 Exercises\n\nRun the following lines of code. Which of the two plots is saved as mpg-plot.png? Why?\nggplot(mpg, aes(x = class)) +   \ngeom_bar() \n\nggplot(mpg, aes(x = cty, y = hwy)) +   \ngeom_point() \n\nggsave(\"mpg-plot.png\")\nThe second plot, i.e., the scatter plot is saved into the file “mpg-plot.png” in the working directory, because the function ggsave() saves only the most recent plot into the file.\nWhat do you need to change in the code above to save the plot as a PDF instead of a PNG? How could you find out what types of image files would work in ggsave()?\nTo save the plot as a PDF file, we will need to add the arguments device  = \"pdf\" to the ggsave() function call. We can find out the types of image files that would work by using the help for ggsave() function by running the code ?ggsave at the command prompt.\nThe documentation for the device argument within ggsave() function tells us that following image document types work with it:--\n\na device function (e.g. png), or\none of “eps”, “ps”, “tex” (pictex), “pdf”, “jpeg”, “tiff”, “png”, “bmp”, “svg” or “wmf” (windows only).\n\n\n\n\n\n\n\nReferences\n\nGorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” Edited by André Chiaradia. PLoS ONE 9 (3): e90081. https://doi.org/10.1371/journal.pone.0090081.\n\n\nHorst, Allison M, Alison Presmanes Hill, and Kristen B Gorman. 2020. Allisonhorst/Palmerpenguins: V0.1.0. Zenodo. https://doi.org/10.5281/ZENODO.3960218.\n\n\nIannone, Richard, Joe Cheng, Barret Schloerke, Ellis Hughes, Alexandra Lauer, and JooYoung Seo. 2023. “Gt: Easily Create Presentation-Ready Display Tables.” https://CRAN.R-project.org/package=gt.\n\n\nMehio-Sibai, Abla, Manning Feinleib, Tarek A. Sibai, and Haroutune K. Armenian. 2005. “A Positive or a Negative Confounding Variable? A Simple Teaching Aid for Clinicians and Students.” Annals of Epidemiology 15 (6): 421–23. https://doi.org/10.1016/j.annepidem.2004.10.004.\n\n\nMock, Thomas. 2022. “gtExtras: Extending ’Gt’ for Beautiful HTML Tables.” https://CRAN.R-project.org/package=gtExtras."
  },
  {
    "objectID": "Chapter30.html",
    "href": "Chapter30.html",
    "title": "Chapter 30",
    "section": "",
    "text": "library(DT)\nlibrary(tidyverse)\nlibrary(knitr)\n\n\n\nLet us try to create an interactive map in HTML below Figure 1:—̥\n\nlibrary(leaflet)\nleaflet() |&gt;\n  setView(76.801175, 30.761403, zoom = 14) |&gt; \n  addTiles() |&gt;\n  addMarkers(76.801175, 30.761403, \n             popup = \"Haryana Civil Secretariat\")  |&gt;\n  addMarkers(76.803773534,\n             30.752910586,\n             popup = \"Rock Garden\")\n\n\n\n\nFigure 1: A map of Chandigarh, India using Leaflet\n\n\n\n\n\nAn example of using DT for an interactive table is at Figure 2 :---\n\ndiamonds |&gt;\n  filter(carat &gt; 3) |&gt;\n  datatable(colnames = c(\"Carat\", \"Cut\", \"Color\",\n                         \"Clarity\", \"Depth\", \"Table\",\n                         \"Price\", \"X\", \"Y\", \"Z\"),\n            rownames = FALSE)\n\n\n\n\n\nFigure 2: A visually pleasing table produced using DT package"
  },
  {
    "objectID": "Chapter30.html#htmlwidgets",
    "href": "Chapter30.html#htmlwidgets",
    "title": "Chapter 30",
    "section": "",
    "text": "Let us try to create an interactive map in HTML below Figure 1:—̥\n\nlibrary(leaflet)\nleaflet() |&gt;\n  setView(76.801175, 30.761403, zoom = 14) |&gt; \n  addTiles() |&gt;\n  addMarkers(76.801175, 30.761403, \n             popup = \"Haryana Civil Secretariat\")  |&gt;\n  addMarkers(76.803773534,\n             30.752910586,\n             popup = \"Rock Garden\")\n\n\n\n\nFigure 1: A map of Chandigarh, India using Leaflet\n\n\n\n\n\nAn example of using DT for an interactive table is at Figure 2 :---\n\ndiamonds |&gt;\n  filter(carat &gt; 3) |&gt;\n  datatable(colnames = c(\"Carat\", \"Cut\", \"Color\",\n                         \"Clarity\", \"Depth\", \"Table\",\n                         \"Price\", \"X\", \"Y\", \"Z\"),\n            rownames = FALSE)\n\n\n\n\n\nFigure 2: A visually pleasing table produced using DT package"
  },
  {
    "objectID": "Chapter4.html",
    "href": "Chapter4.html",
    "title": "Chapter 4",
    "section": "",
    "text": "4.2.5 Exercises\n\nlibrary(tidyverse) \nlibrary(nycflights13) \nlibrary(gt)\ndata(\"flights\")\n\n\nIn a single pipeline for each condition, find all flights that meet the condition:\n\nHad an arrival delay of two or more hours\n\nflights |&gt;   \n  filter(arr_delay &gt;= 120)\n\n# A tibble: 10,200 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      811            630       101     1047            830\n 2  2013     1     1      848           1835       853     1001           1950\n 3  2013     1     1      957            733       144     1056            853\n 4  2013     1     1     1114            900       134     1447           1222\n 5  2013     1     1     1505           1310       115     1638           1431\n 6  2013     1     1     1525           1340       105     1831           1626\n 7  2013     1     1     1549           1445        64     1912           1656\n 8  2013     1     1     1558           1359       119     1718           1515\n 9  2013     1     1     1732           1630        62     2028           1825\n10  2013     1     1     1803           1620       103     2008           1750\n# ℹ 10,190 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nFlew to Houston (IAH or HOU)\n\nflights |&gt;     \n  filter(dest %in% c(\"IAH\", \"HOU\"))\n\n# A tibble: 9,313 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      623            627        -4      933            932\n 4  2013     1     1      728            732        -4     1041           1038\n 5  2013     1     1      739            739         0     1104           1038\n 6  2013     1     1      908            908         0     1228           1219\n 7  2013     1     1     1028           1026         2     1350           1339\n 8  2013     1     1     1044           1045        -1     1352           1351\n 9  2013     1     1     1114            900       134     1447           1222\n10  2013     1     1     1205           1200         5     1503           1505\n# ℹ 9,303 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nWere operated by United, American, or Delta\n\nflights |&gt;   \n  filter(carrier %in% c(\"UA\", \"AA\", \"DL\"))\n\n# A tibble: 139,504 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      554            600        -6      812            837\n 5  2013     1     1      554            558        -4      740            728\n 6  2013     1     1      558            600        -2      753            745\n 7  2013     1     1      558            600        -2      924            917\n 8  2013     1     1      558            600        -2      923            937\n 9  2013     1     1      559            600        -1      941            910\n10  2013     1     1      559            600        -1      854            902\n# ℹ 139,494 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nDeparted in summer (July, August, and September)\n\nflights |&gt;   \n  filter(month %in% c(7, 8, 9))\n\n# A tibble: 86,326 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     7     1        1           2029       212      236           2359\n 2  2013     7     1        2           2359         3      344            344\n 3  2013     7     1       29           2245       104      151              1\n 4  2013     7     1       43           2130       193      322             14\n 5  2013     7     1       44           2150       174      300            100\n 6  2013     7     1       46           2051       235      304           2358\n 7  2013     7     1       48           2001       287      308           2305\n 8  2013     7     1       58           2155       183      335             43\n 9  2013     7     1      100           2146       194      327             30\n10  2013     7     1      100           2245       135      337            135\n# ℹ 86,316 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nArrived more than two hours late, but didn’t leave late\n\nflights |&gt;     \n  filter(dep_delay &lt;= 0) \n\n# A tibble: 200,089 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      544            545        -1     1004           1022\n 2  2013     1     1      554            600        -6      812            837\n 3  2013     1     1      554            558        -4      740            728\n 4  2013     1     1      555            600        -5      913            854\n 5  2013     1     1      557            600        -3      709            723\n 6  2013     1     1      557            600        -3      838            846\n 7  2013     1     1      558            600        -2      753            745\n 8  2013     1     1      558            600        -2      849            851\n 9  2013     1     1      558            600        -2      853            856\n10  2013     1     1      558            600        -2      924            917\n# ℹ 200,079 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nWere delayed by at least an hour, but made up over 30 minutes in flight\n\nflights |&gt;     \n  filter(dep_delay - arr_delay &gt;= 30) \n\n# A tibble: 20,395 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      701            700         1     1123           1154\n 2  2013     1     1      820            820         0     1249           1329\n 3  2013     1     1      840            845        -5     1311           1350\n 4  2013     1     1      857            851         6     1157           1222\n 5  2013     1     1      909            810        59     1331           1315\n 6  2013     1     1     1025            951        34     1258           1302\n 7  2013     1     1     1153           1200        -7     1450           1529\n 8  2013     1     1     1245           1249        -4     1722           1800\n 9  2013     1     1     1610           1615        -5     1913           1948\n10  2013     1     1     1625           1550        35     2054           2050\n# ℹ 20,385 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nSort flights to find the flights with longest departure delays. Find the flights that left earliest in the morning.\nThe top 5 flights that has the longest departure delays are shown in Table 1 .\n\nflights |&gt;   \n  arrange(desc(dep_delay)) |&gt;   \n  head(n = 5) |&gt;\n  select(year, month, day, dep_time, sched_dep_time,\n         dep_delay, carrier, flight, origin) |&gt;\n  mutate(Day = as_date(paste(year, month, day, sep = \"-\"))) |&gt;\n  select(-year, -month, -day) |&gt;\n  relocate(Day, .before = dep_time) |&gt;\n  gt::gt()\n\n\n\n\n\nTable 1:  5 Flights with longest departure delays \n  \n\n    \n      Day\n      dep_time\n      sched_dep_time\n      dep_delay\n      carrier\n      flight\n      origin\n    \n  \n  \n    2013-01-09\n641\n900\n1301\nHA\n51\nJFK\n    2013-06-15\n1432\n1935\n1137\nMQ\n3535\nJFK\n    2013-01-10\n1121\n1635\n1126\nMQ\n3695\nEWR\n    2013-09-20\n1139\n1845\n1014\nAA\n177\nJFK\n    2013-07-22\n845\n1600\n1005\nMQ\n3075\nJFK\n  \n\n\n\n\n\n\n\nSort flights to find the fastest flights. (Hint: Try including a math calculation inside of your function.)\nThe speed of a flight can be found as distance/air_time . The Table 2 displays the 5 fastest flights.\n\nflights |&gt;\n  arrange(desc(distance/air_time)) |&gt;\n  slice_head(n=5) |&gt;\n  select(year, month, day, distance, air_time,\n         carrier, flight, origin, dest) |&gt;\n  gt()\n\n\n\n\n\nTable 2:  5 fastest Flights (by speed) \n  \n\n    \n      year\n      month\n      day\n      distance\n      air_time\n      carrier\n      flight\n      origin\n      dest\n    \n  \n  \n    2013\n5\n25\n762\n65\nDL\n1499\nLGA\nATL\n    2013\n7\n2\n1008\n93\nEV\n4667\nEWR\nMSP\n    2013\n5\n13\n594\n55\nEV\n4292\nEWR\nGSP\n    2013\n3\n23\n748\n70\nEV\n3805\nEWR\nBNA\n    2013\n1\n12\n1035\n105\nDL\n1902\nLGA\nPBI\n  \n\n\n\n\n\n\n\nWas there a flight on every day of 2013?\nYes, there was a flight on every day of 2013, because using distinct() function, we find that there are 365 unique combinations of year , month , and day .\n\nflights |&gt;\n  distinct(year, month, day) |&gt;\n  count() |&gt;\n  as.numeric()\n\n[1] 365\n\n\nWhich flights traveled the farthest distance? Which traveled the least distance?\nThe top 5 flights by the farthest distance traveled are shown in Table 3 .\n\nflights |&gt;\n  arrange(desc(distance)) |&gt;\n  select(origin, dest, distance, air_time, carrier) |&gt;\n  # Distinct added to remove same flight (on different days) repeating in top 5\n  distinct(origin, dest, .keep_all = TRUE) |&gt;\n  slice_head(n = 5) |&gt;\n  gt()\n\n\n\n\n\nTable 3:  5 longest distance flights \n  \n\n    \n      origin\n      dest\n      distance\n      air_time\n      carrier\n    \n  \n  \n    JFK\nHNL\n4983\n659\nHA\n    EWR\nHNL\n4963\n656\nUA\n    EWR\nANC\n3370\n418\nUA\n    JFK\nSFO\n2586\n366\nUA\n    JFK\nOAK\n2576\n330\nB6\n  \n\n\n\n\n\n\n\nThe 5 flights with least distance traveled are shown in\n\nflights |&gt;\n  arrange(distance) |&gt;\n  select(origin, dest, distance, air_time, carrier) |&gt;\n  # Distinct added to remove same flight (which runs\n  # on different days) repeating in top 5\n  distinct(origin, dest, .keep_all = TRUE) |&gt;\n  slice_head(n = 5) |&gt;\n  gt()\n\n\n\n\n\nTable 4:  5 shortest distance flights \n  \n\n    \n      origin\n      dest\n      distance\n      air_time\n      carrier\n    \n  \n  \n    EWR\nLGA\n17\nNA\nUS\n    EWR\nPHL\n80\n30\nEV\n    JFK\nPHL\n94\n35\n9E\n    LGA\nPHL\n96\n32\nUS\n    EWR\nBDL\n116\n25\nEV\n  \n\n\n\n\n\n\n\nDoes it matter what order you used filter() and arrange() if you’re using both? Why/why not? Think about the results and how much work the functions would have to do.\nAlthough, in terms of output received, it does not matter in which order we use them, because when we run the function filter() it removes the rows not required, but leaves the arrangement-ordering the same, i.e. the remaining rows move up.\nHowever, using arrange() before filter() means R will have to arrange all the rows, and then we filter out only a few rows - thus meaning that more work will have to be done computationally.\nFor computational efficiency, it would be better if we use filter() first, then run arrange() only on the subset of rows remaining.\nHere’s the proof for this, using system.time() function in R which tells how much time does an R expression take to run. Here, I compare both functions using the logical operator &gt; (greater than). The elapsed time comes TRUE, meaning that arranging first, and then filtering takes more time.\n\nsystem.time( flights |&gt;\n  arrange(distance) |&gt;\n  filter(air_time &lt; 60)\n  ) &gt; system.time(\n  flights |&gt;\n    filter(air_time &lt; 60) |&gt;\n    arrange(distance)\n)\n\n user.self   sys.self    elapsed user.child  sys.child \n     FALSE      FALSE       TRUE         NA         NA \n\n\n\n\n\n4.3.5 Exercises\n\nCompare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?\nWe would expect dep_delay = dep_time - sched_dep_time . Let us check this in the code, as well.\n\nflights |&gt;\n  mutate(calc = dep_time - sched_dep_time) |&gt;\n  mutate(match = calc == dep_delay, .keep = \"used\") |&gt;\n  summarise(Matching = sum(match, na.rm = TRUE),\n            Total = count(flights)) |&gt;\n  mutate(Percentage = 100*Matching/Total)\n\n# A tibble: 1 × 3\n  Matching Total$n Percentage$n\n     &lt;int&gt;   &lt;int&gt;        &lt;dbl&gt;\n1   228744  336776         67.9\n\n\nThe results indicate that 67.9% of the time, the comparison works out as expected. For others, there might be missing data issues (hence, we had to use na.rm = TRUE) or, any other data error.\nBrainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights.\n\n# Using variable names\nflights |&gt; \n  select(dep_time, dep_delay, arr_time, arr_delay)\n\n# Using starts_with()\nflights |&gt; \n  select(starts_with(\"dep\"), starts_with(\"arr\"))\n\n# Using column numbers\nflights |&gt;\n  select(4,6,7,9)\n\n# Using from, to, ie., \":\" along with \"!\" to remove sched_ \nflights |&gt; \n  select(dep_time:arr_delay) |&gt;\n  select(!starts_with(\"sched\"))\n\n# Using column numbers with :\nflights |&gt;\n  select(4:9, -5, -8)\n\nWhat happens if you specify the name of the same variable multiple times in a select() call?\nIf we specify the name of the same variable multiple times, the dplyr package understands the mistake, and only produces one copy of the variable in the output. The place of the variable is the one that first appears in the code within the select() function. Here are two examples:---\n\nflights |&gt;\n  select(dep_time, dep_time) |&gt;\n  slice_head(n=2)\n\n# A tibble: 2 × 1\n  dep_time\n     &lt;int&gt;\n1      517\n2      533\n\nflights |&gt;\n  select(dep_time:dep_delay, sched_dep_time) |&gt;\n  slice_head(n=2)\n\n# A tibble: 2 × 3\n  dep_time sched_dep_time dep_delay\n     &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;\n1      517            515         2\n2      533            529         4\n\n\nWhat does the any_of() function do? Why might it be helpful in conjunction with this vector?\nvariables &lt;- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")\nThe two functions, any_of() and all_of() are called selection helpers. They help select variables contained in a character vector, such as variables .\nIn present scenario, the any_of() can be used with variables vector to select these columns (or, remove these columns) from the flights data-set, as shown in the code below:---\n\nvariables &lt;- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")\n\nflights |&gt;\n  select(any_of(variables)) |&gt;\n  slice_head(n=2)\n\n# A tibble: 2 × 5\n   year month   day dep_delay arr_delay\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1  2013     1     1         2        11\n2  2013     1     1         4        20\n\n\nBut, there is a difference between any_of() and all_of() . As shown in R help, all_of() is for strict selection. If any of the variables in the character vector is missing, an error is thrown. But, any_of() doesn’t check for missing variables. It is especially useful with negative selections, when you would like to make sure a variable is removed. Here’s an example to demonstrate:---\n\n# Change \"day\" to \"date\" to delibertely cause a missing variable name\nvariables &lt;- c(\"year\", \"month\", \"date\", \"dep_delay\", \"arr_delay\")\n\n# all_of() should not work\nflights |&gt;\n  select(all_of(variables)) |&gt;\n  slice_head(n=2)\n\nError in `all_of()`:\n! Can't subset columns that don't exist.\n✖ Column `date` doesn't exist.\n\n# any_of() will still work\nflights |&gt;\n  select(any_of(variables)) |&gt;\n  slice_head(n=2)\n\n# A tibble: 2 × 4\n   year month dep_delay arr_delay\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1  2013     1         2        11\n2  2013     1         4        20\n\n\nDoes the result of running the following code surprise you? How do the select helpers deal with upper and lower case by default? How can you change that default? flights |&gt; select(contains(\"TIME\"))\nYes, the running of this code surprises me because generally, R is very picky about upper-case vs. lower-case. Since \"TIME\" is not contained in any variable name, I expected it to throw an error. Yet, it returns all variables which contain \"time\".\nThus, this means that the following select helpers from tidyselect package ignore the case of the match provided by default.\n\nstarts_with(): Starts with an exact prefix.\nends_with(): Ends with an exact suffix.\ncontains(): Contains a literal string.\nmatches(): Matches a regular expression.\n\nTo change this, we can set the argument ignore.case = FALSE.\nRename air_time to air_time_min to indicate units of measurement and move it to the beginning of the data frame.\nThe following code does the job, as shown in the output:--\n\nflights |&gt;\n  rename(air_time_min = air_time) |&gt;\n  relocate(air_time_min)\n\n# A tibble: 336,776 × 19\n   air_time_min  year month   day dep_time sched_dep_time dep_delay arr_time\n          &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1          227  2013     1     1      517            515         2      830\n 2          227  2013     1     1      533            529         4      850\n 3          160  2013     1     1      542            540         2      923\n 4          183  2013     1     1      544            545        -1     1004\n 5          116  2013     1     1      554            600        -6      812\n 6          150  2013     1     1      554            558        -4      740\n 7          158  2013     1     1      555            600        -5      913\n 8           53  2013     1     1      557            600        -3      709\n 9          140  2013     1     1      557            600        -3      838\n10          138  2013     1     1      558            600        -2      753\n# ℹ 336,766 more rows\n# ℹ 11 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nWhy doesn’t the following work, and what does the error mean?\nflights |&gt;    \n  select(tailnum) |&gt;    \n  arrange(arr_delay) \n\n#&gt; Error in `arrange()`: #&gt; ℹ In argument: `..1 = arr_delay`.\n#&gt; Caused by error: #&gt; ! object 'arr_delay' not found\nThe above code does not work because the select(tailnum) has removed all other variables (columns) from the tibble. Thus, when arrange(arr_delay) runs, it is unable to find any variable by the name of arr_delay in the tibble.\nThe error means that object (i.e. variable) 'arr_delay' has not been found in the tibble by the arrange() function.\n\n\n\n4.5.7 Exercises\n\nWhich carrier has the worst average delays? Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights |&gt; group_by(carrier, dest) |&gt; summarize(n()))\nThe overall carrier with worst average delays is F9, as seen from the code below:\n\nflights |&gt;\n  group_by(carrier) |&gt;\n  summarise(avg_delay = mean(arr_delay, na.rm = TRUE)) |&gt;\n  slice_max(order_by = avg_delay, n = 1)\n\n# A tibble: 1 × 2\n  carrier avg_delay\n  &lt;chr&gt;       &lt;dbl&gt;\n1 F9           21.9\n\n\nYes, we can disentangle the effect of bad airports vs. bad carriers using the code below:---\n\nflights |&gt;\n  group_by(dest, carrier) |&gt;\n  summarise(avg_delay = mean(arr_delay, na.rm = TRUE)) |&gt;\n  # taking the highest average delay flight at each airport\n  slice_max(order_by = avg_delay, n = 1) |&gt;\n  ungroup() |&gt;\n  # for each airline, summarize the number of airports where it is\n  # the most delayed airline\n  summarise(n = n(), .by = carrier) |&gt;\n  slice_head(n=5)|&gt;\n  arrange(desc(n)) |&gt;\n  rename(Carrier = carrier,\n         `Number of Airports` = n) |&gt;\n  gt()\n\n\n\n\n\nTable 5:  The airlines which have highest average delay at the maximum number\nof Airports \n  \n\n    \n      Carrier\n      Number of Airports\n    \n  \n  \n    EV\n42\n    B6\n20\n    UA\n14\n    AA\n6\n    FL\n2\n  \n\n\n\n\n\n\n\nFind the flights that are most delayed upon departure from each destination.\nWe can use the following codes (either one works, and the code checks this equality of results) to find the flight with highest departure delay (dep_delay ) for each destination (Table 6).\n\n\nCode\n# Option 1: Group by \"dest\" and then use slice_max\na = flights |&gt;\n  group_by(dest) |&gt;\n  slice_max(n=1, order_by = dep_delay, na_rm = TRUE) |&gt;\n  select(dest, flight, origin, dep_delay, month, day) |&gt;\n  arrange(desc(dep_delay), desc(flight))\n# Option 2: Directly use slice_max() with \"by\" argument\nb = flights |&gt;\n  slice_max(n=1, order_by = dep_delay, by = dest, na_rm = TRUE) |&gt;\n  select(dest, flight, origin, dep_delay, month, day) |&gt;\n  arrange(desc(dep_delay), desc(flight))\n# Check results\nsum(a != b)\n\n\n[1] 0\n\n\nCode\n# Display results\nb |&gt; \n  slice_head(n=5) |&gt; \n  gt() |&gt;\n  cols_label(dest = \"Destination\", \n             flight = \"Flight\", \n             origin = \"Origin Airport\",\n             dep_delay = \"Departure Depay (minutes)\", \n             month = \"Month\", \n             day = \"Date\") |&gt;\n  cols_align(align = \"center\")\n\n\n\n\n\n\nTable 6:  Flights with highest departure delay; displayed here only for 5\ndestinations highest departure delay \n  \n\n    \n      Destination\n      Flight\n      Origin Airport\n      Departure Depay (minutes)\n      Month\n      Date\n    \n  \n  \n    HNL\n51\nJFK\n1301\n1\n9\n    CMH\n3535\nJFK\n1137\n6\n15\n    ORD\n3695\nEWR\n1126\n1\n10\n    SFO\n177\nJFK\n1014\n9\n20\n    CVG\n3075\nJFK\n1005\n7\n22\n  \n\n\n\n\n\n\n\nHow do delays vary over the course of the day. Illustrate your answer with a plot.\nThe following graph of average delay (on y-axis) plotted against scheduled departure time (on x-axis) shows the overall trend that the average delays rise over the course of the day to hit a peak around 6 pm.\nNote: The scheduled departure time is not accurate in the data-set, since it is written in hhmm format, and thus is not continuous variable. For example, 1:59 am is 159, and then 2:00 am is 200. So there are no values in 60s, 70s, 80s, 90s. I rectified this using mathematical operators %/% and %% to obtain hours and minutes, and then combined them. Now, the result is a smoother graph.\n\nflights |&gt;\n  group_by(sched_dep_time) |&gt;\n  summarise(avg_delay = mean(dep_delay, na.rm = TRUE)) |&gt;\n  mutate(hour = sched_dep_time %/% 100,\n         minute = sched_dep_time %% 100) |&gt;\n  mutate(time_hr = hour + minute/60) |&gt;\n  ggplot(aes(x = time_hr, y = avg_delay)) +\n  geom_line() +\n  geom_smooth(color = \"red\", se = FALSE) +\n  theme_bw() + \n  labs(x = \"Scheduled Departure time (in Hrs.)\",\n       y = \"Average delay in minutes\") +\n  scale_x_continuous(breaks = seq(from = 0, to = 24, by = 4))\n\n\n\nFigure 1: Graph showing average delays over the course of the day at various scheduled times\n\n\n\n\n\nWhat happens if you supply a negative n to slice_min() and friends?\nThe inbuilt R help tells me that “A negative value of n will be subtracted from the group size. For example, n = -2 with a group of 5 rows will select 5 - 2 = 3 rows.”\nHere’s an example to explain. First, I create a tibble a (shown in Table 7) to contain the average departure delay from JFK airport to 10 destinations.\n\na = flights |&gt;\n  filter(origin == \"JFK\") |&gt;\n  group_by(origin, dest) |&gt;\n  summarise(avg_delay = mean(dep_delay, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_delay)) |&gt;\n  slice_head(n = 10) |&gt;\n  ungroup()\ngt(a) |&gt;\n  fmt_number(decimals = 2)\n\n\n\n\n\nTable 7:  The average departure delay from JFK airport to 10 destinations \n  \n\n    \n      origin\n      dest\n      avg_delay\n    \n  \n  \n    JFK\nCVG\n27.35\n    JFK\nSDF\n23.98\n    JFK\nEGE\n23.44\n    JFK\nSAT\n23.41\n    JFK\nMCI\n23.09\n    JFK\nCMH\n22.02\n    JFK\nORD\n21.55\n    JFK\nMSP\n21.33\n    JFK\nDEN\n20.10\n    JFK\nSTL\n20.00\n  \n\n\n\n\n\n\n\nNow, I use slice_min function with arguments n = 2 and then, with arguments n = -2 to show the difference in output. The first code n = 2 displays the two rows with minimum average delay. The second code, n = -2 displays the (total rows minus 2), i.e., 8 rows with minimum average delay.\n\n# n=2 displays the two rows with minimum average delay\na |&gt;\n  slice_min(n = 2, order_by = avg_delay)\n\n# A tibble: 2 × 3\n  origin dest  avg_delay\n  &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;\n1 JFK    STL        20  \n2 JFK    DEN        20.1\n\n# n=-2 displays the (total rows minus 2), i.e., 8 rows with minimum average delay\na |&gt;\n  slice_min(n = -2, order_by = avg_delay)\n\n# A tibble: 8 × 3\n  origin dest  avg_delay\n  &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;\n1 JFK    STL        20  \n2 JFK    DEN        20.1\n3 JFK    MSP        21.3\n4 JFK    ORD        21.6\n5 JFK    CMH        22.0\n6 JFK    MCI        23.1\n7 JFK    SAT        23.4\n8 JFK    EGE        23.4\n\n\nExplain what count() does in terms of the dplyr verbs you just learned. What does the sort argument to count() do?\nInstead of using the group_by() and summarize() verbs, the count() function can be used as a shortcut to quickly compute the number of unique values of each combination of a variable occurring in the data-set. Thus, count() helps us to calculate the number of values (rows) for each unique combination of variables which have been used as an argument in the count() function.\nThe inbuilt help in R tells us that df %&gt;% count(a, b) is roughly equivalent to df %&gt;% group_by(a, b) %&gt;% summarise(n = n()) .\nFurther, the sort = TRUE argument in count() tells R to display the largest groups (by count, i.e., n) to be displayed at the top.\nHere’s an example. The following code displays the 5 routes with maximum number of flights. For example, JFK to LAX had 11,262 flights in 2013. We can achieve this by using group_by(), summarize(), arrange() and ungroup(). Or, we can simply achieve the same result with a single function count().\n\nflights |&gt;\n  group_by(origin, dest) |&gt;\n  summarise(n = n()) |&gt;\n  arrange(desc(n)) |&gt;\n  ungroup() |&gt;\n  slice_head(n = 5)\n\n# A tibble: 5 × 3\n  origin dest      n\n  &lt;chr&gt;  &lt;chr&gt; &lt;int&gt;\n1 JFK    LAX   11262\n2 LGA    ATL   10263\n3 LGA    ORD    8857\n4 JFK    SFO    8204\n5 LGA    CLT    6168\n\nflights |&gt;\n  count(origin, dest, sort = TRUE) |&gt;\n  slice_head(n = 5)\n\n# A tibble: 5 × 3\n  origin dest      n\n  &lt;chr&gt;  &lt;chr&gt; &lt;int&gt;\n1 JFK    LAX   11262\n2 LGA    ATL   10263\n3 LGA    ORD    8857\n4 JFK    SFO    8204\n5 LGA    CLT    6168\n\n\nSuppose we have the following tiny data frame:\n\ndf &lt;- tibble(x = 1:5,   \n             y = c(\"a\", \"b\", \"a\", \"a\", \"b\"),   \n             z = c(\"K\", \"K\", \"L\", \"L\", \"K\") )\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what group_by() does.\ndf |&gt;   \n  group_by(y)\nIn my understanding, the output should look the same as df except that on top of it, a line mentioning that data is grouped by y should appear. When we run the code, it shows the following header # A tibble: 5 X 3 and #Groups: y[2] . Thus, there are two groups formed by two unique values of variable y , i.e., a and b .\n\ndf |&gt;\n  group_by(y)\n\n# A tibble: 5 × 3\n# Groups:   y [2]\n      x y     z    \n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 a     K    \n2     2 b     K    \n3     3 a     L    \n4     4 a     L    \n5     5 b     K    \n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what arrange() does. Also comment on how it’s different from the group_by() in part (a)?\ndf |&gt;   arrange(y)\nThe function arrange() re-orders the data-frame rows in ascending order of the variable mentioned, i.e. y . So, I expect the output to be the df tibble with ascending order of variable y . The ties will be arranged in the same order as they appeared in the original data-frame.\n\ndf |&gt;\n  arrange(y)\n\n# A tibble: 5 × 3\n      x y     z    \n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 a     K    \n2     3 a     L    \n3     4 a     L    \n4     2 b     K    \n5     5 b     K    \n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what the pipeline does.\ndf |&gt;   \n  group_by(y) |&gt;   \n  summarize(mean_x = mean(x))\nThe output should display the mean values of x for different values of y . For y = a , I expect mean_x = (1+3+4)/3 = 2.67 and for y = b , I expect mean_x = (2+5)/2 = 3.5 . I expect the output to be a 2 X 2 tibble with first column y and second column mean_x .\n\ndf |&gt;   \n  group_by(y) |&gt;   \n  summarize(mean_x = mean(x))\n\n# A tibble: 2 × 2\n  y     mean_x\n  &lt;chr&gt;  &lt;dbl&gt;\n1 a       2.67\n2 b       3.5 \n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what the pipeline does. Then, comment on what the message says.\ndf |&gt;   \n  group_by(y, z) |&gt;   \n  summarize(mean_x = mean(x))\nNow, I expect R to form groups of various combinations of y and z , and then display average value of x for each combination. The output should be a tibble of 3 X 3, and still containing two groups of y .\n\ndf |&gt;   \n  group_by(y, z) |&gt;   \n  summarize(mean_x = mean(x))\n\n# A tibble: 3 × 3\n# Groups:   y [2]\n  y     z     mean_x\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 a     K        1  \n2 a     L        3.5\n3 b     K        3.5\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what the pipeline does. How is the output different from the one in part (d).\ndf |&gt;   \n  group_by(y, z) |&gt;   \n  summarize(mean_x = mean(x), .groups = \"drop\")\nI think the output will still be a 3 X 3 tibble with same values as answer from Question 6 (d), i.e. displaying average values of x for different combinations of y and z . But, now the remaining grouping, i.e., of y will be dropped from the output. So the output is visually the same, but now it is an un-grouped tibble, rather than the grouped tibble output of Question 6 (d).\n\ndf |&gt;   \n  group_by(y, z) |&gt;   \n  summarize(mean_x = mean(x), .groups = \"drop\")\n\n# A tibble: 3 × 3\n  y     z     mean_x\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 a     K        1  \n2 a     L        3.5\n3 b     K        3.5\n\n\nWrite down what you think the outputs will look like, then check if you were correct, and describe what each pipeline does. How are the outputs of the two pipelines different?\n# Code Chunk 1\ndf |&gt;   \n  group_by(y, z) |&gt;   \n  summarize(mean_x = mean(x))  \n\n# Code Chunk 2\ndf |&gt;   \n  group_by(y, z) |&gt;   \n  mutate(mean_x = mean(x))\nThe answers should be different because summarize() collapses all the rows for a unique combination of grouped variables to produce one summary row. On the other hand, mutate() preserves each row of the original data-frame (or, tibble) and produces and additional variable with mean of x to be entered in each row.\nThus, I expect the # Code Chunk 1 to generate a tibble of 3 X 3 (like the output in Question 6(d), while I expect the # Code Chunk 2 to generate a tibble of 5 X 4, with the 4th column of mean_x having different values for each unique combination of y and z .\nFurther, I expect that the # Code Chunk 1 will re-order the output in ascending of order grouping variables. But, the # Code Chunk 2 will preserve the original ordering of the rows as in the original df tibble.\n\n# Code Chunk 1\ndf |&gt;   \n  group_by(y, z) |&gt;   \n  summarize(mean_x = mean(x))  \n\n# A tibble: 3 × 3\n# Groups:   y [2]\n  y     z     mean_x\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 a     K        1  \n2 a     L        3.5\n3 b     K        3.5\n\n# Code Chunk 2\ndf |&gt;   \n  group_by(y, z) |&gt;   \n  mutate(mean_x = mean(x))\n\n# A tibble: 5 × 4\n# Groups:   y, z [3]\n      x y     z     mean_x\n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1     1 a     K        1  \n2     2 b     K        3.5\n3     3 a     L        3.5\n4     4 a     L        3.5\n5     5 b     K        3.5\n\n\nNote: It is only by chance that the mean_x is 3.5 for both combinations:\n\ny=b, z=K , mean_x = (2+5)/2 = 3.5\ny=a, z=L , mean_x = (3+4)/2 = 3.5"
  },
  {
    "objectID": "Chapter3.html",
    "href": "Chapter3.html",
    "title": "Chapter 3",
    "section": "",
    "text": "Some important tips:\n\nUse Alt + - to write the assignment operator &lt;- in R.\nIn the comments, i.e, text written after # in code, explain the WHY of your code, not the WHAT or HOW.\n\n\n\n3.5 Exercises\n\nWhy does this code not work?\nmy_variable &lt;- 10 \nmy_varıable \n#&gt; Error in eval(expr, envir, enclos): object 'my_varıable' not found\nLook carefully! (This may seem like an exercise in pointlessness, but training your brain to notice even the tiniest difference will pay off when programming.)\nThe code does not work because of the minor spelling difference, i.e., i vs. ī .\nTweak each of the following R commands so that they run correctly:\nlibary(todyverse)  \nggplot(dTA = mpg) +    \n  geom_point(maping = aes(x = displ y = hwy)) +   \n  geom_smooth(method = \"lm)\nThe corrected code is as follows:---\n\nlibrary(tidyverse)  \nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +    \n  geom_point() +   \n  geom_smooth(method = \"lm\")\n\nPress Option + Shift + K / Alt + Shift + K. What happens? How can you get to the same place using the menus?\nThe Alt + Shift + K shortcut brings up the Keyboard Shortcut Quick Reference. We could get to the same using menus as Help –&gt; Keyboard Shortcuts Help.\nLet’s revisit an exercise from the Section 2.6. Run the following lines of code. Which of the two plots is saved as mpg-plot.png? Why?\nmy_bar_plot &lt;- ggplot(mpg, aes(x = class)) +   \n  geom_bar() \n\nmy_scatter_plot &lt;- ggplot(mpg, aes(x = cty, y = hwy)) +\n  geom_point() \n\nggsave(filename = \"mpg-plot.png\", plot = my_bar_plot)\nThis time, the bar plot, i.e. my_bar_plot is saved into the file mpg-plot.png because in the arguments to the function ggsave() we have specified the name of the plot. The plot argument tells ggsave() the Plot to save, and by default, it goes to the last plot displayed."
  },
  {
    "objectID": "Chapter5.html",
    "href": "Chapter5.html",
    "title": "Chapter 5",
    "section": "",
    "text": "5.6 Exercises\n\nRestyle the following pipelines following the guidelines above.\nflights|&gt;filter(dest==\"IAH\")|&gt;group_by(year,month,day)|&gt;summarize(n=n(),\ndelay=mean(arr_delay,na.rm=TRUE))|&gt;filter(n&gt;10)\n\nflights|&gt;filter(carrier==\"UA\",dest%in%c(\"IAH\",\"HOU\"),sched_dep_time&gt;\n0900,sched_arr_time&lt;2000)|&gt;group_by(flight)|&gt;summarize(delay=mean(\narr_delay,na.rm=TRUE),cancelled=sum(is.na(arr_delay)),n=n())|&gt;filter(n&gt;10)\nThe restyled code is as below:---\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nflights |&gt;\n  filter(dest == \"IAH\") |&gt;\n  group_by(year, month, day) |&gt;\n  summarize(\n    n = n(),\n    delay = mean(arr_delay, na.rm = TRUE)\n  ) |&gt;\n  filter(n &gt; 10)\n\nflights |&gt;\n  filter(\n    carrier == \"UA\",\n    dest %in% c(\"IAH\", \"HOU\"),\n    sched_dep_time &gt; 0900,\n    sched_arr_time &lt; 2000\n  ) |&gt;\n  group_by(flight) |&gt;\n  summarize(\n    delay = mean(arr_delay, na.rm = TRUE),\n    cancelled = sum(is.na(arr_delay)), n = n()\n  ) |&gt;\n  filter(n &gt; 10)\n\nLet us try to use the styler package for the same task, using Ctrl + Shift + P . The styled code using styler is shown below:---\n\nflights |&gt;\n  filter(dest == \"IAH\") |&gt;\n  group_by(year, month, day) |&gt;\n  summarise(\n    n = n(),\n    delay = mean(arr_delay, na.rm = TRUE)\n  ) |&gt;\n  filter(n &gt; 10)\n\nflights |&gt;\n  filter(carrier == \"UA\", dest %in% c(\"IAH\", \"HOU\"), sched_dep_time &gt;\n    0900, sched_arr_time &lt; 2000) |&gt;\n  group_by(flight) |&gt;\n  summarise(delay = mean(\n    arr_delay,\n    na.rm = TRUE\n  ), cancelled = sum(is.na(arr_delay)), n = n()) |&gt;\n  filter(n &gt; 10)"
  },
  {
    "objectID": "Chapter6.html",
    "href": "Chapter6.html",
    "title": "Chapter 6",
    "section": "",
    "text": "6.2.1 Exercises\n\nQuestion 1\nFor each of the sample tables, describe what each observation and each column represents.\nFor table1 , the following columns represent:--\n\nCountry\nYear of the observation of cases and population\nNumber of cases\nTotal Population for that year\n\nFor table1 , each observation represents number of cases and total population for a country in a given year.\n\nFor table2 , the following columns represent:--\n\nCountry\nYear of the Observation\nWhich type of variable is represented in column 4 - i.e., cases or population. Thus, this column in itself is not a variable. Thus, this data-set is not tidy.\nThe actual value (i.e. observation) of the variable mentioned in Column 3.\n\nFor table2 , each observation represents either the number of cases or the total population for a country in a given year.\n\nFor table3 , the columns represent the following:--\n\nCountry\nYear of the observation\nThe ratio of two observations, i.e. rate = cases divided by the population. Thus, the column 3 represents two observations, not one. Hence, the data is not tidy.\n\nFor table3 , each observation is a rate, i.e., actually it is a ratio of two observations, namely, cases and population.\n\n\nQuestion 2\nSketch out the process you’d use to calculate the rate for table2 and table3. You will need to perform four operations:\n\na.\nExtract the number of TB cases per country per year.\nFor table2 , we will have to filter out rows where type == \"cases\" .\nFor table3 , we will have to extract the numerator from rate variable for each row.\n\n\nb.\nExtract the matching population per country per year.\nFor table2 , we will have to filter out rows where type == \"population\" .\nFor table3 , we will have to extract the denominator from rate variable for each row.\n\n\nc.\nDivide cases by population, and multiply by 10000.\nFor table2 , we will have to divide the observations from question 2 (a) by observations from question 2(b). We might also want to check that the year and country match, row by row.\nFor table3 , we can divide the numerator by denominator, and multiply by 10,000. Or simply calculate the expression in rate column, as a numeric.\n\n\nd.\nStore back in the appropriate place.\nFor table2 , we will have to re-save the rates in a new set of rows, where type == \"rate\" and count will be the calculated rate. Thus, the table2 will have 6 new rows.\nFor table3 , we will have to convert rate column into numeric, to get the ratio per 10,000. But we will end up losing information, i.e. the cases and population of each country for different years will be lost if data is reported directly as rate .\n\n\n\n\nSection 6.3.4\nData and variable names in the column headers\nHere’s an attempt to recreate \".values\" argument method in R :---\n\nlibrary(tidyverse)\nhousehold |&gt;\n  pivot_longer(\n    cols = !family,\n    names_to = c(\".value\", \"child\"),\n    names_sep = \"_\",\n    values_to = \"Value\",\n    values_drop_na = TRUE\n  )\n\n# A tibble: 9 × 4\n  family child  dob        name  \n   &lt;int&gt; &lt;chr&gt;  &lt;date&gt;     &lt;chr&gt; \n1      1 child1 1998-11-26 Susan \n2      1 child2 2000-01-29 Jose  \n3      2 child1 1996-06-22 Mark  \n4      3 child1 2002-07-11 Sam   \n5      3 child2 2004-04-05 Seth  \n6      4 child1 2004-10-10 Craig \n7      4 child2 2009-08-27 Khai  \n8      5 child1 2000-12-05 Parker\n9      5 child2 2005-02-28 Gracie\n\n\n\n\nSection 6.4.1\nHow does the pivot_wider() work?\nHere I try to understand what is the output from pivot_wider() is there are more than 1 unique values for a measurement, i.e.. there are two bp1 ’s for A .\n\ndf &lt;- tribble(\n  ~id, ~measurement, ~value,\n  \"A\",        \"bp1\",    100,\n  \"A\",        \"bp1\",    102,\n  \"A\",        \"bp2\",    120,\n  \"B\",        \"bp1\",    140, \n  \"B\",        \"bp2\",    115\n)\ndf |&gt;\n  pivot_wider(\n    id_cols = id,\n    names_from = measurement,\n    values_from = value\n  )\n## Warning: Values from `value` are not uniquely identified; output will contain list-cols.\n## • Use `values_fn = list` to suppress this warning.\n## • Use `values_fn = {summary_fun}` to summarise duplicates.\n## • Use the following dplyr code to identify duplicates.\n##   {data} %&gt;%\n##   dplyr::group_by(id, measurement) %&gt;%\n##   dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %&gt;%\n##   dplyr::filter(n &gt; 1L)\n## # A tibble: 2 × 3\n##   id    bp1       bp2      \n##   &lt;chr&gt; &lt;list&gt;    &lt;list&gt;   \n## 1 A     &lt;dbl [2]&gt; &lt;dbl [1]&gt;\n## 2 B     &lt;dbl [1]&gt; &lt;dbl [1]&gt;\n\n# Using the Code given by R in Warning to find out the \n# duplicate observation\ndf |&gt;\n  group_by(id, measurement) |&gt;\n  summarise(n = n(), .groups = \"drop\") |&gt;\n  filter(n &gt; 1)\n## # A tibble: 1 × 3\n##   id    measurement     n\n##   &lt;chr&gt; &lt;chr&gt;       &lt;int&gt;\n## 1 A     bp1             2"
  },
  {
    "objectID": "Chapter7.html",
    "href": "Chapter7.html",
    "title": "Chapter 7",
    "section": "",
    "text": "7.3 Exercises\n\nQuestion 1\nGo to the RStudio Tips Twitter account, https://twitter.com/rstudiotips and find one tip that looks interesting. Practice using it!\nOne tip that I found interesting is the use of Ctrl + Shift + P to open the R-Studio Command Palette. I have embedded the tweet here by inserting a simple HTML block, and pasting the HTML code from public.twitter.com link.\nAccess it with Ctrl + Shift + P (Windows / Linux) or Cmd + Shift + P (macOS)!#rstats https://t.co/pWAYHGCWRr— RStudio Tips (@rstudiotips) October 26, 2022 \n\n\nQuestion 2\nWhat other common mistakes will RStudio diagnostics report? Read https://support.posit.co/hc/en-us/articles/205753617-Code-Diagnostics to find out.\nSome of the common mistakes that RStudio diagnostics will report are:--\n\nIt can detect if a variable named in a function has not yet been defined (i.e. the variable used has no definition in scope), or is misspelt.\nIt can detect if a variable has been defined, but is not being used within a function.\nIt can detect the missing punctuation, i.e. a missing comma or missing brackets.\nIt can detect whether the call within a function can work or not, i.e., whether the arguments to a function are matched, partially matched or unmatched.\nIt can detect if an essential argument to a function is missing.\nIt can provide us R code style diagnostics, e.g. white-space etc.\nIt can even detect mistakes in other languages such as C , C++ , JavaScript and Python ."
  },
  {
    "objectID": "Chapter8.html",
    "href": "Chapter8.html",
    "title": "Chapter 8",
    "section": "",
    "text": "8.2.4 Exercises\n\nlibrary(tidyverse)\nlibrary(gt)\n\n\nQuestion 1\nWhat function would you use to read a file where fields were separated with “|”?\nLet us first create a data set with “|” delimiter. I used ChatGPT to create a random data set and named it as the data to be imported, i.e., imp_df .\nI will use the function read_delim() to read a file where fields were separated with “|”, as shown below. The output is\n\n#| label: tbl-q1-Ex-8.2.4\n#| tbl-cap: \"Imported data using read_delim() function\"\n\nimport_df = \"Name|Age|Gender|City|Salary\nJohn|28|Male|New York|75000\nEmily|22|Female|Los Angeles|60000\nMichael|31|Male|Chicago|80000\nJessica|25|Female|Houston|65000\nWilliam|29|Male|Miami|70000\nSophia|27|Female|San Francisco|75000\nDaniel|24|Male|Seattle|72000\nOlivia|30|Female|Boston|78000\nJames|26|Male|Dallas|67000\nAva|23|Female|Atlanta|62000\"\n\ndf = read_delim(import_df, delim = \"|\")\n\ndf |&gt;\n  gt()\n\n\n\n\n\n  \n    \n    \n      Name\n      Age\n      Gender\n      City\n      Salary\n    \n  \n  \n    John\n28\nMale\nNew York\n75000\n    Emily\n22\nFemale\nLos Angeles\n60000\n    Michael\n31\nMale\nChicago\n80000\n    Jessica\n25\nFemale\nHouston\n65000\n    William\n29\nMale\nMiami\n70000\n    Sophia\n27\nFemale\nSan Francisco\n75000\n    Daniel\n24\nMale\nSeattle\n72000\n    Olivia\n30\nFemale\nBoston\n78000\n    James\n26\nMale\nDallas\n67000\n    Ava\n23\nFemale\nAtlanta\n62000\n  \n  \n  \n\n\n\n\nNote: The same read_delim() function will even work without the argument delim = \"|\" because it has in-built capacity to identify the delimiter.\n\n\nQuestion 2\nApart from file, skip, and comment, what other arguments do read_csv() and read_tsv() have in common?\nWhile read_csv() works for comma separated files, read_tsv() works for tab-separated files. The arguments for each of them are as follows:-\n\n\n\nArguments for read_csv\n\nfile,\ncol_names = TRUE,\ncol_types = NULL,\ncol_select = NULL,\nid = NULL,\nlocale = default_locale(),\nna = c(\"\", \"NA\"),\nquoted_na = TRUE,\nquote = \"\\\"\",\ncomment = \"\",\ntrim_ws = TRUE,\nskip = 0,\nn_max = Inf,\nguess_max = min(1000, n_max),\nname_repair = \"unique\",\nnum_threads = readr_threads(),\nprogress = show_progress(),\nshow_col_types = should_show_types(),\nskip_empty_rows = TRUE,\nlazy = should_read_lazy()\n\n\n\nArguments for read_tsv\n\nfile,\ncol_names = TRUE,\ncol_types = NULL,\ncol_select = NULL,\nid = NULL,\nlocale = default_locale(),\nna = c(\"\", \"NA\"),\nquoted_na = TRUE,\nquote = \"\\\"\",\ncomment = \"\",\ntrim_ws = TRUE,\nskip = 0,\nn_max = Inf,\nguess_max = min(1000, n_max),\nname_repair = \"unique\",\nnum_threads = readr_threads(),\nprogress = show_progress(),\nshow_col_types = should_show_types(),\nskip_empty_rows = TRUE,\nlazy = should_read_lazy()\n\n\n\n\nThus, all the arguments to both the functions are common, and have the exact same role.\n\n\nQuestion 3\nWhat are the most important arguments to read_fwf()?\nThe fixed width files are very fast to parse because each field will be in exact sample place in each line. However, this means, we must know the exact width of each column. Hence, the most important argument to read_fwf() is the cols_position = , which can take the following values:\n\nfwf_empty() - Guesses based on the positions of empty columns.\nfwf_widths() - Supply the widths of the columns.\nfwf_positions() - Supply paired vectors of start and end positions.\nfwf_cols() - Supply named arguments of paired start and end positions or column widths.\n\nAlso, another important argument is cols_types which will tell whether each column will be of which class - character, integer, factor etc.\nHere’s an example shown in Table 1 .\n\nimport_fwf_data = \"John    Smith   35  New York\nAlice   Johnson 28  Los Angeles\nMichael Williams 42  Chicago\"\n\ndf2 = read_fwf(\n  import_fwf_data,\n  col_positions = fwf_widths(c(8, 8, 3, 12))\n) \n\ncolnames(df2) = c(\"Name\", \"Surname\", \"Age\", \"City\")\n\ndf2 |&gt;\n  gt()\n\n\n\n\n\nTable 1:  Fixed Width File Data parsed using read_fwf \n  \n    \n    \n      Name\n      Surname\n      Age\n      City\n    \n  \n  \n    John\nSmith\n35\nNew York\n    Alice\nJohnson\n28\nLos Angeles\n    Michael\nWilliams\n42\nChicago\n  \n  \n  \n\n\n\n\n\n\n\nQuestion 4\nSometimes strings in a CSV file contain commas. To prevent them from causing problems, they need to be surrounded by a quoting character, like ” or ’. By default, read_csv() assumes that the quoting character will be “. To read the following text into a data frame, what argument to read_csv() do you need to specify?\n\"x,y\\n1,'a,b'\"\nTo read a data above text into a data-frame, we will need to used the argument quote = \"'\" . Here’s an example in Table 2 .\n\nimport_quote = \"x,y\\n1,'a,b'\"\n\nread_csv(\n  import_quote,\n  quote = \"'\",\n  col_names = FALSE\n) |&gt;\n  gt()\n\n\n\n\n\nTable 2:  A data-frame imported from csv file with different quotes \n  \n    \n    \n      X1\n      X2\n    \n  \n  \n    x\ny\n    1\na,b\n  \n  \n  \n\n\n\n\n\n\n\nQuestion 5\nIdentify what is wrong with each of the following inline CSV files. What happens when you run the code?\n\nread_csv(\"a,b\\n1,2,3\\n4,5,6\") : This data is not rectangular, there are only two columns in first row, but three in other two rows. Thus, R ends up reading only two columns by default and joins the second and third column values for the two observations.\n\nread_csv(\"a,b\\n1,2,3\\n4,5,6\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 2 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): a\nnum (1): b\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 2\n      a     b\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1    23\n2     4    56\n\n\nread_csv(\"a,b,c\\n1,2\\n1,2,3,4\"): This data is again not rectangular, there are three columns (column names) in first row, but two values in second row, and four in the third row. Thus, R ends up reading three columns by default, creates an NA and joins the second and third column values for the second row.\n\nread_csv(\"a,b,c\\n1,2\\n1,2,3,4\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 2 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): a, b\nnum (1): c\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 3\n      a     b     c\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     2    NA\n2     1     2    34\n\n\nread_csv(\"a,b\\n\\\"1\"): This data is entered wrong, as the double quotes don’t match up in pairs, i.e., there are three double quotes (\"), so R will read only the data between first two, i.e, a and b as variable names, and the data-frame will be empty. An error with also be displayed, as shown below:---\n\nread_csv(\"a,b\\n\\\"1\")\n\nRows: 0 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): a, b\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 0 × 2\n# ℹ 2 variables: a &lt;chr&gt;, b &lt;chr&gt;\n\n\nread_csv(\"a,b\\n1,2\\na,b\"): This data is rectangular, but the names of the columns are repeated in the second row, i.e. the second observation. Further, the data in each column is not of a single type, i.e. either &lt;chr&gt; or &lt;dbl&gt; or &lt;int&gt; etc. Thus, each column is not a variable, and each row is not an observation. The data is not tidy.\n\nread_csv(\"a,b\\n1,2\\na,b\")\n\nRows: 2 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): a, b\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 2\n  a     b    \n  &lt;chr&gt; &lt;chr&gt;\n1 1     2    \n2 a     b    \n\n\nread_csv(\"a;b\\n1;3\"): This data is wrong coded, i.e. it is not a comma-separated data, rather it is a semi-colon-separated data. Thus, read_csv() will end up reading a;b as a single string (i.e., column name) and the 1;3 as the single observation, i.e. first row. Instead, we should have used read_csv2() here.\n\nread_csv(\"a;b\\n1;3\")\n\nRows: 1 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): a;b\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 1 × 1\n  `a;b`\n  &lt;chr&gt;\n1 1;3  \n\n\nUsing read_csv2() , the data is read-in correctly:---\n\nread_csv2(\"a;b\\n1;3\")\n\n# A tibble: 1 × 2\n      a     b\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     3\n\n\n\n\n\nQuestion 6\nPractice referring to non-syntactic names in the following data frame by:\n\nannoying &lt;- tibble(\n  `1` = 1:10,\n  `2` = `1` * 2 + rnorm(length(`1`))\n)\nannoying\n\n# A tibble: 10 × 2\n     `1`   `2`\n   &lt;int&gt; &lt;dbl&gt;\n 1     1  4.42\n 2     2  5.82\n 3     3  6.75\n 4     4  8.48\n 5     5 10.3 \n 6     6 12.9 \n 7     7 14.4 \n 8     8 16.7 \n 9     9 16.8 \n10    10 18.6 \n\n\n\nExtracting the variable called 1.\n\nannoying |&gt;\n  pull(`1`)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nPlotting a scatterplot of 1 vs. 2.\n\nannoying |&gt;\n  ggplot(aes(x = `1`, y = `2`)) +\n  geom_point() +\n  theme_classic()\n\n\n\n\nCreating a new column called 3, which is 2 divided by 1.\n\nannoying = annoying |&gt;\n  mutate(`3` = `2` / `1`)\nannoying\n\n# A tibble: 10 × 3\n     `1`   `2`   `3`\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1  4.42  4.42\n 2     2  5.82  2.91\n 3     3  6.75  2.25\n 4     4  8.48  2.12\n 5     5 10.3   2.05\n 6     6 12.9   2.14\n 7     7 14.4   2.06\n 8     8 16.7   2.09\n 9     9 16.8   1.86\n10    10 18.6   1.86\n\n\nRenaming the columns to one, two, and three.\n\nannoying |&gt;\n  rename(\n    \"one\" = `1`,\n    \"two\" = `2`,\n    \"three\" = `3`\n  )\n\n# A tibble: 10 × 3\n     one   two three\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1  4.42  4.42\n 2     2  5.82  2.91\n 3     3  6.75  2.25\n 4     4  8.48  2.12\n 5     5 10.3   2.05\n 6     6 12.9   2.14\n 7     7 14.4   2.06\n 8     8 16.7   2.09\n 9     9 16.8   1.86\n10    10 18.6   1.86\n\n\n\n\n\n\n8.3 Controlling column types\nHere’s an example data set to use the arguments col_types and na associated with the powerful read_csv() function.\n\nraw_df1 = \"Name,Age,Value,DateTime,Flag\nJohn Doe,25,123.45,2023-08-07 10:30:00,True\nJane Smith,42,987.65,2023-08-06 15:45:00,False\nBob Johnson,32,543.21,2023-08-05 08:00:00,True\nMary Williams,28,.,2023-08-04 12:15:00,False\nMichael Brown,,789.01,2023-08-03 18:30:00,True\nEmily Davis,38,234.56,,False\nDavid Lee,50,.,2023-08-01 09:45:00,True\n.,22,345.67,2023-07-31 14:00:00,False\"\n\nread_csv(raw_df1)\n\n# A tibble: 8 × 5\n  Name            Age Value  DateTime            Flag \n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;  &lt;dttm&gt;              &lt;lgl&gt;\n1 John Doe         25 123.45 2023-08-07 10:30:00 TRUE \n2 Jane Smith       42 987.65 2023-08-06 15:45:00 FALSE\n3 Bob Johnson      32 543.21 2023-08-05 08:00:00 TRUE \n4 Mary Williams    28 .      2023-08-04 12:15:00 FALSE\n5 Michael Brown    NA 789.01 2023-08-03 18:30:00 TRUE \n6 Emily Davis      38 234.56 NA                  FALSE\n7 David Lee        50 .      2023-08-01 09:45:00 TRUE \n8 .                22 345.67 2023-07-31 14:00:00 FALSE\n\n\nHere, we see that by default, read_csv() does an amazing job. It identifies most column types, but fails to understand that “.” is a missing value in Value variable, which is otherwise numerical.\nLet’s improve this behavior.\n\nread_csv(\n  raw_df1,\n  na = \".\"\n)\n\n# A tibble: 8 × 5\n  Name            Age Value DateTime            Flag \n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt;              &lt;lgl&gt;\n1 John Doe         25  123. 2023-08-07 10:30:00 TRUE \n2 Jane Smith       42  988. 2023-08-06 15:45:00 FALSE\n3 Bob Johnson      32  543. 2023-08-05 08:00:00 TRUE \n4 Mary Williams    28   NA  2023-08-04 12:15:00 FALSE\n5 Michael Brown    NA  789. 2023-08-03 18:30:00 TRUE \n6 Emily Davis      38  235. NA                  FALSE\n7 David Lee        50   NA  2023-08-01 09:45:00 TRUE \n8 &lt;NA&gt;             22  346. 2023-07-31 14:00:00 FALSE\n\n\nNow, let’s use col_types argument to force some variables into certain classes we desire. Here I will try to force Age into an integer, Value into a number (i.e., &lt;dbl&gt;), and DateTime into a character, and Flag into a character.\n\nread_csv(\n  raw_df1,\n  na = \".\",\n  col_types = list(\n    Name = col_character(),\n    Age = col_integer(),\n    Value = col_double(),\n    DateTime = col_character()\n  )\n)\n\n# A tibble: 8 × 5\n  Name            Age Value DateTime              Flag \n  &lt;chr&gt;         &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;                 &lt;lgl&gt;\n1 John Doe         25  123. \"2023-08-07 10:30:00\" TRUE \n2 Jane Smith       42  988. \"2023-08-06 15:45:00\" FALSE\n3 Bob Johnson      32  543. \"2023-08-05 08:00:00\" TRUE \n4 Mary Williams    28   NA  \"2023-08-04 12:15:00\" FALSE\n5 Michael Brown    NA  789. \"2023-08-03 18:30:00\" TRUE \n6 Emily Davis      38  235. \"\"                    FALSE\n7 David Lee        50   NA  \"2023-08-01 09:45:00\" TRUE \n8 &lt;NA&gt;             22  346. \"2023-07-31 14:00:00\" FALSE\n\n\nNow, I will try to read-in only a few columns to save on memory space in R by using col_skip().\n\nread_csv(\n  raw_df1,\n  na = \".\",\n  col_types = list(\n    Name = col_character(),\n    Age = col_integer(),\n    Value = col_double(),\n    DateTime = col_skip(),\n    Flag = col_skip()\n  )\n)\n\n# A tibble: 8 × 3\n  Name            Age Value\n  &lt;chr&gt;         &lt;int&gt; &lt;dbl&gt;\n1 John Doe         25  123.\n2 Jane Smith       42  988.\n3 Bob Johnson      32  543.\n4 Mary Williams    28   NA \n5 Michael Brown    NA  789.\n6 Emily Davis      38  235.\n7 David Lee        50   NA \n8 &lt;NA&gt;             22  346.\n\n\nNow, I will repeat this using cols_only() function:---\n\nread_csv(\n  raw_df1,\n  na = \".\",\n  col_types = cols_only(\n    Name = col_character(), \n    Age = col_integer(), \n    Value = col_double())\n)\n\n# A tibble: 8 × 3\n  Name            Age Value\n  &lt;chr&gt;         &lt;int&gt; &lt;dbl&gt;\n1 John Doe         25  123.\n2 Jane Smith       42  988.\n3 Bob Johnson      32  543.\n4 Mary Williams    28   NA \n5 Michael Brown    NA  789.\n6 Emily Davis      38  235.\n7 David Lee        50   NA \n8 &lt;NA&gt;             22  346.\n\n\nNow, let’s try to read-in data from three different files at the same time..\n\nread_csv(\n  c(\n    \"https://pos.it/r4ds-01-sales\", \n    \"https://pos.it/r4ds-02-sales\", \n    \"https://pos.it/r4ds-03-sales\"\n  ),\n  na = \".\",\n  id = \"file\"\n) |&gt;\n  slice_head(n = 5)\n\n# A tibble: 5 × 6\n  file                         month    year brand  item     n\n  &lt;chr&gt;                        &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 https://pos.it/r4ds-01-sales January  2019     1  1234     3\n2 https://pos.it/r4ds-01-sales January  2019     1  8721     9\n3 https://pos.it/r4ds-01-sales January  2019     1  1822     2\n4 https://pos.it/r4ds-01-sales January  2019     2  3333     1\n5 https://pos.it/r4ds-01-sales January  2019     2  2156     9"
  },
  {
    "objectID": "Chapter9.html#making-a-reprex",
    "href": "Chapter9.html#making-a-reprex",
    "title": "Chapter 9",
    "section": "9.2 Making a reprex",
    "text": "9.2 Making a reprex\nHere, I try to create a reprex for a deliberate mistake I am making in ggplot2 :--\nFirst, I create the faulty ggplot2 code:--\n\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(gt)\ndata(\"gtcars\")\ngtcars |&gt;\n  slice_head(n = 3, \n             by = ctry_origin) |&gt;\n  ggplot(aes(x = hp,\n             y = mpg_h,\n             color = ctry_origin,\n             label = model)) +\n  geom_point() +\n  geom_text_repel(force = 2) +\n  theme_classic() +\n  labs(x = \"Horse Power\",\n       y = \"Miles per Gallon (Highway)\") +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n\n\n# Copy the code above\n\n# Then create reprex data set\ngtcars |&gt;\n  slice_head(n = 3, \n             by = ctry_origin) |&gt;\n  dput()\n\nHere’s the reprex:\n\ngtcars = \nstructure(list(mfr = c(\"Ford\", \"Chevrolet\", \"Dodge\", \"Ferrari\", \n\"Ferrari\", \"Ferrari\", \"Acura\", \"Nissan\", \"Bentley\", \"Aston Martin\", \n\"Aston Martin\", \"BMW\", \"BMW\", \"BMW\"), model = c(\"GT\", \"Corvette\", \n\"Viper\", \"458 Speciale\", \"458 Spider\", \"458 Italia\", \"NSX\", \"GT-R\", \n\"Continental GT\", \"DB11\", \"Rapide S\", \"6-Series\", \"i8\", \"M4\"), \n    year = c(2017, 2016, 2017, 2015, 2015, 2014, 2017, 2016, \n    2016, 2017, 2016, 2016, 2016, 2016), trim = c(\"Base Coupe\", \n    \"Z06 Coupe\", \"GT Coupe\", \"Base Coupe\", \"Base\", \"Base Coupe\", \n    \"Base Coupe\", \"Premium Coupe\", \"V8 Coupe\", \"Base Coupe\", \n    \"Base Sedan\", \"640 I Coupe\", \"Mega World Coupe\", \"Base Coupe\"\n    ), bdy_style = c(\"coupe\", \"coupe\", \"coupe\", \"coupe\", \"convertible\", \n    \"coupe\", \"coupe\", \"coupe\", \"coupe\", \"coupe\", \"sedan\", \"coupe\", \n    \"coupe\", \"coupe\"), hp = c(647, 650, 645, 597, 562, 562, 573, \n    545, 500, 608, 552, 315, 357, 425), hp_rpm = c(6250, 6400, \n    5000, 9000, 9000, 9000, 6500, 6400, 6000, 6500, 6650, 5800, \n    5800, 5500), trq = c(550, 650, 600, 398, 398, 398, 476, 436, \n    487, 516, 465, 330, 420, 406), trq_rpm = c(5900, 3600, 5000, \n    6000, 6000, 6000, 2000, 3200, 1700, 1500, 5500, 1400, 3700, \n    1850), mpg_c = c(11, 15, 12, 13, 13, 13, 21, 16, 15, 15, \n    14, 20, 28, 17), mpg_h = c(18, 22, 19, 17, 17, 17, 22, 22, \n    25, 21, 21, 30, 29, 24), drivetrain = c(\"rwd\", \"rwd\", \"rwd\", \n    \"rwd\", \"rwd\", \"rwd\", \"awd\", \"awd\", \"awd\", \"rwd\", \"rwd\", \"rwd\", \n    \"awd\", \"rwd\"), trsmn = c(\"7a\", \"7m\", \"6m\", \"7a\", \"7a\", \"7a\", \n    \"9a\", \"6a\", \"8am\", \"8am\", \"8am\", \"8am\", \"6am\", \"6m\"), ctry_origin = c(\"United States\", \n    \"United States\", \"United States\", \"Italy\", \"Italy\", \"Italy\", \n    \"Japan\", \"Japan\", \"United Kingdom\", \"United Kingdom\", \"United Kingdom\", \n    \"Germany\", \"Germany\", \"Germany\"), msrp = c(447000, 88345, \n    95895, 291744, 263553, 233509, 156000, 101770, 198500, 211195, \n    205300, 77300, 140700, 65700)), class = c(\"spec_tbl_df\", \n\"tbl_df\", \"tbl\", \"data.frame\"), row.names = c(NA, -14L), spec = structure(list(\n    cols = list(mfr = structure(list(), class = c(\"collector_character\", \n    \"collector\")), model = structure(list(), class = c(\"collector_character\", \n    \"collector\")), year = structure(list(), class = c(\"collector_double\", \n    \"collector\")), trim = structure(list(), class = c(\"collector_character\", \n    \"collector\")), bdy_style = structure(list(), class = c(\"collector_character\", \n    \"collector\")), hp = structure(list(), class = c(\"collector_double\", \n    \"collector\")), hp_rpm = structure(list(), class = c(\"collector_double\", \n    \"collector\")), trq = structure(list(), class = c(\"collector_double\", \n    \"collector\")), trq_rpm = structure(list(), class = c(\"collector_double\", \n    \"collector\")), mpg_c = structure(list(), class = c(\"collector_double\", \n    \"collector\")), mpg_h = structure(list(), class = c(\"collector_double\", \n    \"collector\")), drivetrain = structure(list(), class = c(\"collector_character\", \n    \"collector\")), trsmn = structure(list(), class = c(\"collector_character\", \n    \"collector\")), ctry_origin = structure(list(), class = c(\"collector_character\", \n    \"collector\")), msrp = structure(list(), class = c(\"collector_double\", \n    \"collector\"))), default = structure(list(), class = c(\"collector_guess\", \n    \"collector\")), skip = 1), class = \"col_spec\"))\n\nreprex::reprex()\n\n``` r\nlibrary(tidyverse)\nlibrary(ggrepel)\n\ngtcars |&gt;\n  slice_head(n = 3, \n             by = ctry_origin) |&gt;\n  ggplot(aes(x = hp,\n             y = mpg_h,\n             color = ctry_origin,\n             label = model)) +\n  geom_point() +\n  geom_text_repel(force = 2) +\n  theme_classic() +\n  labs(x = \"Horse Power\",\n       y = \"Miles per Gallon (Highway)\") +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n\n```\n\n&lt;sup&gt;Created on 2023-08-13 with [reprex v2.0.2](https://reprex.tidyverse.org)&lt;/sup&gt;"
  },
  {
    "objectID": "Chapter21.html",
    "href": "Chapter21.html",
    "title": "Chapter 21",
    "section": "",
    "text": "21.2.9 Exercises\nThe two main packages for reading data from and writing data to excel spreadsheets are readxl and writexl . But they are not core-tidyverse, so let us load them first.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(writexl)\n\nThe few important functions we will use are:\n\nread_excel()\nexcel_sheets()\n\n\nQuestion 1.\nIn an Excel file, create the following dataset and save it as survey.xlsx. Alternatively, you can download it as an Excel file from here. Then, read it into R, with survey_id as a character variable and n_pets as a numerical variable.\n\nsurvey_df = tibble(\n  survey_id = c(1:6),\n  n_pets = c(0,1,\"N/A\", \"two\", 2, \"\")\n)\n\nsurvey_df |&gt;\n  write_xlsx(\"docs/survey.xlsx\")\n\ndf = read_excel(\n  path = \"docs/survey.xlsx\",\n  col_names = TRUE,\n  col_types = c(\"text\", \"text\"),\n  na = c(\"N/A\", \"\")\n  ) |&gt;\n  mutate(\n    n_pets = ifelse(n_pets == \"two\", 2, n_pets),\n    n_pets = parse_number(n_pets)\n    )\n\ndf\n\n# A tibble: 6 × 2\n  survey_id n_pets\n  &lt;chr&gt;      &lt;dbl&gt;\n1 1              0\n2 2              1\n3 3             NA\n4 4              2\n5 5              2\n6 6             NA\n\n\n\n\nQuestion 2.\nIn another Excel file, create the following data-set and save it as roster.xlsx. Alternatively, you can download it as an Excel file from here. Then, read it into R. The resulting data frame should be called roster and should look like the following.\nThere are two ways of doing this:\n\nUsing the read_excel() function with fill() function of the tidyr package.\nUsing the package openxlsx and its function read.xlsx() which has an argument fillMergedCells = TRUE to do the same task in one go. However, the output is a data.frame, which we must then convert to a tibble.\n\n\n# Use readxl package with fill() from tidyr\nread_excel(\n  path = \"docs/roster.xlsx\"\n  ) |&gt;\n  fill(group, subgroup)\n\n# A tibble: 12 × 3\n   group subgroup    id\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1     1 A            1\n 2     1 A            2\n 3     1 A            3\n 4     1 B            4\n 5     1 B            5\n 6     1 B            6\n 7     1 B            7\n 8     2 A            8\n 9     2 A            9\n10     2 B           10\n11     2 B           11\n12     2 B           12\n\n# Option 2: using the openxlsx package\nlibrary(openxlsx)\nread.xlsx(\n  xlsxFile = \"docs/roster.xlsx\",\n  fillMergedCells = TRUE\n  ) |&gt;\n  as_tibble()\n\n# A tibble: 12 × 3\n   group subgroup    id\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1     1 A            1\n 2     1 A            2\n 3     1 A            3\n 4     1 B            4\n 5     1 B            5\n 6     1 B            6\n 7     1 B            7\n 8     2 A            8\n 9     2 A            9\n10     2 B           10\n11     2 B           11\n12     2 B           12\n\n\n\n\nQuestion 3.\nIn a new Excel file, create the following dataset and save it as sales.xlsx. Alternatively, you can download it as an Excel file from here.\n\nRead sales.xlsx in and save as sales. The data frame should look like the following, with id and n as column names and with 9 rows.\n\nsales = read_excel(\n  \"docs/sales.xlsx\",\n  skip = 4,\n  col_names = c(\"id\", \"n\")\n)\nsales\n\n# A tibble: 9 × 2\n  id      n    \n  &lt;chr&gt;   &lt;chr&gt;\n1 Brand 1 n    \n2 1234.0  8.0  \n3 8721.0  2.0  \n4 1822.0  3.0  \n5 Brand 2 n    \n6 3333.0  1.0  \n7 2156.0  3.0  \n8 3987.0  6.0  \n9 3216.0  5.0  \n\n\nModify sales further to get it into the following tidy format with three columns (brand, id, and n) and 7 rows of data. Note that id and n are numeric, brand is a character variable.\n\nsales |&gt;\n  mutate(\n    brand = ifelse(str_detect(id, \"Brand\"), id, NA),\n    id = parse_number(id),\n    n = parse_number(n, na = \"n\")) |&gt;\n  fill(brand) |&gt;\n  drop_na() |&gt;\n  relocate(brand)\n\n# A tibble: 7 × 3\n  brand      id     n\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Brand 1  1234     8\n2 Brand 1  8721     2\n3 Brand 1  1822     3\n4 Brand 2  3333     1\n5 Brand 2  2156     3\n6 Brand 2  3987     6\n7 Brand 2  3216     5\n\n\n\n\n\nQuestion 4.\nRecreate the bake_sale data frame, write it out to an Excel file using the write.xlsx() function from the openxlsx package.\n\nbake_sale = tibble(\n  item = factor(c(\"brownie\", \"cupcake\", \"cookie\")),\n  quantity = c(10, 5, 8)\n)\nbake_sale |&gt;\n  write.xlsx(\"docs/bake_sale.xlsx\")\n\n\n\nQuestion 5.\nIn Chapter 8 you learned about the janitor::clean_names() function to turn columns names into snake case. Read the students.xlsx file that we introduced earlier in this section and use this function to “clean” the column names.\n\n# Option 1: Read in data from the google sheets\n# library(googlesheets4)\n# id = \"1V1nPp1tzOuutXFLb3G9Eyxi3qxeEhnOXUzL5_BcCQ0w\"\n\n# For an easy reprex, write in the data now, suing:--\n# read_sheet(id) |&gt;\n#   dput()\n\nraw_data = structure(list(`Student ID` = c(1, 2, 3, 4, 5, 6), `Full Name` = c(\"Sunil Huffmann\", \"Barclay Lynn\", \"Jayendra Lyne\", \"Leon Rossini\", \"Chidiegwu Dunkel\",\"Güvenç Attila\"), favourite.food = c(\"Strawberry yoghurt\", \"French fries\", \"N/A\", \"Anchovies\", \"Pizza\", \"Ice cream\"), mealPlan = c(\"Lunch only\", \"Lunch only\", \"Breakfast and lunch\", \"Lunch only\", \"Breakfast and lunch\", \"Lunch only\"), AGE = list(4, 5, 7, NULL, \"five\", 6)), class = c(\"tbl_df\", \"tbl\", \"data.frame\"), row.names = c(NA, -6L))\n\nraw_data |&gt;\n  janitor::clean_names() |&gt;\n  as_tibble()\n\n# A tibble: 6 × 5\n  student_id full_name        favourite_food     meal_plan           age      \n       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;list&gt;   \n1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          &lt;dbl [1]&gt;\n2          2 Barclay Lynn     French fries       Lunch only          &lt;dbl [1]&gt;\n3          3 Jayendra Lyne    N/A                Breakfast and lunch &lt;dbl [1]&gt;\n4          4 Leon Rossini     Anchovies          Lunch only          &lt;NULL&gt;   \n5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch &lt;chr [1]&gt;\n6          6 Güvenç Attila    Ice cream          Lunch only          &lt;dbl [1]&gt;\n\n\n\n\nQuestion 6.\nWhat happens if you try to read in a file with .xlsx extension with read_xls()?\nIf we try to open a *.xlsx file with read_xls() , an error is displayed that Error: filepath libxls error: Unable to open file\n\nread_xls(\"docs/sales.xlsx\")\n\n\n\n\n21.3.6 Exercises\n\nQuestion 1.\nRead the students data set from earlier in the chapter from Excel and also from Google Sheets, with no additional arguments supplied to the read_excel() and read_sheet() functions. Are the resulting data frames in R exactly the same? If not, how are they different?\nThe two resulting data frames are not exactly the same. The data frame created from read_excel() , i.e. df_xl has the has variable AGE saved as character because one of the values is written in characters, instead of a number. Whenever some data is numeric and some data is character , read_excel() converts all data within a column into character format.\nOn the other hand, the data frame created from read_sheet() of googlesheets4 package, i.e. df_gs has this variable stored as a “list”, which contains both numeric and character types of data.\n\ndf_xl = read_excel(\"docs/students.xlsx\")\n\nlibrary(googlesheets4)\nurl_id = \"1V1nPp1tzOuutXFLb3G9Eyxi3qxeEhnOXUzL5_BcCQ0w\"\n\ndf_gs = read_sheet(url_id)\n# Comparing the types of columns in the two data.frames\nsapply(df_xl, class) == sapply(df_gs, class)\n\n    Student ID      Full Name favourite.food       mealPlan            AGE \n          TRUE           TRUE           TRUE           TRUE          FALSE \n\nclass(df_xl$AGE)\n\n[1] \"character\"\n\nclass(df_gs$AGE)\n\n[1] \"list\"\n\nsapply(df_gs$AGE, class)\n\n[1] \"numeric\"   \"numeric\"   \"numeric\"   \"NULL\"      \"character\" \"numeric\"  \n\n\n\n\nQuestion 2.\nRead the Google Sheet titled survey from https://pos.it/r4ds-survey, with survey_id as a character variable and n_pets as a numerical variable.\nWhen we read Google sheets, using col_types argument, we introduce NAs by coercion.\n\nurl_gs = \"https://docs.google.com/spreadsheets/d/1yc5gL-a2OOBr8M7B3IsDNX5uR17vBHOyWZq6xSTG2G8/edit#gid=0\"\n\nread_sheet(\n  ss = url_gs,\n  col_types = \"cd\")\n\n# A tibble: 6 × 2\n  survey_id n_pets\n  &lt;chr&gt;      &lt;dbl&gt;\n1 1              0\n2 2              1\n3 3             NA\n4 4             NA\n5 5              2\n6 6             NA\n\n\n\n\nQuestion 3.\nRead the Google Sheet titled roster from https://pos.it/r4ds-roster. The resulting data frame should be called roster and should look like the following.\n\nurl_gs1 = \"https://docs.google.com/spreadsheets/d/1LgZ0Bkg9d_NK8uTdP2uHXm07kAlwx8-Ictf8NocebIE/edit#gid=0\"\n\nread_sheet(\n  ss = url_gs1\n) |&gt;\nfill(group, subgroup)  \n\n# A tibble: 12 × 3\n   group subgroup    id\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1     1 A            1\n 2     1 A            2\n 3     1 A            3\n 4     1 B            4\n 5     1 B            5\n 6     1 B            6\n 7     1 B            7\n 8     2 A            8\n 9     2 A            9\n10     2 B           10\n11     2 B           11\n12     2 B           12"
  },
  {
    "objectID": "Chapter9.html",
    "href": "Chapter9.html",
    "title": "Chapter 9",
    "section": "",
    "text": "We can copy any error; paste and search it on google.com"
  },
  {
    "objectID": "Chapter9.html#google-is-your-friend",
    "href": "Chapter9.html#google-is-your-friend",
    "title": "Chapter 9",
    "section": "",
    "text": "We can copy any error; paste and search it on google.com"
  },
  {
    "objectID": "Chapter10.html",
    "href": "Chapter10.html",
    "title": "Chapter 10",
    "section": "",
    "text": "“The greatest value of a picture is when it forces us to notice what we never expected to see.” — John Tukey\n\n\nggplot2 will use maximum 6 shapes at a time. The 7th shape is treated as a missing value.\nUsing alpha aesthetic for a discrete variable is not advised.\nThe shapes used in ggplot2 are as follows(Wickham 2016) :--\n\n\n\nShapes available to use in ggplot2.\n\n\nThe best place to explore ggplot2 extensions and graphs is the ggplot2 extensions gallery.\nBest place to search for and understand the geoms within ggplot2 is ggplot2 Function Reference.\n\n\nlibrary(tidyverse)\nlibrary(gt)\ndata(\"mpg\")\ndata(\"diamonds\")"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nHarvard University | Boston, MA MPH in Global Health | Aug 2021 - May 2022\nAll India Institute of Medical Sciences | New Delhi, India MBBS in Medicine | Aug 2005-Dec 2010\nIndira Gandhi National Open University | New Delhi, India MA in Public Policy | Jan 2012 - Dec 2013"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nIndian Administrative Service | Director | Aug 2011 - present\nNeuro-Radiology, AIIMS New Delhi | Junior Resident Doctor | Jan 2011 - Aug 2011"
  },
  {
    "objectID": "Chapter10.html#footnotes",
    "href": "Chapter10.html#footnotes",
    "title": "Chapter 10",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWritten with help from ChatGPT 3.5. OpenAI. (2023). ChatGPT (Aug 22, 2023 version) [Large language model]. https://chat.openai.com↩︎\nThis portion of answer was created using help from ChatGPT 3.5. OpenAI. (2023). ChatGPT (Aug 22, 2023 version) [Large language model]. https://chat.openai.com↩︎\nggplot2, Map projections. https://ggplot2.tidyverse.org/reference/coord_map.html↩︎"
  },
  {
    "objectID": "Chatper11.html",
    "href": "Chatper11.html",
    "title": "Chapter 11",
    "section": "",
    "text": "library(tidyverse)\nlibrary(gt)\n\n\n11.3.3 Exercises\n\nQuestion 1\nExplore the distribution of each of the x, y, and z variables in diamonds. What do you learn? Think about a diamond and how you might decide which dimension is the length, width, and depth.\nUpon exploratory data analysis (code shown below), I learn the following insights: —\n\nThere are outliers in distribution of x , there are eight diamonds with zero value of x, but no outliers on higher side.\nThere are outliers in distribution of y , there are eight diamonds with zero value of y, and 2 outliers on higher side.\nThere are outliers in distribution of z , there are 20 diamonds with zero value of z, and 1 outlier on higher side.\nThe correlation between the variables show that x , y , and z are strongly positively correlated amongst themselves and with the weight (carat).\nThe mean values of x , y and z are 5.73, 5.73 and 3.54. Thus, it is possible that x and y represent either of length and width, while z represents depth.\nNow, upon visualizing the density plots of x , y and z , we see that x and y are similar distributed so, they must be length and breadth, but z is smaller in value. So, z must be depth.\n\n\n\nCode\ndata(\"diamonds\")\ndiamonds |&gt;\n  ggplot(aes(x = x,\n             fill = (x ==  0 | x &gt; 12))) +\n  geom_histogram(binwidth = 0.1) +\n  coord_cartesian(ylim = c(0,10))\n\n\ndiamonds |&gt;\n  ggplot(aes(x = y,\n             fill = (y ==  0 | y &gt; 12))) +\n  geom_histogram(binwidth = 0.1) +\n  coord_cartesian(ylim = c(0,10))\n\ndiamonds |&gt;\n  ggplot(aes(x = z,\n             fill = (z ==  0 | z &gt; 12))) +\n  geom_histogram(binwidth = 0.1) +\n  coord_cartesian(ylim = c(0,20))\n\ndiamonds |&gt;\n  summarize(x = mean(x),\n            y = mean(y),\n            z = mean(z))\n\ndiamonds |&gt;\n  filter(x == 0 | z == 0 | y == 0)\n\ndiamonds |&gt;\n  select(x, y, z) |&gt;\n  pivot_longer(cols = everything(),\n               names_to = \"dimension\",\n               values_to = \"value\") |&gt;\n  ggplot() +\n  geom_density(aes(x = value,\n                   col = dimension)) +\n  theme_classic() +\n  theme(legend.position = \"bottom\") +\n  coord_cartesian(xlim = c(0, 10))\n\n\n\n\nQuestion 2\nExplore the distribution of price. Do you discover anything unusual or surprising? (Hint: Carefully think about the binwidth and make sure you try a wide range of values.)\nThe distribution of price shows a surprising fact in Figure 1 that there are no diamonds priced between $1,450 and $1,550.\n\ndiamonds |&gt;\n  ggplot(aes(price)) +\n  geom_histogram(binwidth = 10, \n                 fill = \"lightgrey\", \n                 color = \"darkgrey\") + \n  coord_cartesian(xlim = c(500, 2000)) + \n  scale_x_continuous(breaks = seq(from = 500, to = 2000, by = 100)) +\n  theme_minimal()\n\n\n\n\nFigure 1: The hisotgram of diamonds’ prices, focussed in area around $1500 price tag\n\n\n\n\n\n\nQuestion 3\nHow many diamonds are 0.99 carat? How many are 1 carat? What do you think is the cause of the difference?\nThere are only 23 diamonds of 0.99 carat, but 1,558 diamonds of 1 carat.\nThe possible cause of this difference is that the weight recorder or measurement instrument rounded off to the nearest integer, especially if carat was 0.99.\n\ndiamonds |&gt;\n  select(carat) |&gt;\n  filter(carat == 0.99 | carat == 1) |&gt;\n  group_by(carat) |&gt;\n  count()\n\n# A tibble: 2 × 2\n# Groups:   carat [2]\n  carat     n\n  &lt;dbl&gt; &lt;int&gt;\n1  0.99    23\n2  1     1558\n\n\n\n\nQuestion 4\nCompare and contrast coord_cartesian() vs. xlim() or ylim() when zooming in on a histogram. What happens if you leave binwidth unset? What happens if you try and zoom so only half a bar shows?\nBoth coord_cartesian() and xlim() / ylim() serve a similar purpose of adjusting the visible range of data in a plot, but they do so in slightly different ways. The Figure 2 shows the difference.\n\nxlim() and ylim() are functions in R that directly modify the data range that is displayed on the x-axis and y-axis, respectively. The xlim() / ylim() replace all values outside the range into NAs . They remove the data outside the limits. They can be used to zoom in on specific portions of the plot.\n\nPros:\n\nCan help emphasize specific details or patterns in the data after removal of outliers.\n\nCons:\n\nData points outside the specified range are removed from the plot, potentially leading to a loss of context.\nIf used improperly, it can distort the visual representation of the data, making it appear more or less significant than it actually is.\n\n\n\ncoord_cartesian() allows us to adjust the visible range of data without altering the underlying data.\n\nPros:\n\nIt does not remove any data points from the plot; it only changes the visible range.\nUseful when you want to focus on a specific part of the plot while still having access to the full data context.\n\nCons:\n\nIf there are outliers or extreme values, they might still affect the appearance of the plot.\n\n\nComparison\n\n\n\n\n\n\n\n\nAspect\ncoord_cartesian()\nxlim()/ylim()\n\n\n\n\nPurpose\nAdjust visible range without altering data\nSet specific data range to be displayed, removes data outside the range\n\n\nData Integrity\nMaintains original data and scaling\nCan exclude data points outside range\n\n\nContext\nPreserves overall data context\nMay lose context due to excluded data; or reveal new insights upon removal of outliers.\n\n\nImpact on Plot\nAdjusts only the visible area\nAlters axes scaling and data representation\n\n\nHandling outliers\nKeeps outliers within context\nRemove outliers outside the specified range\n\n\nControl over Range\nLimited control over axes scaling\nPrecise control over displayed range\n\n\nSuitability for Histograms\nRecommended for maintaining bin sizes\nCan distort histogram representation\n\n\n\n\ngridExtra::grid.arrange(\ndiamonds |&gt;\n  ggplot(aes(x = y)) +\n  geom_histogram(binwidth = 0.1) + \n  ylim(0, 1000) +\n  xlim(0, 10) +\n  labs(subtitle = \"xlim and ylim remove data outside the limits, \\neg. counts &gt; 1000; or the observation at zero\"),\n\ndiamonds |&gt;\n  ggplot(aes(x = y)) +\n  geom_histogram(binwidth = 0.1) + \n  coord_cartesian(ylim = c(0, 1000),\n                  xlim = c(0, 10)) +\n  labs(subtitle = \"coord_cartesian preserves data outside the limits, \\neg. counts &gt; 1000; or the observation at zero\"),\n\nncol = 2)\n\n\n\n\nFigure 2: Difference between coord_cartesian() and xlim()/ylim()\n\n\n\n\n\n\n\n11.4.1 Exercises\n\nQuestion 1\nWhat happens to missing values in a histogram? What happens to missing values in a bar chart? Why is there a difference in how missing values are handled in histograms and bar charts?\nIn a histogram, missing values are typically ignored. If there are missing values in your data, they won’t be placed into any bin and won’t contribute to the creation of bars in the histogram. Thus, histogram only shows the distribution of the non-missing values.\nIn a bar chart, which is used to display categorical data, missing values are treated as a distinct category. When you create a bar chart using ggplot2, each unique category in your data is represented by a bar. If there are missing values, ggplot2 will include a separate bar to represent the missing category, often labeled as “NA” or “Missing”.\nThe difference in how missing values are handled in histograms and bar charts arises from their underlying purposes:\n\nHistograms are primarily used to visualize the distribution of continuous or numeric data. Since missing values don’t have a specific numeric value to be placed into bins, it’s common practice to exclude them.\nBar charts, on the other hand, are used to compare the frequency or count of different categories. Missing values are treated as a category themselves.\n\nIn summary, the distinction in handling missing values is based on the type of data being visualized and the purpose of each plot. Histograms focus on the distribution of non-missing numeric data, while bar charts emphasize the comparison of categorical data, including missing values as a separate category.\n\n# Set a random seed for reproducibility\nset.seed(123)\n\n# Create a sample dataset with missing values\nn = 200\ndf = data.frame(\n  Category = sample(x = c(\"A\", \"B\", \"C\", \"D\"), \n                    size = n, \n                    replace = TRUE),\n  Value = rnorm(n)\n)\n\n# Introduce missing values\ndf$Value[sample(1:n, 40)] = NA\ndf$Category[sample(1:n, 40)] = NA\n\n# Create plots to demonstrate\ngridExtra::grid.arrange(\n  ggplot(df, aes(x = Value)) +\n    geom_histogram(col = \"grey\", fill = \"lightgrey\") +\n    theme_minimal() +\n    labs(subtitle = \"Histogram drops the missing values\"),\n\n  ggplot(df, aes(x = Category)) +\n    geom_bar(col = \"grey\", fill = \"lightgrey\") + \n    theme_minimal() +\n    labs(subtitle = \"Bar Chart includes missing values as a category\"),\n  \n  ncol = 2)\n\n\n\n\n\n\nQuestion 2\nWhat does na.rm = TRUE do in mean() and sum()?\nWhen na.rm is set to TRUE, the function will remove any NA values from the input vector before performing the calculation. This means that the resulting mean or sum will only consider the non-missing values.\nThis is important because in R , NAs cannot be added or subtrated or operated upon, for example, NA + 1 = NA. Thus, even if one observation is NA, the mean or sum of the entire vector will be NA . Hence, using na.rm = TRUE is important.\n\nmean(df$Value)\n\n[1] NA\n\nmean(df$Value, na.rm = TRUE)\n\n[1] 0.006409991\n\nsum(df$Value)\n\n[1] NA\n\nsum(df$Value, na.rm = TRUE)\n\n[1] 1.025599\n\n\n\n\nQuestion 3\nRecreate the frequency plot of scheduled_dep_time colored by whether the flight was cancelled or not. Also facet by the cancelled variable. Experiment with different values of the scales variable in the faceting function to mitigate the effect of more non-cancelled flights than cancelled flights.\nThe best value of scales to use is scales = \"free_y\" so that the two facets’ y-axis are completely free and we can compare the distribution of cancelled flights vs. non-cancelled flights in Figure 3.\n\nnycflights13::flights |&gt; \n  mutate(\n    cancelled = is.na(dep_time),\n    sched_hour = sched_dep_time %/% 100,\n    sched_min = sched_dep_time %% 100,\n    sched_dep_time = sched_hour + (sched_min / 60)\n  ) |&gt;\n  # Create nice names for \"cancelled\" to show in the eventual plot\n  mutate(cancelled = as_factor(ifelse(cancelled, \n                                      \"Cancelled Flights\",\n                                      \"Flights Not Cancelled\"))) |&gt;\n  ggplot(aes(x = sched_dep_time)) +\n  geom_freqpoly(lwd = 1) +\n  theme_minimal() + \n  facet_wrap(~cancelled, \n             scales = \"free_y\") +\n  labs(x = \"Scheduled Departure Time (in hrs)\",\n       y = \"Number of flights\") +\n  scale_x_continuous(breaks = seq(0, 24, 4))\n\n\n\n\nFigure 3: Comparison of cancelled vs. non-cancelled flights by faceting\n\n\n\n\n\n\n\n11.5.1.1 Exercises\n\nQuestion 1\nUse what you’ve learned to improve the visualization of the departure times of cancelled vs. non-cancelled flights.\nThe Figure 4 shows an example to demonstrate exploratory data analysis in missing values in data-set flights of the package nycflights13 . It shows that as the day progresses, more flights get cancelled. Evening flights are more likely to get cancelled than morning flights.\n\nnycflights13::flights |&gt; \n  mutate(\n    cancelled = is.na(dep_time),\n    sched_hour = sched_dep_time %/% 100,\n    sched_min = sched_dep_time %% 100,\n    sched_dep_time = sched_hour + (sched_min / 60)\n  ) |&gt;\n  ggplot(aes(x = sched_dep_time,\n             y = after_stat(density))) +\n  geom_freqpoly(aes(col = cancelled),\n                lwd = 1) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"Scheduled Departure Time (in hrs)\",\n       y = \"Proportion of flights departed\",\n       color = \"Whether the flight was cancelled?\",\n       subtitle = \"Comparison of density frequency polygons of cancelled vs. non-cancelled flights\") +\n  scale_x_continuous(breaks = seq(0,24, by = 2))\n\n\n\n\nFigure 4: Visualizing departure times of cancelled vs. non-cancelled flights\n\n\n\n\nAnother method to visualize it is shown using stat = \"density\" argument in the geom_freqpoly() in Figure 5 below.\n\n\nCode\nnycflights13::flights |&gt; \n  mutate(\n    cancelled = is.na(dep_time),\n    sched_hour = sched_dep_time %/% 100,\n    sched_min = sched_dep_time %% 100,\n    sched_dep_time = sched_hour + (sched_min / 60)\n  ) |&gt;\n  ggplot(aes(x = sched_dep_time)) +\n  geom_freqpoly(aes(col = cancelled),\n                stat = \"density\",\n                lwd = 1) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"Scheduled Departure Time (in hrs)\",\n       y = \"Proportion of flights departed\",\n       color = \"Whether the flight was cancelled?\",\n       subtitle = \"Comparison of density frequency polygons of cancelled vs. non-cancelled flights\") +\n  scale_x_continuous(breaks = seq(0,24, by = 2))\n\n\n\n\n\nFigure 5: Another method of visualizing departure times of cancelled vs. non-cancelled flights\n\n\n\n\nLastly, we can also work on the data set, compute the percentage of flights that are cancelled within each hour and plot the percentage as shown in Figure 6 .\n\n\nCode\nnycflights13::flights |&gt; \n  mutate(\n    cancelled = is.na(dep_time),\n    sched_hour = sched_dep_time %/% 100\n  ) |&gt;\n  group_by(sched_hour) |&gt;\n  summarise(\n    cancelled = sum(cancelled),\n    total = n()\n  ) |&gt;\n  mutate (prop_cancelled = cancelled/total) |&gt;\n  ggplot(aes(x = sched_hour,\n             y = prop_cancelled*100)) +\n  geom_line() +\n  geom_point() +\n  xlim(4,24) +\n  ylim(0, 5) +\n  scale_x_continuous(breaks = seq(4, 24, 2)) +\n  coord_cartesian(xlim = c(4, 24)) +\n  labs(x = \"Scheduled Departure Time (in hrs)\",\n       y = \"Percentage of flights that were cancelled\",\n       subtitle = \"Percentage of cancelled flights over different scheduled departure times\") +\n  theme_minimal()\n\n\n\n\n\nFigure 6: Percentage of flights cancelled each hour\n\n\n\n\n\n\nQuestion 2\nBased on EDA, what variable in the diamonds dataset appears to be most important for predicting the price of a diamond? How is that variable correlated with cut? Why does the combination of those two relationships lead to lower quality diamonds being more expensive?\n\n\nQuestion 3\nInstead of exchanging the x and y variables, add coord_flip() as a new layer to the vertical boxplot to create a horizontal one. How does this compare to exchanging the variables?\n\n\nQuestion 4\nOne problem with boxplots is that they were developed in an era of much smaller datasets and tend to display a prohibitively large number of “outlying values”. One approach to remedy this problem is the letter value plot. Install the lvplot package, and try using geom_lv() to display the distribution of price vs. cut. What do you learn? How do you interpret the plots?\n\n\nQuestion 5\nCreate a visualization of diamond prices vs. a categorical variable from the diamonds data-set using geom_violin(), then a faceted geom_histogram(), then a colored geom_freqpoly(), and then a colored geom_density(). Compare and contrast the four plots. What are the pros and cons of each method of visualizing the distribution of a numerical variable based on the levels of a categorical variable?\n\n\nQuestion 6\nIf you have a small data-set, it’s sometimes useful to use geom_jitter() to avoid overplotting to more easily see the relationship between a continuous and categorical variable. The ggbeeswarm package provides a number of methods similar to geom_jitter(). List them and briefly describe what each one does."
  },
  {
    "objectID": "Chapter11.html",
    "href": "Chapter11.html",
    "title": "Chapter 11",
    "section": "",
    "text": "library(tidyverse)\nlibrary(gt)\nlibrary(RColorBrewer)\ndata(\"diamonds\")"
  },
  {
    "objectID": "Chapter11.html#footnotes",
    "href": "Chapter11.html#footnotes",
    "title": "Chapter 11",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis my my interpretation, and needs confirmation. Please comment or leave a pull request / issue on GitHub.↩︎"
  },
  {
    "objectID": "Chapter12.html",
    "href": "Chapter12.html",
    "title": "Chapter 12",
    "section": "",
    "text": "Important lessons from R for Data Science 2nd Edition, Chapter 12: –\nlibrary(tidyverse)   # the tidyverse\nlibrary(scales)      # to adjust display of numbers\nlibrary(ggrepel)     # to clearly position text labels\nlibrary(patchwork)   # to display multiple plots\nlibrary(gt)          # to display beautiful tables in Quarto"
  },
  {
    "objectID": "Chapter12.html#footnotes",
    "href": "Chapter12.html#footnotes",
    "title": "Chapter 12",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\nNote: The BODMAS rules (also known as PEMDAS or PEDMAS in different regions) are a set of rules that dictate the order of operations to be followed when evaluating mathematical expressions. These rules ensure that mathematical expressions are evaluated consistently and accurately. BODMAS stands for:\n\nBrackets: Evaluate expressions inside brackets first.\nOrders: Evaluate exponential or power operations.\nDivision and Multiplication: Perform division and multiplication.\nAddition and Subtraction: Perform addition and subtraction operations from left to right.\n\n\n↩︎"
  },
  {
    "objectID": "Chapter13.html",
    "href": "Chapter13.html",
    "title": "Chapter 13",
    "section": "",
    "text": "Some important take-away points\nlibrary(tidyverse)\nlibrary(nycflights13)\ndata(\"flights\")\nlibrary(gt)"
  },
  {
    "objectID": "Chapter13.html#question-1",
    "href": "Chapter13.html#question-1",
    "title": "Chapter 13",
    "section": "Question 1",
    "text": "Question 1\nHow does dplyr::near() work? Type near to see the source code. Is sqrt(2)^2 near 2?\ndplyr::near() is used for testing whether two numeric values are close to each other within a certain tolerance. This is useful when working with floating-point numbers, where exact equality can be problematic due to precision limitations. It takes three arguments:\n\nx: The first numeric value to compare.\ny: The second numeric value to compare.\ntol: Tolerance level, which is a small positive numeric value that defines how close x and y need to be to be considered “near.” By default, it uses .Machine$double.eps^0.5, which is a good default for most cases.\n\nNote 1: The variable .Machine contains data concerning the numerical attributes of the computer system where R is currently operating. This information encompasses details like the maximum values for double, and integer types, as well as the machine’s precision.\nNote 2: The double.eps value represents the smallest positive floating-point number x for which the equation 1 + x != 1 holds true. Its calculation involves the base of the double data type and the number of significant digits (ulp.digits). Specifically, if the base is 2 or the rounding method is 0, it equals double.base ^ ulp.digits. In other cases, it is (double.base ^ double.ulp.digits) / 2. Typically, this value is approximately 2.220446e-16.\n\nnear\n## function (x, y, tol = .Machine$double.eps^0.5) \n## {\n##     abs(x - y) &lt; tol\n## }\n## &lt;bytecode: 0x000001b2a68444a8&gt;\n## &lt;environment: namespace:dplyr&gt;\n.Machine$double.eps\n## [1] 2.220446e-16\n.Machine$double.eps^0.5\n## [1] 1.490116e-08\n\nThus, dplyr::near() works by checking whether the absolute value of the difference between x and y is less that the square-root of .Machine$double.eps or not.\nAs shown in the code below, yes, sqrt(2)^2 is near 2.\n\nnear(sqrt(2)^2, 2)\n## [1] TRUE"
  },
  {
    "objectID": "Chapter13.html#question-2",
    "href": "Chapter13.html#question-2",
    "title": "Chapter 13",
    "section": "Question 2",
    "text": "Question 2\nUse mutate(), is.na(), and count() together to describe how the missing values in dep_time, sched_dep_time and dep_delay are connected.\nFirst, let us try to compute the number of rows where, as it should be, sched_dep_time - dep_time == dep_delay. As we see below, the results is NA since NAs are contagious in addition, the result returns NA .\n\nflights |&gt;\n  select(dep_time, sched_dep_time, dep_delay) |&gt;\n  mutate(check1 = sched_dep_time - dep_time == dep_delay) |&gt;\n  summarise(\n    n = n(),\n    equal = sum(check1)\n  )\n\n# A tibble: 1 × 2\n       n equal\n   &lt;int&gt; &lt;int&gt;\n1 336776    NA\n\n\nNow, let us rework the maths with using is.na() to remove missing values of departure time, i.e. cancelled flights. We can use filter(!is.na(dep_time)) . The results indicate that 69.% of flights, the departure delay is equal to difference between departure time and scheduled departure time.\n\nflights |&gt;\n  select(dep_time, sched_dep_time, dep_delay) |&gt;\n  filter(!is.na(dep_time)) |&gt;\n  mutate(check1 = dep_time - sched_dep_time == dep_delay) |&gt;\n  summarise(\n    total = n(),\n    equal = sum(check1)\n  ) |&gt;\n  mutate(perc_equal = (equal*100)/total)\n\n# A tibble: 1 × 3\n   total  equal perc_equal\n   &lt;int&gt;  &lt;int&gt;      &lt;dbl&gt;\n1 328521 228744       69.6\n\n\nNow, onto checking the relation between missing values. We observe that none of the Scheduled Departure Time values are missing. There are 8,255 missing Departure Time values, which indicate a cancelled flight. There are also 8,255 missing Departure Delay values, and we show below that these are the exact same flights for which the departure time is missing. Thus, the missing values in dep_delay and dep_time are connected and exaclty occurring for same rows.\n\n# Number of row with missing Scheduled Departure Time\nflights |&gt;\n  filter(is.na(sched_dep_time)) |&gt;\n  count() |&gt;\n  as.numeric()\n\n[1] 0\n\n# The number of rows with missing Departure Time\nflights |&gt;\n  filter(is.na(dep_time)) |&gt;\n  count() |&gt;\n  as.numeric()\n\n[1] 8255\n\n# The number of rows with missing Departure Delay\nflights |&gt;\n  filter(is.na(dep_delay)) |&gt;\n  count() |&gt;\n  as.numeric()\n\n[1] 8255\n\n# Checking whether the exact same rows have missing values\n# for Departure Time and Departure Delay\nsum(which(is.na(flights$dep_time)) != which(is.na(flights$dep_delay)))\n\n[1] 0"
  },
  {
    "objectID": "Chapter13.html#question-1-1",
    "href": "Chapter13.html#question-1-1",
    "title": "Chapter 13",
    "section": "Question 1",
    "text": "Question 1\nFind all flights where arr_delay is missing but dep_delay is not. Find all flights where neither arr_time nor sched_arr_time are missing, but arr_delay is.\nThe following code displays all flights where arr_delay is missing but dep_delay is not in Figure 1. There are 1,175 such flights.\n\nflights |&gt;\n  filter(is.na(arr_delay) & !is.na(dep_delay)) |&gt;\n  select(flight, arr_delay, dep_delay) |&gt;\n  gt() |&gt;\n  cols_label(flight = \"Flight Number\",\n             arr_delay = \"Arrival Delay (in min)\",\n             dep_delay = \"Departure Delay (in min)\") |&gt;\n  opt_interactive(pagination_type = \"jump\")\n\n\n\n\n\n\n\n\nFigure 1: Table of all flights where arr_delay is missing but dep_delay is not\n\n\n\nThe following code displays all flights where neither arr_time nor sched_arr_time are missing, but arr_delay is missing reflected in Figure 2. There are 717 such flights.\n\nflights |&gt;\n  filter(!is.na(arr_time) & !is.na(sched_arr_time) & is.na(arr_delay)) |&gt;\n  select(flight, arr_time, sched_arr_time, arr_delay) |&gt;\n  gt() |&gt;\n  cols_label(flight = \"Flight Number\",\n             arr_delay = \"Arrival Delay (in min)\",\n             arr_time = \"Arrival Time (hrs)\",\n             sched_arr_time = \"Scheduled Arrival Time (hrs)\") |&gt;\n  opt_interactive(pagination_type = \"jump\")\n\n\n\n\n\n\n\n\nFigure 2: Table of all flights where arr_delay is missing but dep_delay is not"
  },
  {
    "objectID": "Chapter13.html#question-2-1",
    "href": "Chapter13.html#question-2-1",
    "title": "Chapter 13",
    "section": "Question 2",
    "text": "Question 2\nHow many flights have a missing dep_time? What other variables are missing in these rows? What might these rows represent?\nThe following code shows us that 8,255 flights have a missing dep_time . Further, Figure 3 shows us that these flights have missing dep_delay , arr_time , arr_delay and air_time. Further, around 30% of these have missing tailnum.\nThus, these rows most likely represent cancelled flights.\n\nflights |&gt;\n  filter(is.na(dep_time)) |&gt;\n  count() |&gt; as.numeric()\n\n[1] 8255\n\nflights |&gt;\n  filter(is.na(dep_time)) |&gt;\n  naniar::vis_miss() +\n  theme(axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.text.x.top = element_text(angle = 90))\n\n\n\n\nFigure 3: Visualization of missing values in flights with missing dep_time"
  },
  {
    "objectID": "Chapter13.html#question-3",
    "href": "Chapter13.html#question-3",
    "title": "Chapter 13",
    "section": "Question 3",
    "text": "Question 3\nAssuming that a missing dep_time implies that a flight is cancelled, look at the number of cancelled flights per day. Is there a pattern? Is there a connection between the proportion of cancelled flights and the average delay of non-cancelled flights?\nAs shown in Figure 4 below, when seen over the entire course of year, no specific pattern emerges. However, certain months show more number of spikes - there is one day with large number of cancellations (over 400) in February. When seen month-wise, as in Figure 5 , it becomes apparent that most flight cancellations occur in December - March; or in June-July.\n\n\nCode\nflights |&gt;\n  filter(is.na(dep_time)) |&gt;\n  mutate(date = make_date(year = year,\n                          month = month,\n                          day = day)) |&gt;\n  group_by(date) |&gt;\n  count() |&gt;\n  ggplot(aes(x = date, y = n)) +\n  geom_line() +\n  ggthemes::theme_fivethirtyeight() +\n  labs(title = \"Cancelled flight numbers show spikes on certain days\",\n       subtitle = \"There is consistent cancellation in December - may be due to snowstorms!\") +\n  theme(\n    plot.background = element_rect(fill = \"white\"), \n    panel.background = element_rect(fill = \"white\", colour = \"white\")\n    )\n\n\n\n\n\nFigure 4: ?(caption)\n\n\n\n\n\n\nCode\nmths_lab = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\",\n             \"May\", \"Jun\", \"Jul\", \"Aug\",\n             \"Sep\", \"Oct\", \"Nov\", \"Dec\")\ngridExtra::grid.arrange(\n  flights |&gt;\n    filter(is.na(dep_time)) |&gt;\n    group_by(month) |&gt;\n    count() |&gt;\n    ggplot(aes(x = month, y = n)) +\n    geom_line() +\n    ggthemes::theme_fivethirtyeight() +\n    labs(y = \"No. of cancelled flights\",\n         title = \"Flight cancellations occur most \\nin a few months\",\n         subtitle = \"December-Feruary, June-July see maximum cancelled flights. \\nOctober-November have the least cancellations\") +\n    scale_x_continuous(breaks = 1:12,\n                       labels = mths_lab) +\n    theme(title = element_text(size = 8)) +\n    theme(\n      plot.background = element_rect(fill = \"white\"), \n      panel.background = element_rect(fill = \"white\", colour = \"white\")\n    ),\n  \n  flights |&gt;\n    filter(is.na(dep_time)) |&gt;\n    group_by(day) |&gt;\n    count() |&gt;\n    ggplot(aes(x = day, y = n)) +\n    geom_line() +\n    geom_point(col = \"darkgrey\", size = 2) +\n    ggthemes::theme_fivethirtyeight() +\n    labs(y = \"No. of cancelled flights\",\n         title = \"Flight cancellations spike on 8th-9th days \\nof the months\",\n         subtitle = \"This is unlikely to be a pattern since \\nthis is a sum, influenced by a few high values\") + \n    scale_x_continuous(breaks = seq(1, 31, 5)) +\n    theme(title = element_text(size = 8)) +\n    theme(\n      plot.background = element_rect(fill = \"white\"), \n      panel.background = element_rect(fill = \"white\", colour = \"white\")\n    ),\n\nncol = 2)\n\n\n\n\n\nFigure 5: ?(caption)\n\n\n\n\nThe following Figure 6 depicts the connection between the proportion of cancelled flights and the average delay of non-cancelled flights. It shows that there is high average delay on the days where there are more cancelled flights, perhaps because bad weather causes both.\nNote: Here, I used geom_smooth() instead of geom_line() to make the pattern more easily perceptible.\n\n\nCode\ncoeff = 0.15\n\nflights |&gt;\n  mutate(date = make_date(year = year,\n                          month = month,\n                          day = day)) |&gt;\n  group_by(month, date) |&gt;\n  summarise(\n    n = n(),\n    cancelled = sum(is.na(dep_time)),\n    prop_cancelled = sum(is.na(dep_time)) / n(),\n    avg_delay = mean(dep_delay, na.rm = TRUE)\n  ) |&gt;\n  ggplot(aes(x = date)) +\n  geom_smooth(aes(y = prop_cancelled * 100), \n              method = \"loess\",\n              span = coeff,\n              col = \"red\",\n              se = FALSE) +\n  geom_smooth(aes(y = avg_delay), , \n              method = \"loess\",\n              span = coeff,\n              col = \"blue\",\n              se = FALSE) +  \n  theme_minimal() +\n  scale_y_continuous(\n    name = \"Percentage Cancelled Flights (%)\",\n    sec.axis = sec_axis(trans = ~ .*0.5,\n                        name = \"Avg. Delay (min.)\")) +\n  labs(x = NULL) +\n  theme(axis.text.y.left = element_text(color = \"red\"),\n        axis.title.y.left = element_text(color = \"red\"),\n        axis.text.y.right = element_text(color = \"blue\"),\n        axis.title.y.right = element_text(color = \"blue\")) +\n  labs(title = \"Flight Cancellations and average delays are correlated\",\n       subtitle = \"Higher average delay occurs on same days as more flight cancellations\")\n\n\n\n\n\nFigure 6: Correlation between proportion of cancelled flights and the average delay of non-cancelled flights"
  },
  {
    "objectID": "Chapter13.html#question-1-2",
    "href": "Chapter13.html#question-1-2",
    "title": "Chapter 13",
    "section": "Question 1",
    "text": "Question 1\nWhat will sum(is.na(x)) tell you? How about mean(is.na(x))?\nThe expression sum(is.na(x)) tells us the number of missing values in the vector x. The expression mean(is.na(x)) tells us the proportion of missing values in the vector x .\nThis is because is.na(x) is a function or operation used to determine which elements of a vector or data structure x are missing or NA (Not Available) values. NA values typically represent missing or undefined data points. Here’s what the two expressions you provided mean:\n\nsum(is.na(x)):\n\nThis expression will count the number of NA (missing) values in the vector or data structure x. It calculates the total count of NA values in the entire dataset.\nThe result will be an integer representing the count of NA values.\n\nmean(is.na(x)):\n\nThis expression will calculate the proportion of NA (missing) values in the vector or data structure x. It calculates the average of a binary vector where each element is either 1 (if NA) or 0 (if not NA).\nThe result will be a numeric value between 0 and 1, representing the fraction of missing values in the dataset. It can be interpreted as the percentage of missing values when multiplied by 100.\n\n\nHere’s a simple example in R to illustrate these concepts:\n\n# Sample vector with missing values \nx &lt;- c(1, NA, 3, NA, 5, 6)  \n\n# Count of missing values \ncount_missing &lt;- sum(is.na(x)) \ncat(\"Count of missing values:\", count_missing, \"\\n\")  \n\nCount of missing values: 2 \n\n# Proportion of missing values \nprop_missing &lt;- mean(is.na(x)) \ncat(\"Proportion of missing values:\", prop_missing, \"\\n\")\n\nProportion of missing values: 0.3333333"
  },
  {
    "objectID": "Chapter13.html#question-2-2",
    "href": "Chapter13.html#question-2-2",
    "title": "Chapter 13",
    "section": "Question 2",
    "text": "Question 2\nWhat does prod() return when applied to a logical vector? What logical summary function is it equivalent to? What does min() return when applied to a logical vector? What logical summary function is it equivalent to? Read the documentation and perform a few experiments.\n\n# A logical vector with random TRUE and FALSE\nrandom &lt;- sample(c(TRUE, FALSE),\n            size = 10,\n            replace = TRUE)\nrandom\n##  [1]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE\n# A logical vector with all TRUE\nall_true &lt;- rep(TRUE, 10)\nall_true\n##  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n# A logical vector with all FALSE\nall_false &lt;- rep(FALSE, 10)\nall_false\n##  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\nprod(random)\n## [1] 0\nprod(all_true)\n## [1] 1\nprod(all_false)\n## [1] 0\n\nmin(random)\n## [1] 0\nmin(all_true)\n## [1] 1\nmin(all_false)\n## [1] 0\n\nIn R, when we apply the prod() function to a logical vector, it treats TRUE as 1 and FALSE as 0, and then computes the product of the elements in the vector. Essentially, it multiplies all the elements together. This can be useful when we want to check if all elements in a logical vector are TRUE, as the product will be 1 if all are TRUE and 0 if any of them is FALSE.\nThis is equivalent to using the all() function, which checks if all elements in a logical vector are TRUE. The all() function returns TRUE if all elements are TRUE and FALSE otherwise.\nNow, when we apply the min() function to a logical vector, it also treats TRUE as 1 and FALSE as 0, and then computes the minimum value. Since 0 represents FALSE and 1 represents TRUE, the minimum value in a logical vector is FALSE (0). Therefore, when we use min() on a logical vector with even one value FALSE, it will return FALSE.\nIn summary, prod() and min() applied to logical vectors have specific behavior related to the interpretation of TRUE and FALSE, and they are equivalent to the all().\nNote: max() will act as equivalent to the any() function."
  },
  {
    "objectID": "Chapter13.html#question-1-3",
    "href": "Chapter13.html#question-1-3",
    "title": "Chapter 13",
    "section": "Question 1",
    "text": "Question 1\nA number is even if it's divisible by two, which in R you can find out with x %% 2 == 0. Use this fact and if_else() to determine whether each number between 0 and 20 is even or odd.\nThe easiest way to do this would be if_else(x %% 2 == 0, true = \"even\", false = \"odd\"), as shown below: –\n\nx = 0:20\nif_else(x %% 2 == 0,\n        true = \"even\",\n        false = \"odd\")\n\n [1] \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\" \n[11] \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\" \n[21] \"even\""
  },
  {
    "objectID": "Chapter13.html#question-2-3",
    "href": "Chapter13.html#question-2-3",
    "title": "Chapter 13",
    "section": "Question 2",
    "text": "Question 2\nGiven a vector of days like x &lt;- c(\"Monday\", \"Saturday\", \"Wednesday\"), use an ifelse() statement to label them as weekends or weekdays.\nThe code shown below does the job: –\n\ndays = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\",\n      \"Friday\", \"Saturday\", \"Sunday\")\nweeknd = c(\"Saturday\", \"Sunday\")\n\nx = sample(days, size = 10, replace = TRUE)\n\ncbind(x,\n      if_else(x %in% weeknd,\n              \"Weekends\",\n              \"Weekdays\", \n              \"NA\")) |&gt;\n  as_tibble() |&gt; gt()\n\n\n\n\n\n  \n    \n    \n      x\n      V2\n    \n  \n  \n    Friday\nWeekdays\n    Friday\nWeekdays\n    Thursday\nWeekdays\n    Friday\nWeekdays\n    Saturday\nWeekends\n    Thursday\nWeekdays\n    Sunday\nWeekends\n    Thursday\nWeekdays\n    Wednesday\nWeekdays\n    Sunday\nWeekends"
  },
  {
    "objectID": "Chapter13.html#question-3-1",
    "href": "Chapter13.html#question-3-1",
    "title": "Chapter 13",
    "section": "Question 3",
    "text": "Question 3\nUse ifelse() to compute the absolute value of a numeric vector called x.\nWe can use the code if_else(x &lt; 0, true = -x, false = x, missing = 0) to do so, as shown below: –\n\nx = sample(x = -10:10,\n           replace = TRUE,\n           size = 100)\n\ntibble(\n  x = x,\n  abs_x = if_else(x &lt; 0,\n                  true = -x,\n                  false = x,\n                  missing = 0)\n) |&gt;\n  gt() |&gt;\n  opt_interactive(use_pagination = TRUE,\n                  pagination_type = \"simple\")"
  },
  {
    "objectID": "Chapter13.html#question-4",
    "href": "Chapter13.html#question-4",
    "title": "Chapter 13",
    "section": "Question 4",
    "text": "Question 4\nWrite a case_when() statement that uses the month and day columns from flights to label a selection of important US holidays (e.g., New Years Day, 4th of July, Thanksgiving, and Christmas). First create a logical column that is either TRUE or FALSE, and then create a character column that either gives the name of the holiday or is NA.\nHere is the code: –\n\nflights |&gt;\n  mutate(holiday = case_when(\n    month == 1 & day == 1   ~ \"New Year’s Day\",\n    month == 6 & day == 19  ~ \"Juneteenth National Independence Day\",\n    month == 7 & day == 4   ~ \"Independence Day\",\n    month == 11 & day == 11 ~ \"Veterans’ Day\",\n    month == 12 & day == 25 ~ \"Christmas Day\",\n    .default = NA\n  ),\n  .keep = \"used\") |&gt;\n  mutate(is_holiday = if_else(!is.na(holiday),\n                              true = TRUE,\n                              false = FALSE))\n\n# A tibble: 336,776 × 4\n   month   day holiday        is_holiday\n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;          &lt;lgl&gt;     \n 1     1     1 New Year’s Day TRUE      \n 2     1     1 New Year’s Day TRUE      \n 3     1     1 New Year’s Day TRUE      \n 4     1     1 New Year’s Day TRUE      \n 5     1     1 New Year’s Day TRUE      \n 6     1     1 New Year’s Day TRUE      \n 7     1     1 New Year’s Day TRUE      \n 8     1     1 New Year’s Day TRUE      \n 9     1     1 New Year’s Day TRUE      \n10     1     1 New Year’s Day TRUE      \n# ℹ 336,766 more rows\n\n\nAnother way to code this creating the logical column first is as follows: –\n\nflights |&gt;\n  mutate(\n    is_holiday = case_when(\n      (month == 1 & day == 1) ~ TRUE,        # New Year's Day\n      (month == 7 & day == 4) ~ TRUE,        # 4th of July\n      (month == 12 & day == 25) ~ TRUE,      # Christmas\n      TRUE ~ FALSE                           # Not a holiday\n    ),\n    holiday_name = case_when(\n      is_holiday ~ case_when(\n        (month == 1 & day == 1) ~ \"New Year's Day\",\n        (month == 7 & day == 4) ~ \"4th of July\",\n        (month == 12 & day == 25) ~ \"Christmas\",\n        TRUE ~ NA_character_\n      ),\n      TRUE ~ NA_character_\n    ),\n    .keep = \"used\"\n  )\n\n# A tibble: 336,776 × 4\n   month   day is_holiday holiday_name  \n   &lt;int&gt; &lt;int&gt; &lt;lgl&gt;      &lt;chr&gt;         \n 1     1     1 TRUE       New Year's Day\n 2     1     1 TRUE       New Year's Day\n 3     1     1 TRUE       New Year's Day\n 4     1     1 TRUE       New Year's Day\n 5     1     1 TRUE       New Year's Day\n 6     1     1 TRUE       New Year's Day\n 7     1     1 TRUE       New Year's Day\n 8     1     1 TRUE       New Year's Day\n 9     1     1 TRUE       New Year's Day\n10     1     1 TRUE       New Year's Day\n# ℹ 336,766 more rows"
  },
  {
    "objectID": "Chapter14.html",
    "href": "Chapter14.html",
    "title": "Chapter 14",
    "section": "",
    "text": "library(tidyverse)\nlibrary(gt)\nlibrary(janitor)\nlibrary(nycflights13)\ndata(\"flights\")"
  },
  {
    "objectID": "Chapter14.html#question-1",
    "href": "Chapter14.html#question-1",
    "title": "Chapter 14",
    "section": "Question 1",
    "text": "Question 1\nHow can you use count() to count the number rows with a missing value for a given variable?\nWe can use count() with weights as is.na(var_name) to the number of rows with missing values, as shown below\n\nflights |&gt;\n  group_by(month) |&gt;\n  summarise(total = n(),\n            missing = sum(is.na(dep_time))) |&gt;\n  gt()\nflights |&gt;\n  group_by(month) |&gt;\n  count(wt = is.na(dep_time)) |&gt;\n  ungroup() |&gt;\n  gt()\n\n\n\n\n\n\n\n  \n    \n    \n      month\n      total\n      missing\n    \n  \n  \n    1\n27004\n521\n    2\n24951\n1261\n    3\n28834\n861\n    4\n28330\n668\n    5\n28796\n563\n    6\n28243\n1009\n    7\n29425\n940\n    8\n29327\n486\n    9\n27574\n452\n    10\n28889\n236\n    11\n27268\n233\n    12\n28135\n1025\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n      month\n      n\n    \n  \n  \n    1\n521\n    2\n1261\n    3\n861\n    4\n668\n    5\n563\n    6\n1009\n    7\n940\n    8\n486\n    9\n452\n    10\n236\n    11\n233\n    12\n1025"
  },
  {
    "objectID": "Chapter14.html#question-2",
    "href": "Chapter14.html#question-2",
    "title": "Chapter 14",
    "section": "Question 2",
    "text": "Question 2\nExpand the following calls to count() to instead use group_by(), summarize(), and arrange():\n\nflights |&gt; count(dest, sort = TRUE)\n\nflights |&gt;\n  group_by(dest) |&gt;\n  summarise(n = n()) |&gt;\n  arrange(desc(n))\n\n# A tibble: 105 × 2\n   dest      n\n   &lt;chr&gt; &lt;int&gt;\n 1 ORD   17283\n 2 ATL   17215\n 3 LAX   16174\n 4 BOS   15508\n 5 MCO   14082\n 6 CLT   14064\n 7 SFO   13331\n 8 FLL   12055\n 9 MIA   11728\n10 DCA    9705\n# ℹ 95 more rows\n\n\nflights |&gt; count(tailnum, wt = distance)\n\nflights |&gt;\n  group_by(tailnum) |&gt;\n  summarise(n = sum(distance))\n\n# A tibble: 4,044 × 2\n   tailnum      n\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 D942DN    3418\n 2 N0EGMQ  250866\n 3 N10156  115966\n 4 N102UW   25722\n 5 N103US   24619\n 6 N104UW   25157\n 7 N10575  150194\n 8 N105UW   23618\n 9 N107US   21677\n10 N108UW   32070\n# ℹ 4,034 more rows"
  },
  {
    "objectID": "Chapter14.html#question-1-1",
    "href": "Chapter14.html#question-1-1",
    "title": "Chapter 14",
    "section": "Question 1",
    "text": "Question 1\nExplain in words what each line of the code used to generate Figure 14.1 does.\nThe explanation is given as annotation in the code below (i.e, lines starting with #): –\n\n# Load in the data-set flights from package nycflights13\nflights |&gt; \n  # Create a variable hour, which is the quotient of the division of\n  # sched_dep_time by 100. Further, group the dataset by this newly \n  # created variable \"hour\" to get data into 24 groups - one for each\n  # hour.\n  group_by(hour = sched_dep_time %/% 100) |&gt; \n  \n  # For each gropu, i.e. all flights scheduled to depart in that\n  # hour, compute the NAs, i.e. cancelled flights, then compute their\n  # mean, i.e. proportion of cancelled flights; and also create a \n  # variable n, which is the total number of flights\n  summarize(prop_cancelled = mean(is.na(dep_time)), n = n()) |&gt; \n  \n  # Remove the flights departing between 12 midnight and 1 am, since\n  # these are very very few, and all are cancelled leading to a highly\n  # skewed and uninformative graph\n  filter(hour &gt; 1) |&gt; \n  \n  # Start a ggplot call, plotting the hour on the x-axis, and \n  # proportion of cancelled flights on the y-axis\n  ggplot(aes(x = hour, y = prop_cancelled)) +\n  \n  # Create a line graph,which joins the proportion of cancelled \n  # flights for each hour. Also, put in in dark grey colour\n  geom_line(color = \"grey50\") + \n  \n  # Add points for each hour, whose size varies with the total number\n  # of flights in that hour\n  geom_point(aes(size = n))"
  },
  {
    "objectID": "Chapter14.html#question-2-1",
    "href": "Chapter14.html#question-2-1",
    "title": "Chapter 14",
    "section": "Question 2",
    "text": "Question 2\nWhat trigonometric functions does R provide? Guess some names and look up the documentation. Do they use degrees or radians?\nR provides several trigonometric functions, most of which operate in radians. Here’s a table listing some of the commonly used trigonometric functions in R, along with short descriptions and information about whether they use degrees or radians:\n\n\n\n\n\n\n\n\nFunction\nDescription\nAngle Measure\n\n\n\n\nsin(x)\nSine function: Computes the sine of the angle x.\nRadians\n\n\ncos(x)\nCosine function: Computes the cosine of the angle x.\nRadians\n\n\ntan(x)\nTangent function: Computes the tangent of the angle x.\nRadians\n\n\nasin(x) or acos(x)\nInverse sine or inverse cosine: Computes the angle whose sine or cosine is x.\nRadians\n\n\natan(x) or atan2(y, x)\nInverse tangent or arctangent: Computes the angle whose tangent is x or the angle between the point (x, y) and the origin.\nRadians\n\n\nsinh(x)\nHyperbolic sine function: Computes the hyperbolic sine of x.\nRadians\n\n\ncosh(x)\nHyperbolic cosine function: Computes the hyperbolic cosine of x.\nRadians\n\n\ntanh(x)\nHyperbolic tangent function: Computes the hyperbolic tangent of x.\nRadians\n\n\n\nIn R, trigonometric functions like sin, cos, and tan expect angles to be in radians by default. However, we can convert between degrees and radians using the deg2rad and rad2deg functions. For example, to compute the sine of an angle in degrees, you can use sin(deg2rad(angle)).\n\n\nCode\ndf = tibble(x = seq(from = -5, to = +5, by = 0.1))\ng = ggplot(df, aes(x = x)) + \n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  labs(x = NULL, y = NULL) +\n  scale_x_continuous(breaks = -5:5)\ngridExtra::grid.arrange(\n  g + geom_line(aes(y = sin(x))) + labs(title = \"sin(x)\"),\n  g + geom_line(aes(y = cos(x))) + labs(title = \"cos(x)\"),\n  g + geom_line(aes(y = tan(x))) + labs(title = \"tan(x)\"),\n  g + geom_line(aes(y = asin(x))) + labs(title = \"asin(x)\"),\n  g + geom_line(aes(y = acos(x))) + labs(title = \"acos(x)\"),\n  g + geom_line(aes(y = atan(x))) + labs(title = \"atan(x)\"),\n  g + geom_line(aes(y = sinh(x))) + labs(title = \"sinh(x)\"),\n  g + geom_line(aes(y = cosh(x))) + labs(title = \"cosh(x)\"),\n  \n  nrow = 2\n)\n\n\n\n\n\nFigure 1: Graphs from some common trigonometric functions"
  },
  {
    "objectID": "Chapter14.html#question-3",
    "href": "Chapter14.html#question-3",
    "title": "Chapter 14",
    "section": "Question 3",
    "text": "Question 3\nCurrently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. You can see the basic problem by running the code below: there’s a gap between each hour.\nflights |&gt;    \n  filter(month == 1, day == 1) |&gt;    \n  ggplot(aes(x = sched_dep_time, y = dep_delay)) +   \n  geom_point()\nConvert them to a more truthful representation of time (either fractional hours or minutes since midnight).\nWe can correct the sched_dep_time() using the two arithmetical functions %/% and %% to generate the decimal representation of time in hours as shown in the code for the graph on the right hand side in\n\n\nCode\ngridExtra::grid.arrange(\n  flights |&gt;    \n    filter(month == 1, day == 1) |&gt;    \n    ggplot(aes(x = sched_dep_time, y = dep_delay)) +   \n    geom_point(size = 0.5) +\n    labs(subtitle = \"Incorrect scheduled departure time\"),\n\n  flights |&gt;\n    mutate(\n      hour_dep = sched_dep_time %/% 100,\n      min_dep  = sched_dep_time %%  100,\n      time_dep = hour_dep + (min_dep/60)\n    ) |&gt;\n    filter(month == 1, day == 1) |&gt;    \n    ggplot(aes(x = time_dep, y = dep_delay)) +   \n    geom_point(size = 0.5) +\n    labs(subtitle = \"Improved and accurate scheduled departure time\",\n         x = \"Scheduled Departure Time (in hrs)\") +\n    scale_x_continuous(breaks = seq(0,24,4)),\n  \n  ncol = 2)\n\n\n\n\n\nFigure 2: Improved represenation of scheduled departure time to remove breaks in the data owing to represention of time as hhmm"
  },
  {
    "objectID": "Chapter14.html#question-4",
    "href": "Chapter14.html#question-4",
    "title": "Chapter 14",
    "section": "Question 4",
    "text": "Question 4\nRound dep_time and arr_time to the nearest five minutes.\n\nattach(flights)\nflights |&gt;\n  slice_head(n = 50) |&gt;\n  mutate(\n    dep_time_5 = round(dep_time/5) * 5,\n    arr_time_5 = round(arr_time/5) * 5,\n    .keep = \"used\"\n  ) |&gt;\n  gt() |&gt;\n  opt_interactive()"
  },
  {
    "objectID": "Chapter14.html#question-1-2",
    "href": "Chapter14.html#question-1-2",
    "title": "Chapter 14",
    "section": "Question 1",
    "text": "Question 1\nFind the 10 most delayed flights using a ranking function. How do you want to handle ties? Carefully read the documentation for min_rank().\nWe can find the 10 most delayed flights by using min_rank() since it gives ties equal rank, while creating gaps, thus keeping the total number of the flights to 10.\n\nflights |&gt;\n  select(sched_dep_time, dep_time, dep_delay, tailnum, dest, carrier) |&gt;\n  mutate(rank_delay = min_rank(desc(dep_delay))) |&gt;\n  arrange(rank_delay) |&gt;\n  slice_head(n = 10) |&gt;\n  gt()\n\n\n\n\n\n  \n    \n    \n      sched_dep_time\n      dep_time\n      dep_delay\n      tailnum\n      dest\n      carrier\n      rank_delay\n    \n  \n  \n    900\n641\n1301\nN384HA\nHNL\nHA\n1\n    1935\n1432\n1137\nN504MQ\nCMH\nMQ\n2\n    1635\n1121\n1126\nN517MQ\nORD\nMQ\n3\n    1845\n1139\n1014\nN338AA\nSFO\nAA\n4\n    1600\n845\n1005\nN665MQ\nCVG\nMQ\n5\n    1900\n1100\n960\nN959DL\nTPA\nDL\n6\n    810\n2321\n911\nN927DA\nMSP\nDL\n7\n    1900\n959\n899\nN3762Y\nPDX\nDL\n8\n    759\n2257\n898\nN6716C\nATL\nDL\n9\n    1700\n756\n896\nN5DMAA\nMIA\nAA\n10"
  },
  {
    "objectID": "Chapter14.html#question-2-2",
    "href": "Chapter14.html#question-2-2",
    "title": "Chapter 14",
    "section": "Question 2",
    "text": "Question 2\nWhich plane (tailnum) has the worst on-time record?\nUsing the code below, we can find the that the flight with tail number N844MH has the highest average departure delay of 32 minutes. However, this flight flew only once, so we might want to re-look for this delay statistic by removing the flights that flew less than 5 times.\nThus, amongst the flights that flew 5 times or more, the highest mean departure delay of the flight with tail number N665MQ.\n\n# The flight tailnum with the highest average delay\nflights |&gt;\n  group_by(tailnum) |&gt;\n  summarize(mean_delay = mean(dep_delay, na.rm = TRUE),\n            n = n()) |&gt;\n  arrange(desc(mean_delay)) |&gt;\n  slice_head(n = 5)\n\n# A tibble: 5 × 3\n  tailnum mean_delay     n\n  &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;\n1 N844MH         297     1\n2 N922EV         274     1\n3 N587NW         272     1\n4 N911DA         268     1\n5 N851NW         233     1\n\n# The flight tailnum with the highest average delay amongst \n# the flights that flew atleast 5 times or more\nflights |&gt;\n  group_by(tailnum) |&gt;\n  summarize(mean_delay = mean(dep_delay, na.rm = TRUE),\n            nos_of_flights = n()) |&gt;\n  filter(nos_of_flights &gt;= 5) |&gt;\n  arrange(desc(mean_delay)) |&gt;\n  slice_head(n = 5)\n\n# A tibble: 5 × 3\n  tailnum mean_delay nos_of_flights\n  &lt;chr&gt;        &lt;dbl&gt;          &lt;int&gt;\n1 N665MQ       177                6\n2 N276AT        84.8              6\n3 N652SW        79.5              6\n4 N919FJ        78                6\n5 N396SW        69.7              7"
  },
  {
    "objectID": "Chapter14.html#question-3-1",
    "href": "Chapter14.html#question-3-1",
    "title": "Chapter 14",
    "section": "Question 3",
    "text": "Question 3\nWhat time of day should you fly if you want to avoid delays as much as possible?\nAs shown in the Figure 4 , we should fly with the early morning flights, particularly 5 am to 7 am, to avoid delays as much as possible.\n\nflights |&gt;\n  mutate(sched_dep_hour = sched_dep_time %/% 100) |&gt;\n  group_by(sched_dep_hour) |&gt;\n  summarize(\n    mean_dep_delay = mean(dep_delay, na.rm = TRUE),\n    nos_of_flights = n()\n  ) |&gt;\n  drop_na() |&gt;\n  ggplot(aes(x = sched_dep_hour,\n             y = mean_dep_delay)) +\n  geom_line() +\n  geom_point(aes(size = nos_of_flights), \n             alpha = 0.5) +\n  theme_light() +\n  labs(x = \"Depature Hour\", y = \"Average departure delay (min)\",\n       title = \"Early morning flights have the least delay\",\n       size = \"Number of flights\") +\n  scale_x_continuous(breaks = seq(5, 24, 2)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 4: Graph of average departure delay vs. scheduled departure time"
  },
  {
    "objectID": "Chapter14.html#question-4-1",
    "href": "Chapter14.html#question-4-1",
    "title": "Chapter 14",
    "section": "Question 4",
    "text": "Question 4\nWhat does flights |&gt; group_by(dest) |&gt; filter(row_number() &lt; 4) do? What does flights |&gt; group_by(dest) |&gt; filter(row_number(dep_delay) &lt; 4) do?\nThe code flights |&gt; group_by(dest) |&gt; filter(row_number() &lt; 4) displays only three flights for each destination, selected on the basis of the order in which they appear in the flights data-set.\n\nflights |&gt; \n  # reducing the number of columns for easy display\n  select(carrier, dest, sched_dep_time, month, day) |&gt;\n  group_by(dest) |&gt; \n  # arrange(dest, sched_dep_time) |&gt;\n  filter(row_number() &lt; 4)\n\n# A tibble: 311 × 5\n# Groups:   dest [105]\n   carrier dest  sched_dep_time month   day\n   &lt;chr&gt;   &lt;chr&gt;          &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 UA      IAH              515     1     1\n 2 UA      IAH              529     1     1\n 3 AA      MIA              540     1     1\n 4 B6      BQN              545     1     1\n 5 DL      ATL              600     1     1\n 6 UA      ORD              558     1     1\n 7 B6      FLL              600     1     1\n 8 EV      IAD              600     1     1\n 9 B6      MCO              600     1     1\n10 AA      ORD              600     1     1\n# ℹ 301 more rows\n\n\nOn the other hand, the code flights |&gt; group_by(dest) |&gt; filter(row_number(dep_delay) &lt; 4) will display the three flights with the least departure delays for each destination. Further, in case of ties, it will select the flight which appears earlier in the data-set, i.e. on a earlier date.\n\nflights |&gt; \n  # reducing the number of columns for easy display\n  select(carrier, dest, sched_dep_time, month, day, dep_delay) |&gt;\n  group_by(dest) |&gt; \n  filter(row_number(dep_delay) &lt; 4)\n\n# A tibble: 310 × 6\n# Groups:   dest [104]\n   carrier dest  sched_dep_time month   day dep_delay\n   &lt;chr&gt;   &lt;chr&gt;          &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n 1 UA      JAC              851     1     1        -3\n 2 EV      AVL              959     1     1       -13\n 3 DL      PWM             2159     1     4       -19\n 4 UA      MTJ              901     1     5        -2\n 5 EV      BTV             2159     1     5       -16\n 6 B6      SJC             1810     1     7        -9\n 7 FL      CAK             2030     1     7       -17\n 8 9E      PIT              750     1     8       -15\n 9 EV      BHM             1858     1     9       -11\n10 EV      MYR              827     1    10       -17\n# ℹ 300 more rows"
  },
  {
    "objectID": "Chapter14.html#question-5",
    "href": "Chapter14.html#question-5",
    "title": "Chapter 14",
    "section": "Question 5",
    "text": "Question 5\nFor each destination, compute the total minutes of delay. For each flight, compute the proportion of the total delay for its destination.\nThe following code does the calculation desired: –\n\nflights |&gt;\n  group_by(dest) |&gt;\n  mutate(\n    total_delay = sum(dep_delay, na.rm = TRUE),\n    prop_delay = dep_delay / total_delay,\n    .keep = \"used\"\n  )\n\n# A tibble: 336,776 × 4\n# Groups:   dest [105]\n   dep_delay dest  total_delay  prop_delay\n       &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1         2 IAH         77012  0.0000260 \n 2         4 IAH         77012  0.0000519 \n 3         2 MIA        103261  0.0000194 \n 4        -1 BQN         11032 -0.0000906 \n 5        -6 ATL        211391 -0.0000284 \n 6        -4 ORD        225840 -0.0000177 \n 7        -5 FLL        151933 -0.0000329 \n 8        -3 IAD         91555 -0.0000328 \n 9        -3 MCO        157661 -0.0000190 \n10        -2 ORD        225840 -0.00000886\n# ℹ 336,766 more rows"
  },
  {
    "objectID": "Chapter14.html#question-6",
    "href": "Chapter14.html#question-6",
    "title": "Chapter 14",
    "section": "Question 6",
    "text": "Question 6\nDelays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag(), explore how the average flight delay for an hour is related to the average delay for the previous hour.\nHere’s the code for the analysis with annotation at appropriate places: –\n\nflights |&gt;    \n  # Generate hour of departure\n  mutate(hour = dep_time %/% 100) |&gt;    \n  \n  # Creating groups by each hour of depature for different days\n  group_by(year, month, day, hour) |&gt;  \n  \n  # Remove 213 flights with NA as hour (i.e., cancelled flights)\n  # to improve the subsequent plotting\n  filter(!is.na(hour)) |&gt;\n  \n  # Average delay and the number of flights in each hour\n  summarize(     \n    dep_delay = mean(dep_delay, na.rm = TRUE),     \n    n = n(),     \n    .groups = \"drop\"   \n  ) |&gt;    \n  \n  # Removing hours in which there were less than 5 flights departing\n  filter(n &gt; 5) |&gt;\n  \n  # Grouping to prevent using 11 pm hour delays as previous delays of the\n  # next day's 5 am flights\n  group_by(year, month, day) |&gt;\n  \n  # A new variabe to show previous hour's average average departure delay\n  mutate(\n    prev_hour_delay = lag(dep_delay),\n    morning_flights = hour == 5\n  ) |&gt;\n  \n  # Plotting to see correlation\n  ggplot(aes(x = dep_delay,\n             y = prev_hour_delay,\n             col = morning_flights)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(se = FALSE) +\n  geom_abline(slope = 1, intercept = 0, col = \"grey\") +\n  theme_light() +\n  coord_fixed() +\n  scale_x_continuous(breaks = seq(0,300,60)) +\n  scale_y_continuous(breaks = seq(0,300,60)) +\n  scale_color_discrete(labels = c(\"Other flights\",\n                                  \"Early Morning flights (5 am to 6 am)\")) +\n  labs(x = \"Average Departure Delay in an hour (min)\",\n       y = \"Average Departure Delay in the previous hour (min)\",\n       col = NULL,\n       title = \"Departure Delay correlates with delay in previous hour\",\n       subtitle = \"The average departure delay in any hour is worse than previous hours's average delay. \\nFurther, the early morning flights' delay doesn't depend on previous nights' delay.\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 5: Plots of average departure delays in each hour vs. average departure delay in the previous hour"
  },
  {
    "objectID": "Chapter14.html#question-7",
    "href": "Chapter14.html#question-7",
    "title": "Chapter 14",
    "section": "Question 7",
    "text": "Question 7\nLook at each destination. Can you find flights that are suspiciously fast (i.e. flights that represent a potential data entry error)? Compute the air time of a flight relative to the shortest flight to that destination. Which flights were most delayed in the air?\nThe Table 1 computes the average air time to each destination, and then computes the ratio of each flight’s airtime to the average air time. There are four flights with this ratio less than 0.6. Out of these, the flight N666DN to ATL destination with air time of just 65 minutes (compared to average 113 minutes) appears to be suspiciously fast, and a potential data entry error.\n\nflights |&gt;\n  select(month, day, dest, tailnum, dep_time, arr_time, air_time) |&gt;\n  group_by(dest) |&gt;\n  mutate(\n    mean_air_time = mean(air_time, na.rm = TRUE),\n    fast_ratio = air_time/mean_air_time\n  ) |&gt;\n  relocate(air_time, mean_air_time, fast_ratio) |&gt;\n  ungroup() |&gt;\n  arrange(fast_ratio) |&gt;\n  filter(fast_ratio &lt; 0.6) |&gt;\n  gt() |&gt;\n  fmt_number(columns = c(mean_air_time, fast_ratio),\n    decimals = 2)\n\n\n\n\n\nTable 1:  Suspiciously fast flights compared to the average air time for that\ndestination \n  \n    \n    \n      air_time\n      mean_air_time\n      fast_ratio\n      month\n      day\n      dest\n      tailnum\n      dep_time\n      arr_time\n    \n  \n  \n    21\n38.95\n0.54\n3\n2\nBOS\nN947UW\n1450\n1547\n    65\n112.93\n0.58\n5\n25\nATL\nN666DN\n1709\n1923\n    55\n93.39\n0.59\n5\n13\nGSP\nN14568\n2040\n2225\n    23\n38.95\n0.59\n1\n25\nBOS\nN947UW\n1954\n2131\n  \n  \n  \n\n\n\n\n\nThe following code also computes the air time (air_time)of every flight relative to the shortest flight (min_air_time) for each destination in form a ratio (rel_ratio), and Figure 6 displays top 100 flights that were most delayed in the air. We observe that, of the 100 most delayed flights in-air: –\n\n83 of these are flights to Boston (BOS); with shortest air time of 21 minutes (a potential data entry) causing this over-representation of Boston flights in the current computation.\nOnce Boston is removed, most of top 100 delayed in-air flights belong to destinations PHL , DCA and ATL. These again either represent a erroneous data entry (flight N666DN to ATL destination with air time of just 65 minutes) or very short glifht destinations (PHL and DCA).\n\n\nflights |&gt;\n  select(month, day, dest, tailnum, dep_time, arr_time, air_time) |&gt;\n  group_by(dest) |&gt;\n  mutate(\n    min_air_time = min(air_time, na.rm = TRUE),\n    rel_ratio = air_time/min_air_time\n  ) |&gt;\n  relocate(air_time, min_air_time, rel_ratio) |&gt;\n  ungroup() |&gt;\n  arrange(desc(rel_ratio)) |&gt;\n  slice_head(n = 100) |&gt;\n  gt() |&gt;\n  fmt_number(columns = c(min_air_time, rel_ratio),\n    decimals = 2) |&gt;\n  opt_interactive()\n\n\n\n\n\n\n\n\nFigure 6: Flights that were most delayed in the air-time"
  },
  {
    "objectID": "Chapter14.html#question-8",
    "href": "Chapter14.html#question-8",
    "title": "Chapter 14",
    "section": "Question 8",
    "text": "Question 8\nFind all destinations that are flown by at least two carriers. Use those destinations to come up with a relative ranking of the carriers based on their performance for the same destination.\nFirst, as shown in Figure 7 , we find out the destinations which have at-least two or more carriers. There are 76 such destinations.\n\n\nCode\n# Computing the destinations with atleast two carriers\ndf1 = flights |&gt;\n  group_by(dest) |&gt;\n  summarize(no_of_carriers = n_distinct(carrier)) |&gt;\n  filter(no_of_carriers &gt; 1)\n\n# Extracting the names of these carriers as a vector to use in filtering\n# the data in subsequent steps, as a vector called \"dest2\"\ndest2 = df1 |&gt;\n  select(dest) |&gt;\n  as_vector() |&gt;\n  unname()\n\n# Displaying the table\ndf1 |&gt;\n  arrange(desc(no_of_carriers)) |&gt;\n  gt() |&gt;\n  opt_interactive(page_size_default = 5)\n\n\n\n\n\n\n\n\n\nFigure 7: Destinations with two or more carriers\n\n\n\nThen, in Figure 8 , we compute the three important parameters that we use to compare the performance of carriers: –\n\nAverage departure delay.\nProportion of flights cancelled.\nAverage delay in Air Time.\n\n\n\nCode\n# Ranking parameters can be based on: ---\n# 1. Average Departure Delay\n# 2. Proportion of flights cancelled\n# 3. Average Delay in Air Time\n# Note: I have not included arrival delay, as that will automatically\n#       include the delay in the departure, and penalize the airline \n#       twice for a departure delay.\n\ndf2 = flights |&gt;\n  filter(dest %in% dest2) |&gt;\n  group_by(dest, carrier) |&gt;\n  summarise(\n    mean_dep_delay = mean(dep_delay, na.rm = TRUE),\n    prop_cancelled = mean(is.na(dep_time)),\n    mean_air_time  = mean(air_time, na.rm = TRUE),\n    nos_of_flights = n()\n  ) |&gt;\n  drop_na()\n  \ndf2 |&gt;\n  ungroup() |&gt;\n  gt() |&gt;\n  fmt_number(decimals = 2,\n             columns = -nos_of_flights) |&gt;\n  opt_interactive()\n\n\n\n\n\n\n\n\n\nFigure 8: Table displaying the comparison parameters for each carrier at each destination\n\n\n\nThereafter, in Table 2 , we compute scores based on these three parameters for each carrier and rank them accordingly.\n\n\nCode\n# Ranking can be generated on basis of the least score calculated after\n# giving equal weightage to the following in formula for each carrier as\n# calculated by: (x - min(x)) / (max(x) - min(x))\n# 1. Average Departure Delay\n# 2. Proportion of flights cancelled\n# 3. Average Delay in Air Time\ndf3 = df2 |&gt;\n  mutate(\n    score_delay = (mean_dep_delay - min(mean_dep_delay))/(max(mean_dep_delay) - min(mean_dep_delay)),\n    score_cancel = (prop_cancelled - min(prop_cancelled)) / (max(prop_cancelled) - min(prop_cancelled)),\n    score_at = (mean_air_time - min(mean_air_time)) / (max(mean_air_time) - min(mean_air_time)),\n    total_score = score_delay + score_cancel + score_at,\n    rank_carrier = min_rank(total_score)\n  ) |&gt;\n  drop_na()\n\n# Displaying an example ranking for ATL and AUS\ndf3 |&gt;\n  filter(dest %in% c(\"ATL\", \"AUS\")) |&gt;\n  gt() |&gt;\n  fmt_number(columns = -c(rank_carrier, nos_of_flights)) |&gt;\n  tab_spanner(label = \"Scoring and Rank\",\n              columns = score_delay:rank_carrier) |&gt;\n  tab_spanner(label = \"Statistics\",\n              columns = mean_dep_delay:nos_of_flights) |&gt;\n  opt_stylize(style = 1)\n\n\n\n\n\n\nTable 2:  Computed rankings with the method of scoring displayed for two\ndestinations \n  \n    \n    \n      carrier\n      \n        Statistics\n      \n      \n        Scoring and Rank\n      \n    \n    \n      mean_dep_delay\n      prop_cancelled\n      mean_air_time\n      nos_of_flights\n      score_delay\n      score_cancel\n      score_at\n      total_score\n      rank_carrier\n    \n  \n  \n    \n      ATL\n    \n    9E\n0.96\n0.03\n120.11\n59\n0.00\n0.60\n0.75\n1.34\n4\n    DL\n10.41\n0.01\n112.16\n10571\n0.44\n0.15\n0.00\n0.59\n1\n    EV\n22.40\n0.06\n114.05\n1764\n1.00\n1.00\n0.18\n2.18\n7\n    FL\n18.45\n0.02\n114.44\n2337\n0.82\n0.36\n0.21\n1.39\n6\n    MQ\n9.35\n0.03\n113.70\n2322\n0.39\n0.56\n0.14\n1.10\n3\n    UA\n15.79\n0.00\n113.65\n103\n0.69\n0.00\n0.14\n0.83\n2\n    WN\n2.34\n0.02\n122.81\n59\n0.06\n0.30\n1.00\n1.36\n5\n    \n      AUS\n    \n    9E\n19.00\n0.00\n222.00\n2\n1.00\n0.00\n1.00\n2.00\n5\n    AA\n15.25\n0.02\n215.07\n365\n0.75\n1.00\n0.36\n2.11\n6\n    B6\n14.94\n0.00\n213.85\n747\n0.73\n0.24\n0.25\n1.22\n3\n    DL\n10.82\n0.01\n211.96\n357\n0.46\n0.51\n0.07\n1.04\n2\n    UA\n14.92\n0.01\n211.28\n670\n0.73\n0.54\n0.01\n1.28\n4\n    WN\n3.84\n0.01\n211.18\n298\n0.00\n0.61\n0.00\n0.61\n1\n  \n  \n  \n\n\n\n\n\nThe final ranking of the carriers for each destination is displayed below in Table 3 with names of destinations as column names, and within each column, the best carrier at the top, worst carrier at the bottom.\n\n\nCode\ndf3 |&gt;\n  arrange(dest, rank_carrier) |&gt;\n  select(dest, carrier, rank_carrier) |&gt;\n  ungroup() |&gt;\n  pivot_wider(names_from = rank_carrier,\n              values_from = carrier) |&gt;\n  t() |&gt;\n  as_tibble() |&gt;\n  janitor::row_to_names(row_number = 1) |&gt;\n  gt() |&gt;\n  sub_missing(missing_text = \"\") |&gt;\n  gtExtras::gt_theme_538()\n\n\n\n\n\n\nTable 3:  Ranks of carriers for each destination: Best carrier at top, worst\ncarrier at bottom \n  \n    \n    \n      ATL\n      AUS\n      AVL\n      BDL\n      BNA\n      BOS\n      BQN\n      BTV\n      BUF\n      BWI\n      CAE\n      CHS\n      CLE\n      CLT\n      CMH\n      CVG\n      DAY\n      DCA\n      DEN\n      DFW\n      DSM\n      DTW\n      EGE\n      FLL\n      GRR\n      GSO\n      GSP\n      HNL\n      HOU\n      IAD\n      IAH\n      IND\n      JAC\n      JAX\n      LAS\n      LAX\n      MCI\n      MCO\n      MEM\n      MHT\n      MIA\n      MKE\n      MSN\n      MSP\n      MSY\n      MVY\n      OMA\n      ORD\n      ORF\n      PBI\n      PDX\n      PHL\n      PHX\n      PIT\n      PWM\n      RDU\n      RIC\n      ROC\n      RSW\n      SAN\n      SAT\n      SDF\n      SEA\n      SFO\n      SJC\n      SJU\n      SLC\n      SRQ\n      STL\n      STT\n      SYR\n      TPA\n      TVC\n      TYS\n      XNA\n    \n  \n  \n    DL\nWN\n9E\nEV\nDL\nDL\nB6\n9E\nDL\nWN\n9E\nUA\nUA\nUS\nMQ\nDL\n9E\nUA\nDL\nEV\nEV\nDL\nUA\nDL\nEV\n9E\n9E\nHA\nB6\nUA\nUA\nUA\nDL\nDL\nVX\nDL\nDL\nDL\nDL\n9E\nAA\nWN\n9E\nUA\nUA\n9E\nUA\nUA\nEV\nUA\nDL\nDL\nDL\nUA\nDL\nUA\nEV\nB6\n9E\nDL\nUA\nUA\nAS\nDL\nVX\nDL\nDL\nEV\nDL\nDL\nB6\n9E\nMQ\nEV\nEV\n    UA\nDL\nEV\nUA\nWN\nAA\nUA\nB6\nEV\nEV\nEV\nB6\nMQ\nB6\nEV\n9E\nEV\nDL\nUA\nUA\n9E\nUA\nAA\nUA\n9E\nEV\nEV\nUA\nWN\nOO\nAA\nDL\nUA\nB6\nDL\nB6\n9E\nAA\n9E\nEV\nDL\nEV\nEV\nDL\nDL\nB6\nDL\nAA\nMQ\nDL\nUA\nYV\nUA\nDL\nB6\nB6\n9E\n9E\nB6\nB6\nDL\n9E\nDL\nUA\nB6\nB6\nB6\nB6\nWN\nAA\n9E\nEV\nEV\n9E\nMQ\n    MQ\nB6\n\n\nMQ\nUS\n\nEV\nB6\n9E\n\n9E\nOO\nMQ\n9E\nEV\n\nUS\nB6\n9E\n\nOO\n\nB6\n\n\n\n\n\nB6\n\nMQ\n\n9E\nB6\nUA\nEV\nUA\nEV\n\nUA\n9E\n\nOO\nWN\n\nEV\nEV\n9E\nB6\nB6\nUS\nUS\nMQ\nEV\nEV\n\nEV\nUA\nUA\n9E\nEV\nB6\nB6\n\nUA\n\nDL\nUA\nUA\nEV\nMQ\n\n\n\n    9E\nUA\n\n\n9E\nEV\n\n\n9E\nMQ\n\nEV\nEV\nUA\n\nMQ\n\n9E\nF9\nAA\n\nMQ\n\nAA\n\n\n\n\n\n9E\n\nEV\n\nEV\nUA\nAA\n\nB6\n\n\n\nFL\n\nMQ\nB6\n\n\nB6\n\nEV\n\nEV\nB6\nEV\n\nMQ\n\n\nDL\nAA\n\n\nUA\nVX\n\nAA\n\n9E\nAA\n\n\nUA\n\n\n\n    WN\n9E\n\n\nEV\nB6\n\n\n\n\n\n\n9E\nEV\n\n\n\nMQ\nWN\n\n\nEV\n\n\n\n\n\n\n\nEV\n\n9E\n\n\nAA\nVX\n\n\n\n\n\n\n\n9E\n9E\n\n\nMQ\n\nAA\n\n9E\nWN\n9E\n\n9E\n\n\n\n\n\n\nAA\nAA\n\n\n\n\nMQ\n\n\nDL\n\n\n\n    FL\nAA\n\n\n\nUA\n\n\n\n\n\n\n\nYV\n\n\n\nEV\n\n\n\n9E\n\n\n\n\n\n\n\nYV\n\n\n\n\n\n\n\n\n\n\n\n\n\nEV\nEV\n\n\n9E\n\n\n\n\n\nB6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEV\n\n\nAA\n\n\n\n    EV\n\n\n\n\n9E\n\n\n\n\n\n\n\n9E\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nB6"
  },
  {
    "objectID": "Chapter14.html#various-ranking-functions-in-r",
    "href": "Chapter14.html#various-ranking-functions-in-r",
    "title": "Chapter 14",
    "section": "Various Ranking functions in R",
    "text": "Various Ranking functions in R\n\n\n\n\n\n\n\n\nFunction\nDescription\nHandling Ties\n\n\n\n\nmin_rank()\nAssigns the minimum rank to each element. The way ranks are usually computed in sports.\nTied values get the same rank; next rank is skipped.\n\n\nrow_number()\nAssigns every input a unique rank. Ties are given ranks based on the order of appearance, i.e. row number.\nTied values get different ranks, depending on the order of appearance in the data set.\n\n\ndense_rank()\nAssigns the minimum rank to each element. And, assigns the same rank to tied values without gaps.\nTied values get the same rank; no gaps in ranks.\n\n\npercent_rank()\nComputes the rank as a percentage of the total number of elements. It counts the total number of values less than xi, and divides it by the number of observations minus 1.\nTied values get the same rank; no gaps in ranks.\n\n\ncume_dist()\nComputes the cumulative distribution function (CDF) of the data. It counts the total number of values less than or equal to xi, and divides it by the number of observations.\nTied values get the same rank; no gaps in ranks.\n\n\n\n\n\nCode\ndf &lt;- data.frame(Name = c(\"Alice\", \"Bob\", \"Carol\", \"David\", \"Eve\"),                  Score = c(85, 92, 85, 78, 92))   \n\n# Apply ranking functions and store the results in new columns   \ndf |&gt;      \n  mutate(Min_Rank = min_rank(Score),                    \n         Row_Number = row_number(Score),                    \n         Dense_Rank = dense_rank(Score),                    \n         Percent_Rank = percent_rank(Score),                    \n         Cumulative_Dist = cume_dist(Score)) |&gt;      \n  gt()\n\n\n\n\n\n\n\n  \n    \n    \n      Name\n      Score\n      Min_Rank\n      Row_Number\n      Dense_Rank\n      Percent_Rank\n      Cumulative_Dist\n    \n  \n  \n    Alice\n85\n2\n2\n2\n0.25\n0.6\n    Bob\n92\n4\n4\n3\n0.75\n1.0\n    Carol\n85\n2\n3\n2\n0.25\n0.6\n    David\n78\n1\n1\n1\n0.00\n0.2\n    Eve\n92\n4\n5\n3\n0.75\n1.0\n  \n  \n  \n\n\nFigure 3: Various Ranking Functions"
  },
  {
    "objectID": "Chapter14.html#question-1-3",
    "href": "Chapter14.html#question-1-3",
    "title": "Chapter 14",
    "section": "Question 1",
    "text": "Question 1\nBrainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights. When is mean() useful? When is median() useful? When might you want to use something else? Should you use arrival delay or departure delay? Why might you want to use data from planes?\nThe different ways to assess the delay characteristics of a group of flights in form of a tabular comparison of various measures of central tendency are shown below: –\n\n\n\n\n\n\n\n\n\n\n\nDefinition\nAppropriate Data Types\nRobustness to Outliers\nSuitability\n\n\n\n\nMean\nmean()\nThe sum of all values divided by the number of values.\nInterval, Ratio\nSensitive to outliers\nCommonly used for normally distributed data.\n\n\nMedian\nmedian()\nThe middle value in a dataset when values are ordered, i.e., 50th percentile.\nOrdinal, Interval, Ratio\nRobust to outliers\nSuitable for skewed data and data with outliers.\n\n\nTrimmed Mean\nmean(x, trim = 0.05)\nMean calculated after removing a certain percentage of extreme values.\nInterval, Ratio\nReduces the impact of outliers\nUseful for data containing outliers, i.e. flights with abnormally high delay\n\n\nWeighted Mean\nweighted.mean(x, weights)\nThe sum of the products of values and their associated weights divided by the sum of the weights.\nInterval, Ratio\nDepends on the weighting scheme\nAppropriate when different data points have different levels of importance. Example, if we give more importance to delay in short-haul flights, i.e., ratio of dep_delay to air_time\n\n\nInter-Quartile Range\nIQR()\nThe difference between 25th and 75th percentile values in a dataset.\nInterval, Ratio\nNot sensitive to outliers\nProvides a simple measure of central dispersion in the data.\n\n\nPercentile\nquantile()\nThe value below which a given percentage of data falls, e.g. 95th percentile\nInterval, Ratio\nNot influenced by outliers\nUseful for identifying specific positions within a dataset. We could find 95th percentile instead to maximum dep_delay to avoid data entry errors affecting our computation.\n\n\nWinsorized Mean\nDescTools::Winsorize()\nMean calculated after replacing extreme values with less extreme ones, say 5th and 95th percentile.\nInterval, Ratio\nReduces the impact of outliers\nUseful when we want to retain some information from outliers.\n\n\n\nOur choice of a measure depends on the characteristics of flights delay data and our specific analytical objectives.\n\nmean() is useful when: –\n\nWhen we want to find the typical or central value of a dataset, sensitive to all data points.\nWhen our data does not contain extreme outliers. That is, it is commonly used for normally distributed data or data that follows a symmetric bell-shaped curve.\n\nmedian() is useful when: –\n\nWhen we want to find the middle value of a dataset. We want this when our data contains extreme values and outliers, perhaps due to data entry error.\nMedian is robust to outliers and is a better choice when the data contains extreme values or is skewed.\nCaution: median() may not provide as much information about the distribution of data as the mean() does. It doesn’t consider the magnitude of differences between values.\n\nWhen to use something else:\n\nMode: The mode is useful when we want to identify the most frequently occurring value(s) in a dataset. It’s suitable for nominal data (categories or labels) and not for departure delay data.\nGeometric Mean: If we’re dealing with positive data that follows a multiplicative relationship (e.g., growth rates, financial returns), the geometric mean may be more appropriate than the arithmetic mean.\nTrimmed Mean: When dealing with data that contains outliers, we can calculate a trimmed mean by removing a certain percentage of extreme values from both ends of the dataset. This can help reduce the impact of outliers on the mean.\nWeighted Mean: If different data points have different levels of importance or weight, we can calculate a weighted mean to give more weight to specific values.\n\n\nWe should use departure delay instead of arrival delay, as most of the delay occurs only in departure. Once in air, the delay is minimal, and even if so, it is unavoidable due to safety reasons. The following table compares departure and arrival delays, and in my view, since our purpose of analysis is to compare airline carriers, we might use departure delay.\n\n\n\n\n\n\n\n\n\nDeparture Delay\nArrival Delay\n\n\n\n\nPros\nUseful for assessing punctuality in departing flights.\nImportant for passengers concerned about on-time departure.\nProvides a comprehensive view of overall travel delays.\nReflects the entire flight experience, including in-flight issues.\n\n\nCons\nDoesn’t account for delays occurring during the flight.\nLess relevant for passengers with long layovers or no connecting flights.\nMay not fully represent the passenger’s overall experience.\nDoesn’t provide specific insight into departure punctuality.\nMay not be as crucial for passengers with direct flights.\nMay not reflect the airline’s performance in terms of prompt departures.\n\n\n\nWe might use data from planes (to get flight aircraft make and year) and flights together as shown in the analysis below: –\n\n\nCode\n# Compute mean, median and trimmed mean for departure delay and delay\n# in the airtime (i.e., time lost in air calculated by arr_delay minus \n# dep_delay).\ndf1 = \nflights |&gt;\n  group_by(tailnum) |&gt;\n  summarize(\n    mean_dep_delay = mean(dep_delay, na.rm = TRUE),\n    median_dep_delay = median(dep_delay, na.rm = TRUE),\n    trim_mean_dep_delay = mean(dep_delay, na.rm = TRUE, trim = 0.025),\n    mean_air_time_loss = mean(arr_delay - dep_delay, na.rm = TRUE),\n    median_air_time_loss = median(arr_delay - dep_delay, na.rm = TRUE)\n  )\n\ndf1 = inner_join(df1, planes, by = \"tailnum\")\n\n\ndf1 |&gt;\n  group_by(engine) |&gt;\n  summarize(\n    median_delay = mean(median_dep_delay, na.rm = TRUE),\n    nos = n()\n  ) |&gt;\n  mutate(is_positive = if_else(median_delay &gt; 0, +1, -1)) |&gt;\n  ggplot(aes(x = median_delay, \n             y = engine,\n             fill = factor(is_positive),\n             label = paste0(\"Flights = \", nos))) +\n  geom_bar(stat = \"identity\") +\n  geom_text(hjust = 0.5) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(-3, 5, 1)) +\n  scale_fill_manual(values = c(\"#119644\", \"#ed523e\")) +\n  labs(x = \"Average of the Median Depature Delays of each airplane (in minutes)\",\n       y = NULL,\n       title = \"Turbo-jet aircrafts have highest departure delays on average\") +\n  theme(axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        legend.position = \"blank\",\n        panel.grid = element_blank())\n\n\n\n\n\nFigure 9: Bar plot of the average of median departure delays for each aircraft type (make)\n\n\n\n\n\n\nCode\ntop_m = df1 |&gt;\n  group_by(manufacturer) |&gt;\n  count() |&gt;\n  filter(n &gt; 100) |&gt;\n  select(manufacturer) |&gt;\n  as_vector() |&gt;\n  unname()\n\ndf1 |&gt;\n  filter(manufacturer %in% top_m) |&gt;\n  group_by(manufacturer) |&gt;\n  summarize(\n    nos = n(),\n    median_delay = mean(median_dep_delay, na.rm = TRUE)\n  ) |&gt;\n  mutate(is_positive = if_else(median_delay &gt; 0, +1, -1)) |&gt;\n  ggplot(aes(x = median_delay, \n             y = reorder(manufacturer, -median_delay),\n             fill = factor(is_positive),\n             label = paste0(\"Flights = \", nos))) +\n  geom_bar(stat = \"identity\") +\n  geom_text(position = \"fill\", hjust = 1.2) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(-4, 4, 1)) +\n  scale_fill_manual(values = c(\"#119644\", \"#ed523e\")) +\n  labs(x = \"Average of the Median Depature Delays of flights of the manufacturer's aircrafts (in minutes)\",\n       y = NULL,\n       title = \"Average Departure Delays vary by the aircrafts' manufacturers: Boeing's airplanes get delayed the most\") +\n  theme(axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        legend.position = \"blank\",\n        panel.grid = element_blank(),\n        plot.title.position = \"plot\")\n\n\n\n\n\nFigure 10: Bar plot of the average of median departure delays for each aircraft manufacturer"
  },
  {
    "objectID": "Chapter14.html#question-2-3",
    "href": "Chapter14.html#question-2-3",
    "title": "Chapter 14",
    "section": "Question 2",
    "text": "Question 2\nWhich destinations show the greatest variation in air speed?\nTo find the destinations that show the greatest variation in air speed, we can group the flights data-set by dest and then find the destinations which have the highest coefficient of variability, i.e. \\(CV = \\frac{sd}{\\text{mean}} \\cdot 100\\), expressed as a percentage. The top 5 destinations with the greatest variation in air speed are in Table 4 .\nThe \\(CV\\) is expressed as a percentage and is useful to compare the relative variability of different destinations’ air speed because the mean airspeed for each destination is different.\nThe usefulness of the coefficient of variation depends on the context and the nature of the data you are dealing with. Here are a few points to consider. A higher \\(CV\\) indicates greater relative variability compared to the mean, while a lower CV suggests less relative variability.\n\n\nCode\nflights |&gt;\n  mutate(speed = 60 * distance/air_time) |&gt;\n  group_by(dest) |&gt;\n  summarise(\n    nos = n(),\n    mean = mean(speed, na.rm = TRUE),\n    sd   = sd(speed, na.rm = TRUE),\n    CV   = sd/mean) |&gt;\n  arrange(desc(CV)) |&gt;\n  slice_head(n = 5) |&gt;\n  gt() |&gt;\n  fmt_number(columns = -nos, \n             decimals = 2) |&gt;\n  fmt_percent(columns = \"CV\") |&gt;\n  cols_label(\n    dest = \"Destination\",\n    nos = \"Number of flights\",\n    mean = \"Mean Air Speed (mph)\",\n    sd = \"Std. Dev. Air Speed\",\n    CV = \"Coeff. of Variation\"\n  ) |&gt;\n  gtExtras::gt_theme_538()\n\n\n\n\n\n\nTable 4:  The top 5 destinations with the greatest variation in air speed \n  \n    \n    \n      Destination\n      Number of flights\n      Mean Air Speed (mph)\n      Std. Dev. Air Speed\n      Coeff. of Variation\n    \n  \n  \n    PHL\n1632\n176.45\n30.66\n17.38%\n    DCA\n9705\n280.76\n34.04\n12.12%\n    BOS\n15508\n297.62\n32.57\n10.94%\n    BDL\n443\n277.01\n29.85\n10.77%\n    ACK\n265\n288.95\n30.56\n10.58%\n  \n  \n  \n\n\n\n\n\nFurther, we can go one step ahead, and find whether there is a relation between destinations and air speeds along with their variability, graphically as shown in\n\n\nCode\ndest_vec = flights |&gt;\n  group_by(dest) |&gt;\n  count() |&gt;\n  filter(n &gt; 10000) |&gt;\n  select(dest) |&gt;\n  as_vector() |&gt;\n  unname()\n  \nflights |&gt;\n  filter(dest %in% dest_vec) |&gt;\n  mutate(speed = 60 * distance/air_time) |&gt;\n  group_by(dest) |&gt;\n  mutate(m_s = mean(speed, na.rm = TRUE)) |&gt;\n  ggplot(aes(y = reorder(dest, m_s), \n             x = speed)\n         ) +\n  geom_boxplot(outlier.alpha = 0.2,\n               fill = \"#cbc6cf\") +\n  theme_minimal() +\n  labs(x = \"Air speed (miles per hour)\",\n       y = NULL, \n       title = \"Farther destinations have higher average air speed of aircrafts, and lesser variability\",\n       subtitle = \"Los Angeles bound aircrafts' average speed is much higher than Boston-bound aircrafts, but spread of data is lesser\") +\n  theme(panel.grid.major.y = element_blank(),\n        panel.grid.minor = element_blank(),\n        plot.title.position = \"plot\") +\n  scale_x_continuous(breaks = seq(100, 600, 100)) +\n  coord_cartesian(xlim = c(100, 600))\n\n\n\n\n\nFigure 11: A box-plot of the air speeds for the 10 most frequent destinations, with &gt;10,000 flights in 2013"
  },
  {
    "objectID": "Chapter14.html#question-3-2",
    "href": "Chapter14.html#question-3-2",
    "title": "Chapter 14",
    "section": "Question 3",
    "text": "Question 3\nCreate a plot to further explore the adventures of EGE. Can you find any evidence that the airport moved locations? Can you find another variable that might explain the difference?\nOnce we start exploring the data, firstly, we observe in Figure 12 and Table 5 that the flight distances of the 213 flights to EGE in 2013 cluster into two groups: one around 1725 miles and the other around 1746 miles. Thus, this is one piece of evidence that the EGE airport changed locations.\n\n\nCode\nflights |&gt;\n  filter(dest == \"EGE\") |&gt;\n  group_by(distance) |&gt;\n  count() |&gt;\n  ungroup() |&gt;\n  gt() |&gt;\n  cols_label(\n    distance = \"Distance (in miles)\",\n    n = \"Number of flights in 2013\"\n  ) |&gt;\n  gtExtras::gt_theme_538()\n\n\n\n\n\n\nTable 5:  Flight distances for flights to EGE \n  \n    \n    \n      Distance (in miles)\n      Number of flights in 2013\n    \n  \n  \n    1725\n51\n    1726\n59\n    1746\n44\n    1747\n59\n  \n  \n  \n\n\n\n\n\n\n\nCode\nflights |&gt;\n  filter(dest == \"EGE\") |&gt;\n  ggplot(aes(x = distance, y = 1)) +\n  geom_jitter(width = 0, \n              height = 0.1,\n              alpha = 0.3) +\n  theme_minimal() +\n  coord_cartesian(xlim = c(1500, 1800),\n                  ylim = c(0.8, 1.2)) +\n  scale_x_continuous(breaks = c(1500, 1600, 1700, 1725, 1745, 1800)) +\n  labs(title = \"Flight distances to EGE cluster into two groups around 1725 and 1746 miles\", \n       x = \"Flight Distance (miles)\", y = NULL) +\n  theme(panel.grid.minor = element_blank(),\n        panel.grid.major.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\n\n\n\n\nFigure 12: Scatter Plot of flight distances for the 213 flights to EGE in 2013\n\n\n\n\nNow, upon exploring further, we find another variable, i.e., origin of the flights might explain the difference since EWR and JFK are slightly away from each other, even within New York City metropolitan area. The Table 6 explains this, and we conclude that after all, EGE airport may not have shifted after all.\n\n\nCode\nflights |&gt;\n  filter(dest == \"EGE\") |&gt;\n  group_by(origin) |&gt;\n  summarise(\n    mean_of_distance = mean(distance, na.rm = TRUE),\n    std_dev_of_distance = sd(distance, na.rm = TRUE),\n    proportion_of_cancelled_flights = mean(is.na(dep_time)),\n    number_of_flights = n()\n  ) |&gt;\n  gt() |&gt;\n  fmt_percent(columns = proportion_of_cancelled_flights) |&gt;\n  fmt_number(decimals = 2,\n             columns = c(mean_of_distance, std_dev_of_distance)) |&gt;\n  cols_label_with(fn = ~ janitor::make_clean_names(., case = \"title\")) |&gt;\n  gtExtras::gt_theme_pff()\n\n\n\n\n\n\nTable 6:  Comparison of EGE-bound flights’ distance based on airport of\norigin \n  \n    \n    \n      Origin\n      Mean of Distance\n      Std Dev of Distance\n      Proportion of Cancelled Flights\n      Number of Flights\n    \n  \n  \n    EWR\n1,725.54\n0.50\n2.73%\n110\n    JFK\n1,746.57\n0.50\n1.94%\n103"
  },
  {
    "objectID": "Chapter15.html",
    "href": "Chapter15.html",
    "title": "Chapter 15",
    "section": "",
    "text": "Important uses of quoting in R: –"
  },
  {
    "objectID": "Chapter15.html#question-1",
    "href": "Chapter15.html#question-1",
    "title": "Chapter 15",
    "section": "Question 1",
    "text": "Question 1\nCreate strings that contain the following values:\n\nHe said \"That's amazing!\"\n\nx = \"He said \\\"That's amazing!\\\"\"\nstr_view(x)\n\n[1] │ He said \"That's amazing!\"\n\n\n\\a\\b\\c\\d\n\nx = \"\\\\a\\\\b\\\\c\\\\d\"\nstr_view(x)\n\n[1] │ \\a\\b\\c\\d\n\n\n\\\\\\\\\\\\\n\nx = \"\\\\\\\\\\\\\\\\\\\\\\\\\"\nstr_view(x)\n\n[1] │ \\\\\\\\\\\\"
  },
  {
    "objectID": "Chapter15.html#question-2",
    "href": "Chapter15.html#question-2",
    "title": "Chapter 15",
    "section": "Question 2",
    "text": "Question 2\nCreate the string in your R session and print it. What happens to the special “\\u00a0”? How does str_view() display it? Can you do a little googling to figure out what this special character is?\nx &lt;- \"This\\u00a0is\\u00a0tricky\"\nThe \"\\u00a0\" represents a white space. By google, I find out that this represents No-Break Space (NBSP). But, str_view() displays it in form of a greenish-blue font {\\u00a0}.\n\n\"\\u00a0\" # This represents a white space\n\n[1] \" \"\n\nstr_view(\"\\u00a0\")\n\n[1] │ {\\u00a0}\n\nx &lt;- \"This\\u00a0is\\u00a0tricky\"\nprint(x)\n\n[1] \"This is tricky\"\n\nstr_view(x)\n\n[1] │ This{\\u00a0}is{\\u00a0}tricky\n\n\nThe \"\\u00a0\" represents a non-breaking space character in Unicode encoding. Unicode is a standardized character encoding system that assigns a unique numerical code to almost every character from every writing system in the world, including various symbols, letters, and special characters.\nIn Unicode, “\\u” is used to indicate that the following four characters represent a Unicode code point in hexadecimal notation. In this case, \"\\u00a0\" represents the code point for the non-breaking space character.\nA non-breaking space is a type of space character that is used in typography and word processing to prevent a line break or word wrap from occurring at that particular space.\nIt is similar to a regular space character (ASCII code 32), but it has the special property of keeping adjacent words or characters together on the same line when text is justified or formatted."
  },
  {
    "objectID": "Chapter15.html#question-1-1",
    "href": "Chapter15.html#question-1-1",
    "title": "Chapter 15",
    "section": "Question 1",
    "text": "Question 1\nCompare and contrast the results of paste0() with str_c() for the following inputs:\nstr_c(\"hi \", NA) \nstr_c(letters[1:2], letters[1:3])\nAs we can see below, paste0 converts NA into a string \"NA\" and simply joins it with another string. However, str_c() behaves more sensibly - it generates NA if any of the strings being joined is NA.\n\nstr_c(\"hi \", NA)\n\n[1] NA\n\npaste0(\"hi \", NA)\n\n[1] \"hi NA\"\n\n\nFurther, we see below that we are joining two string vectors of unequal length, i.e., letters[1:2] is \"a\" \"b\" and letters[1:3] is \"a\" \"b\" \"c\" , both str_c() and paste0() behave differently.\n\nstr_c() throws an error and informs us that the string vectors being joined are of unequal length.\npaste0 simple recycles the shorter string vector silently.\n\n\n# str_c(letters[1:2], letters[1:3])\npaste0(letters[1:2], letters[1:3])\n\n[1] \"aa\" \"bb\" \"ac\""
  },
  {
    "objectID": "Chapter15.html#question-2-1",
    "href": "Chapter15.html#question-2-1",
    "title": "Chapter 15",
    "section": "Question 2",
    "text": "Question 2\nWhat’s the difference between paste() and paste0()? How can you recreate the equivalent of paste() with str_c()?\nIn R, both paste() and paste0() functions are used to concatenate strings together. However, they differ in how they handle separating the concatenated elements.\npaste() concatenates its arguments with a space character as the default separator. We can specify a different separator using the sep argument.\npaste0() is similar to paste(), but it does not add any separator between the concatenated elements. It simply combines them as-is.\nHere is an example: –\n\nvec1 &lt;- c(\"Hello\", \"Hi\")\nvec2 &lt;- c(\"Amy\", \"Tom\", \"Neal\")\npaste(vec1, vec2)\n\n[1] \"Hello Amy\"  \"Hi Tom\"     \"Hello Neal\"\n\npaste(vec1, vec2, sep = \", \")\n\n[1] \"Hello, Amy\"  \"Hi, Tom\"     \"Hello, Neal\"\n\npaste0(vec1, vec2)\n\n[1] \"HelloAmy\"  \"HiTom\"     \"HelloNeal\"\n\n\nWe can recreate the equivalent of paste() using the str_c() function from the stringr package in R. To do this, we can specify the separator using the sep argument in str_c() as follows: –\n\nvec1 &lt;- c(vec1, \"Hallo\")\npaste(vec1, vec2)\n\n[1] \"Hello Amy\"  \"Hi Tom\"     \"Hallo Neal\"\n\nstr_c(vec1, vec2, sep = \" \")\n\n[1] \"Hello Amy\"  \"Hi Tom\"     \"Hallo Neal\"\n\n\nNote: We had to add a string to vec1 so that both vec1 and vec2 are of length 3. Else, str_c will throw up an error."
  },
  {
    "objectID": "Chapter15.html#question-3",
    "href": "Chapter15.html#question-3",
    "title": "Chapter 15",
    "section": "Question 3",
    "text": "Question 3\nConvert the following expressions from str_c() to str_glue() or vice versa:\n\nstr_c(\"The price of \", food, \" is \", price)\nstr_glue(\"The price of {food} is {price}\")\nstr_glue(\"I'm {age} years old and live in {country}\")\nstr_c(\"I'm \", age, \" years old and live in \", country)\nstr_c(\"\\\\section{\", title, \"}\")\nstr_glue(\"\\\\\\\\section{{{title}}}\")\n\nasd\nfind the distribution of lengths of US babynames and then with filter() to look at the longest names, which happen to have 15 letters\n\ndata(\"babynames\")\nbabynames |&gt;\n  mutate(name_lgth = str_length(name)) |&gt;\n  count(name_lgth, wt = n)\n\n# A tibble: 14 × 2\n   name_lgth        n\n       &lt;int&gt;    &lt;int&gt;\n 1         2   338150\n 2         3  8589596\n 3         4 48506739\n 4         5 87011607\n 5         6 90749404\n 6         7 72120767\n 7         8 25404066\n 8         9 11926551\n 9        10  1306159\n10        11  2135827\n11        12    16295\n12        13    10845\n13        14     3681\n14        15      830\n\nbabynames |&gt;\n  filter(str_length(name) == 15) |&gt;\n  count(name, wt = n, sort = TRUE) |&gt;\n  slice_head(n = 5) |&gt;\n  select(name) |&gt;\n  as_vector() |&gt;\n  unname() |&gt;\n  str_sub(start = -3, end = -1)\n\n[1] \"ier\" \"ohn\" \"her\" \"ame\" \"ich\""
  },
  {
    "objectID": "Chapter15.html#question-1-2",
    "href": "Chapter15.html#question-1-2",
    "title": "Chapter 15",
    "section": "Question 1",
    "text": "Question 1\nWhen computing the distribution of the length of babynames, why did we use wt = n?\nThe babynames data-set (Table 1) displays the column n to reflect the frequency, i.e., number of observations of that name in that year. Thus, when we are computing the distribution of the length of baby names (Table 2), we need to weigh the observations by n otherwise each row will be treated as 1 (Table 2 column 3), instead of the actual number reflected in n leading to erroneous results.\n\n\nCode\nbabynames |&gt;\n  slice_head(n = 5) |&gt;\n  gt() |&gt;\n  fmt_number(prop, decimals = 4)\n\n\n\n\n\n\nTable 1:  The babynames data-set \n  \n    \n    \n      year\n      sex\n      name\n      n\n      prop\n    \n  \n  \n    1880\nF\nMary\n7065\n0.0724\n    1880\nF\nAnna\n2604\n0.0267\n    1880\nF\nEmma\n2003\n0.0205\n    1880\nF\nElizabeth\n1939\n0.0199\n    1880\nF\nMinnie\n1746\n0.0179\n  \n  \n  \n\n\n\n\n\n\n\nCode\ndf1 = babynames |&gt;\n  mutate(name_length = str_length(name)) |&gt;\n  count(name_length, wt = n) |&gt;\n  rename(correct_frequency = n)\n\ndf2 = babynames |&gt;\n  mutate(name_length = str_length(name)) |&gt;\n  count(name_length) |&gt;\n  rename(wrong_frequency_without_weights = n)\n\ninner_join(df1, df2, by = \"name_length\") |&gt;\n  gt() |&gt;\n  fmt_number(-name_length , decimals = 0) |&gt;\n  cols_label_with(\n    fn = ~ janitor::make_clean_names(., case = \"title\")\n  ) |&gt;\n  gt_theme_538()\n\n\n\n\n\n\nTable 2:  The distribution of the length of babynames \n  \n    \n    \n      Name Length\n      Correct Frequency\n      Wrong Frequency without Weights\n    \n  \n  \n    2\n338,150\n4,660\n    3\n8,589,596\n41,274\n    4\n48,506,739\n177,838\n    5\n87,011,607\n404,291\n    6\n90,749,404\n546,519\n    7\n72,120,767\n424,360\n    8\n25,404,066\n213,803\n    9\n11,926,551\n78,946\n    10\n1,306,159\n23,437\n    11\n2,135,827\n6,461\n    12\n16,295\n1,610\n    13\n10,845\n946\n    14\n3,681\n390\n    15\n830\n130"
  },
  {
    "objectID": "Chapter15.html#question-2-2",
    "href": "Chapter15.html#question-2-2",
    "title": "Chapter 15",
    "section": "Question 2",
    "text": "Question 2\nUse str_length() and str_sub() to extract the middle letter from each baby name. What will you do if the string has an even number of characters?\nThe code displayed below extracts the middle letter from each baby name, and the results for first 10 names are displayed in Table 3 . If the string has an even number of characters, we can pick the middle two characters.\n\ndf3 = babynames |&gt;\n  mutate(\n    name_length = str_length(name),\n    middle_letter_start = if_else(name_length %% 2 == 0,\n                             name_length/2,\n                             (name_length/2) + 0.5),\n    middle_letter_end = if_else(name_length %% 2 == 0,\n                                (name_length/2) + 1,\n                                (name_length/2) + 0.5),\n    middle_letter = str_sub(name, \n                            start = middle_letter_start, \n                            end = middle_letter_end)\n  ) |&gt;\n  select(-c(year, sex, n, prop)) |&gt;\n  slice_head(n = 10)\n\ndf3 |&gt;\n  gt() |&gt;\n  cols_label_with(fn = ~ janitor::make_clean_names(., case = \"title\")) |&gt;\n  cols_align(align = \"center\",\n             columns = -name) |&gt;\n  gt_theme_538()\n\n\n\n\n\nTable 3:  Middle letters of names \n  \n    \n    \n      Name\n      Name Length\n      Middle Letter Start\n      Middle Letter End\n      Middle Letter\n    \n  \n  \n    Mary\n4\n2\n3\nar\n    Anna\n4\n2\n3\nnn\n    Emma\n4\n2\n3\nmm\n    Elizabeth\n9\n5\n5\na\n    Minnie\n6\n3\n4\nnn\n    Margaret\n8\n4\n5\nga\n    Ida\n3\n2\n2\nd\n    Alice\n5\n3\n3\ni\n    Bertha\n6\n3\n4\nrt\n    Sarah\n5\n3\n3\nr"
  },
  {
    "objectID": "Chapter15.html#question-3-1",
    "href": "Chapter15.html#question-3-1",
    "title": "Chapter 15",
    "section": "Question 3",
    "text": "Question 3\nAre there any major trends in the length of baby names over time? What about the popularity of first and last letters?\nThe Figure 1, Figure 2 and Figure 3 show the trends over time.\n\n\nCode\ndf4 = babynames |&gt;\n  mutate(\n    name_length = str_length(name),\n    name_start = str_sub(name, 1, 1),\n    name_end = str_sub(name, -1, -1)\n  )\ny_coord = c(5.4, 6.3)\ndf4 |&gt;\n  group_by(year) |&gt;\n  count(name_length, wt = n) |&gt;\n  summarise(mean_length = weighted.mean(name_length, w = n)) |&gt;\n  ggplot(aes(x = year, y = mean_length)) +\n  theme_classic() +\n  labs(y = \"Average name length (for each year)\",\n       x = \"Year\", \n       title = \"Baby names have become longer over the past 12 decades\",\n       subtitle = \"Between 1890-1920, and 1960-1990 baby names became longer\\nBut, since 1990 the names are becoming shorter again\") +\n  scale_x_continuous(breaks = seq(1880, 2000, 20)) +\n  geom_rect(mapping = aes(xmin = 1890, xmax = 1920,\n                          ymin = y_coord[1], ymax = y_coord[2]),\n            alpha = 0.01, fill = \"grey\") +\n  geom_rect(mapping = aes(xmin = 1960, xmax = 1990,\n                          ymin = y_coord[1], ymax = y_coord[2]),\n            alpha = 0.01, fill = \"grey\") +\n  geom_line(lwd = 1) +\n  coord_cartesian(ylim = y_coord) +\n  theme(plot.title.position = \"plot\")\n\n\n\n\n\nFigure 1: Length of babynames over time\n\n\n\n\n\n\nCode\nns_vec = df4 |&gt;\n  count(name_start, wt = n, sort = TRUE) |&gt;\n  slice_head(n = 5) |&gt;\n  select(name_start) |&gt;\n  as_vector() |&gt;\n  unname()\n\ndf4 |&gt;\n  filter(name_start %in% ns_vec) |&gt;\n  group_by(year) |&gt;\n  count(name_start, wt = n) |&gt;\n  mutate(prop = 100*n/sum(n)) |&gt;\n  mutate(lbl = if_else(year == 2017, \n                       name_start, \n                       NA)) |&gt;\n  ggplot(aes(x = year, y = prop, \n             col = name_start, label = lbl)) +\n  geom_line(lwd = 1) +\n  ggrepel::geom_label_repel(nudge_x = 1) +\n  labs(x = \"Year\",\n       y = \"Percentage of names starting with character\",\n       title = \"People's preferences for baby names' starting letter change over time\",\n       subtitle = \"Names starting with A are most popular now\\nNames starting with J were popular in the 1940s\\nIn 1950s, names starting with D became popular, while those starting with A lost popularity\") +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title.position = \"plot\") +\n  scale_x_continuous(breaks = seq(1880, 2020, 20))\n\n\n\n\n\nFigure 2: Trends on the starting letter of babynames over time\n\n\n\n\n\n\nCode\nns_vec = df4 |&gt;\n  count(name_end, wt = n, sort = TRUE) |&gt;\n  slice_head(n = 5) |&gt;\n  select(name_end) |&gt;\n  as_vector() |&gt;\n  unname()\n\ndf4 |&gt;\n  filter(name_end %in% ns_vec) |&gt;\n  group_by(year) |&gt;\n  count(name_end, wt = n) |&gt;\n  mutate(prop = 100*n/sum(n)) |&gt;\n  mutate(lbl = if_else(year == 2017, \n                       name_end, \n                       NA)) |&gt;\n  ggplot(aes(x = year, y = prop, \n             col = name_end, label = lbl)) +\n  geom_line(lwd = 1) +\n  ggrepel::geom_label_repel(nudge_x = 1) +\n  labs(x = \"Year\",\n       y = \"Percentage of names ending with character\",\n       title = \"People's preferences for baby names' ending letter change over time\",\n       subtitle = \"Names ending in N have risen in popularity over the decades.\\nNames ending with E have become less popular over time\") +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title.position = \"plot\") +\n  scale_x_continuous(breaks = seq(1880, 2020, 20))\n\n\n\n\n\nFigure 3: Trends on the ending letter of babynames over time"
  }
]