[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Solutions Manual: R for Data Science (2e)",
    "section": "",
    "text": "Welcome to the Solutions Manual for the second edition of R for Data Science by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund. This manual is your indispensable companion on the path to mastering data science with R.\nInside these pages, you’ll find a rich trove of techniques and best practices that underpin the very essence of data science. I emphasize not just the “how,” but also the “why” behind each step, ensuring a deep understanding of the principles that drive data science.\nThis website is committed to fostering a collaborative learning environment and offers the Solutions Manual for free. Happy learning!\n\n\n."
  },
  {
    "objectID": "index.html#solutions-for-the-book-r-for-data-science-2nd-edition",
    "href": "index.html#solutions-for-the-book-r-for-data-science-2nd-edition",
    "title": "Solutions: R for Data Science (2e)",
    "section": "",
    "text": "Author: Aditya Dahiya\nThis website displays the solutions for the exercises in the book R for Data Science, 2nd Edition.\n."
  },
  {
    "objectID": "Chapter29.html",
    "href": "Chapter29.html",
    "title": "Chapter 29",
    "section": "",
    "text": "Create a new Quarto document using File &gt; New File &gt; Quarto Document. Read the instructions. Practice running the chunks individually. Then render the document by clicking the appropriate button and then by using the appropriate keyboard short cut. Verify that you can modify the code, re-run it, and see modified output.\n\n\nThe document has been created. Some of the chunks are as follows:\n\n\nlibrary(tidyverse)\n\n\ndiamonds |&gt; \n  filter(carat &gt; 2) |&gt;\n  ggplot(mapping = aes(x=carat, y=price,\n                       color = color)) +\n  geom_point(alpha = 0.5) + \n  #scale_y_continuous(trans = \"log\") +\n  geom_smooth(se=FALSE) + \n  scale_color_brewer(palette = 2)\n\n\n\n\n\nVerifying that the code can be modified and re-run:–\n\n\ndiamonds |&gt; \n  filter(carat &gt; 2) |&gt;\n  filter (carat &lt; 3) |&gt;\n  filter (price &gt; 10000) |&gt;\n  ggplot(mapping = aes(x=carat, y=price,\n                       color = color)) +\n  geom_point(alpha = 0.3) + \n  scale_y_continuous(trans = \"log\") +\n  geom_smooth(se=FALSE) + \n  scale_color_brewer(palette = 4) +\n  theme_minimal() +\n  labs(title = \"Plot of relation between Carat and Price\",\n       subtitle = \"For different colours of diamonds\",\n       y = \"Price (in $)\", x = \"Carat\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nCreate one new Quarto document for each of the three built-in formats: HTML, PDF and Word. Render each of the three documents. How do the outputs differ? How do the inputs differ? (You may need to install LaTeX in order to build the PDF output — RStudio will prompt you if this is necessary.)\n\nThe outputs are different in the following ways:--\n\nThe output text and code are same, but rendering to PDF takes more time than Word. HTML rendering seems to be the fastest.\nPDF and MS Word cannot incorporate interactive graphics, while HTML offers interactivity.\nThe size of the PDF document is the largest, followed by MS Word document, while the HTML uses the least disk space."
  },
  {
    "objectID": "Chapter29.html#nd-level-header",
    "href": "Chapter29.html#nd-level-header",
    "title": "Chapter 29",
    "section": "2nd Level Header",
    "text": "2nd Level Header\n\n3rd Level Header"
  },
  {
    "objectID": "Chapter29.html#lists",
    "href": "Chapter29.html#lists",
    "title": "Chapter 29",
    "section": "Lists",
    "text": "Lists\n\nBulleted list item 1\nItem 2\n\nItem 2a\nItem 2b\n\n\n\nNumbered list item 1\nItem 2. The numbers are incremented automatically in the output.\n\n\nLinks and Images\nhttps://example.com/\nlinked phrases\n\n\n\nCredits: Nick Tierney's (mostly) rstats blog\n\n\nTables\n\nMy First Table in Quarto\n\n\nFirst Header\nSecond Header\n\n\n\n\nContent Cell 1.1\nContent Cell 2.1\n\n\nContent Cell 1.2\nContent Cell 2.2\n\n\nContent Cell 1.3\nContent Cell 2.3\n\n\n\n\nUsing the visual editor, insert a code chunk using the Insert menu and then the insert anything tool.\nHere, I am inserting a code chunk using simple the “/” key, and then selecting R-code option:--\n\nprint(\"Hello World\")\n\n[1] \"Hello World\"\n\n\nUsing the visual editor, figure out how to:\n\nAdd a footnote.\nYou can add a foot note by selecting the menu Insert –&gt; Footnote; or, by using Ctrl+Shift+7. Here is an example.1\nAdd a horizontal rule.\nYou can add a foot note by selecting the menu Insert –&gt; Horizontal Rule. Here is an example:--\n\nAdd a block quote.\nYou can add a foot note by selecting the menu Format –&gt; Block quote. Here is how we add a block quote:-\n\nA block quote is a long quote formatted as a separate “block” of text. Instead of using quotation marks, you place the quote on a new line, and indent the entire quote to mark it apart from your own words2\n\n\nIn the visual editor, go to Insert &gt; Citation and insert a citation to the paper titled Welcome to the Tidyverse using its DOI (digital object identifier), which is 10.21105/joss.01686. Render the document and observe how the reference shows up in the document. What change do you observe in the YAML of your document?\n\nLet us first add some text from the paper, so that we can use a citation:---\n\nAt a high level, the tidyverse is a language for solving data science challenges with R code. Its primary goal is to facilitate a conversation between a human and a computer about data. Less abstractly, the tidyverse is a collection of R packages that share a high-level design philosophy and low-level grammar and data structures, so that learning one package makes it easier to learn the next. (Wickham et al. 2019)\n\nOnce we render the document, the citation shows up in the very end of the HTML webpage rendered. It is displayed just above the footnotes. The YAML header of the document, when viewed in the “Source” displays an additional line bibliography: references.bib."
  },
  {
    "objectID": "Chapter29.html#footnotes",
    "href": "Chapter29.html#footnotes",
    "title": "Chapter 29",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is a sample footnote to answer the Question 3(a) of the 29.3.1 Exercises within the Chapter 29 “Quarto” of the Book “R for Data Science, 2nd Edition” by Wickham, Cetinkaya-Rundel & Grolemund.↩︎\nSource: Scribbr.com. What is a block quote?↩︎\nI use Ctrl + Shift + 7 to create a footnote here.↩︎\nThis is a test footnote I wrote in the Source Editor↩︎"
  },
  {
    "objectID": "solutions.html",
    "href": "solutions.html",
    "title": "Solutions",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nChapter 10\n\n\nLayers\n\n\nAug 22, 2023\n\n\n\n\nChapter 11\n\n\nExploratory data analysis\n\n\nAug 28, 2023\n\n\n\n\nChapter 12\n\n\nCommunication\n\n\nAug 28, 2023\n\n\n\n\nChapter 13\n\n\nLogical Vectors\n\n\nSep 3, 2023\n\n\n\n\nChapter 14\n\n\nNumbers\n\n\nSep 8, 2023\n\n\n\n\nChapter 15\n\n\nStrings\n\n\nSep 10, 2023\n\n\n\n\nChapter 16\n\n\nRegular Expressions\n\n\nSep 12, 2023\n\n\n\n\nChapter 17\n\n\nFactors\n\n\nSep 14, 2023\n\n\n\n\nChapter 18\n\n\nDates and times\n\n\nSep 15, 2023\n\n\n\n\nChapter 19\n\n\nMissing Values\n\n\nOct 2, 2023\n\n\n\n\nChapter 2\n\n\nData Visualization\n\n\nJul 29, 2023\n\n\n\n\nChapter 20\n\n\nJoins\n\n\nOct 2, 2023\n\n\n\n\nChapter 21\n\n\nSpreadsheets\n\n\nAug 15, 2023\n\n\n\n\nChapter 22\n\n\nDatabases\n\n\nOct 5, 2023\n\n\n\n\nChapter 23\n\n\nArrow\n\n\nOct 5, 2023\n\n\n\n\nChapter 24\n\n\nHierarchical data\n\n\nOct 5, 2023\n\n\n\n\nChapter 25\n\n\nWeb scraping\n\n\nOct 6, 2023\n\n\n\n\nChapter 26\n\n\nFunctions\n\n\nOct 9, 2023\n\n\n\n\nChapter 27\n\n\nIteration\n\n\nOct 10, 2023\n\n\n\n\nChapter 28\n\n\nA field guide to base R\n\n\nOct 11, 2023\n\n\n\n\nChapter 29\n\n\nQuarto\n\n\nJul 25, 2023\n\n\n\n\nChapter 3\n\n\nWorkflow: Basics\n\n\nJul 29, 2023\n\n\n\n\nChapter 30\n\n\nQuarto Formats\n\n\nJul 25, 2023\n\n\n\n\nChapter 4\n\n\nData Transformation\n\n\nJul 29, 2023\n\n\n\n\nChapter 5\n\n\nWorkflow: code style\n\n\nAug 2, 2023\n\n\n\n\nChapter 6\n\n\nData tidying\n\n\nAug 5, 2023\n\n\n\n\nChapter 7\n\n\nWorkflow: scripts and projects\n\n\nAug 8, 2023\n\n\n\n\nChapter 8\n\n\nData Import\n\n\nAug 10, 2023\n\n\n\n\nChapter 9\n\n\nWorkflow: getting help\n\n\nAug 13, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to the quirky realm of Aditya Dahiya, your friendly neighborhood Indian Administrative Service (IAS) officer, currently working as the Director and Special Secretary in the Government of Haryana, India. Amidst the daily avalanche of files and meetings that could rival a paper mountain, Aditya somehow manages to sneak in quality time with his true loves: data visualization and health financing data. Think of him as your data-wrangling, file-taming, bureaucracy-battling guide with a slightly skewed sense of humor. Need more bureaucratic banter or data insights? Connect with Aditya on LinkedIn or shoot him an email – he promises it won’t be as formal as a government memo!"
  },
  {
    "objectID": "Ch29-6-3-1-3.html",
    "href": "Ch29-6-3-1-3.html",
    "title": "Chapter 29: Exercise 29.6.3.3",
    "section": "",
    "text": "3. Change the size of the figure with the following chunk options, one at a time, render your document, and describe how the figure changes.\n\nHow the figure changes with `fig-width: 6`\n\n\nlibrary(tidyverse)\ndiamonds |&gt;\n  filter(carat &lt;= 2.5) |&gt;\n      ggplot(aes(x = carat)) + \n        geom_freqpoly(binwidth = 0.01) +\n        theme_light() +\n        labs(x=\"Diamond Carat Size\", y = \"Number of Diamonds\")\n\n\n\n\nFigure 1: Plot with width fixed at 10\n\n\n\n\n\nHow the figure changes with `fig-height: 3`\n\n\ndiamonds |&gt;\n  filter(carat &lt;= 2.5) |&gt;\n      ggplot(aes(x = carat)) + \n        geom_freqpoly(binwidth = 0.01) +\n        theme_light() +\n        labs(x=\"Diamond Carat Size\", y = \"Number of Diamonds\")\n\n\n\n\nFigure 2: Plot with height fixed at 10\n\n\n\n\n\nHow the figure changes with `out-width: \"100%\"`\n\n\ndiamonds |&gt;\n  filter(carat &lt;= 2.5) |&gt;\n      ggplot(aes(x = carat)) + \n        geom_freqpoly(binwidth = 0.01) +\n        theme_light() +\n        labs(x=\"Diamond Carat Size\", y = \"Number of Diamonds\")\n\n\n\n\nFigure 3: Plot with output width at 100%\n\n\n\n\n\nHow the figure changes with `out-width: \"20%\"`\n\n\ndiamonds |&gt;\n  filter(carat &lt;= 2.5) |&gt;\n      ggplot(aes(x = carat)) + \n        geom_freqpoly(binwidth = 0.01) +\n        theme_light() +\n        labs(x=\"Diamond Carat Size\", y = \"Number of Diamonds\")\n\n\n\n\nFigure 4: Plot with output width at 20%"
  },
  {
    "objectID": "Chapter2.html",
    "href": "Chapter2.html",
    "title": "Chapter 2",
    "section": "",
    "text": "library(tidyverse)\nlibrary(palmerpenguins)\npenguins = penguins\n\n\n2.2.5 Exercises\n\nHow many rows are in penguins? How many columns?\n\nThe number of rows in penguins data-set is 344 and the number of columns is 8\n\nWhat does the bill_depth_mm variable in the penguins data frame describe? Read the help for ?penguins to find out.\n\nFirst, we find out the names of the variables in the penguins data frame in Table 1.\n\nnames(penguins) |&gt;\n  t() |&gt;\n  as_tibble() |&gt;\n  gt::gt()\n\n\n\n\n\nTable 1:  List of variables in the penguins dataset \n  \n    \n    \n      V1\n      V2\n      V3\n      V4\n      V5\n      V6\n      V7\n      V8\n    \n  \n  \n    species\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n  \n  \n  \n\n\n\n\n# Finding the details of the variables.\n# ?penguins\n\nThe variable name bill_depth_mm depicts “a number denoting bill depth (millimeters)”.[Gorman, Williams, and Fraser (2014)](Horst, Hill, and Gorman 2020)\n\nMake a scatter-plot of bill_depth_mm vs. bill_length_mm. That is, make a scatter-plot with bill_depth_mm on the y-axis and bill_length_mm on the x-axis. Describe the relationship between these two variables.\n\nThe scatter-plot is depicted below.\n\npenguins |&gt;\n  ggplot(mapping = aes(x = bill_length_mm,\n                       y = bill_depth_mm,\n                       col = species)) +\n  geom_point() +\n  geom_smooth(se = FALSE,\n              method = \"lm\") +\n  theme_classic() +\n  labs(x = \"Bill Length (mm)\", y = \"Bill Depth (mm)\")\n\n\n\n\nFigure 1: Scatterplot of relation between Bill Length and Bill Depth\n\n\n\n\nWe now test the correlations, and create a beautiful table using gt (Iannone et al. 2023)and gtExtras packages.(Mock 2022)\n\n# Checking the correlation between the two variables\ntest1 = function(x) {cor.test(x$bill_length_mm, x$bill_depth_mm)$estimate}\n\n# An empty data-frame to collect results\ndf = tibble(Penguins = NA,\n            Correlation = NA,\n            .rows = 4)\n# Finding Correlation by each penguin variety\nfor (y in 1:3) {\n  c = penguins |&gt;\n      filter(species == unique(penguins$species)[y]) |&gt;\n      test1() |&gt;\n      format(digits = 2)\n  df[y,2] = c\n  df[y,1] = unique(penguins$species)[y]\n  }\n# Converting the nature of 1st column from factor to character\ndf$Penguins = as.character(df$Penguins)  \n# Storing the overall correlation\ndf[4,1] = \"Overall\"\ndf[4,2] = penguins |&gt; test1() |&gt; format(digits = 2)\n\n# Displaying the result\ngt::gt(df) |&gt;\n  gt::tab_header(title = \"Correlation Coefficitents\",\n                 subtitle = \"Between Bill Length & Bill Depth amongst   \n                 different penguins\") |&gt;\n  gtExtras::gt_theme_538() |&gt;\n  gtExtras::gt_highlight_rows(rows = 4, fill = \"#d4cecd\")\n\n\n\n\n\nTable 2:  Correlation Table amongst different types of penguins \n  \n    \n      Correlation Coefficitents\n    \n    \n      Between Bill Length & Bill Depth amongst   \n                 different penguins\n    \n    \n      Penguins\n      Correlation\n    \n  \n  \n    Adelie\n0.39\n    Gentoo\n0.64\n    Chinstrap\n0.65\n    Overall\n-0.24\n  \n  \n  \n\n\n\n\n\nThus, we see that the relation is not apparent on a simple scatter plot, but if we plot a different colour for each species, we observe that there is positive correlation between Bill Length and Bill Depth, in all three species. The strongest correlation is amongst Gentoo and Chinstrap penguins.\n\nWhat happens if you make a scatter-plot of species vs. bill_depth_mm? What might be a better choice of geom?\nIf we make a scatter-plot of species vs. bill_depth_mm, the following happens:-\n\npenguins |&gt;\n  ggplot(mapping = aes(x = species,\n                       y = bill_depth_mm)) +\n  geom_point() +\n  theme_bw() +\n  labs(x = \"Species\", y = \"Bill Depth (mm)\")\n\n\n\n\nFigure 2: Scatter plot of species vs. Bill Depth\n\n\n\n\nThis produces an awkward scatter-plot, since the x-axis variable is discrete, and not continuous. A better choice of geom might be a box-plot, which is a good way to present the relationship between a continuous (Bill Depth) and a categorical (species) variable. which shows that the average Bill Depth (in mm) is lower in Gentoo penguins compared to the other two.\n\npenguins |&gt;\n  ggplot(mapping = aes(x = species,\n                       y = bill_depth_mm)) +\n  geom_boxplot() +\n  theme_bw() +\n  labs(x = \"Species\", y = \"Bill Depth (mm)\")\n\n\n\n\nFigure 3: Box-plot of species vs. Bill Depth\n\n\n\n\nWhy does the following give an error and how would you fix it?\nggplot(data = penguins) +    geom_point()\nThe above code will give an error, because we have only given the data to the ggplot call, but not specified the mapping aesthetics, i.e., the x-axis and y-axis for the scatter plot called by the geom_point() . We can fix the error as follows in Figure 4 :---\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nFigure 4: Corrected code to display the plot\n\n\n\n\nWhat does the na.rm argument do in geom_point()? What is the default value of the argument? Create a scatterplot where you successfully use this argument set to TRUE.\nWithin the function geom_point() the na.rm argument can do one of the two things. If it is set to FALSE , as it is by default, then the missing values are removed but the following warning message is displayed:–\nWarning message: \nRemoved 2 rows containing missing values (`geom_point()`)\nBut, if it is set to na.rm = TRUE, then the missing values are silently removed. Here’s the code with na.rm = TRUE to produce Figure 5 :---\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm)) +\n  geom_point(na.rm = TRUE)\n\n\n\n\nFigure 5: Corrected code to display the plot with na.rm = TRUE\n\n\n\n\nAdd the following caption to the plot you made in the previous exercise: “Data come from the palmerpenguins package.” Hint: Take a look at the documentation for labs().\nThe caption is added here with the labs function with ggplot function below in (fi?)\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm)) +\n  geom_point(na.rm = TRUE) +\n  labs(caption = \"Data come from the palmerpenguins package.\")\n\n\n\n\nFigure 6: Plot with a caption added in ggplot call itself\n\n\n\n\nRecreate the following visualization. What aesthetic should bill_depth_mm be mapped to? And should it be mapped at the global level or at the geom level?\n\npenguins |&gt;\n  ggplot(mapping = aes(x = flipper_length_mm,\n                       y = body_mass_g)) + \n  geom_point(mapping = aes(color = bill_depth_mm)) + \n  geom_smooth()\n\n\n\n\nFigure 7: Recreated figure using the ggplot2 code\n\n\n\n\nThe code above recreates the Figure 7. The aesthetic should bill_depth_mm should be mapped the aesthetic colo in the geom_point() function level. It should not be done at the global level, because then it will even be an aesthetic for geom_smooth resulting in multiple smoother lines fitted for each level of bill_depth_mm , and possible result in an error because bill_depth_mm is not a categorical variable or a factor variable with certain distinct categories or levels.\nLuckily, ggplot2 recognizes this error and still produces the same plot by droppin the color aesthetic, i.e., The following aesthetics were dropped during statistical transformation: colour. So, ggplot2 is trying to guess our intentions, and it works, but the code not correct. The wrong code is tested at Figure 8.\n\npenguins |&gt;\n  ggplot(mapping = aes(x = flipper_length_mm,\n                       y = body_mass_g,\n                       color = bill_depth_mm)) + \n  geom_point() + \n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: The following aesthetics were dropped during statistical transformation: colour\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nFigure 8: The Wrong Code - Recreated figure is the same - but the code is fundamentally flawed\n\n\n\n\nRun this code in your head and predict what the output will look like. Then, run the code in R and check your predictions.\n\nggplot(data = penguins,\n       mapping = aes(x = flipper_length_mm, \n                     y = body_mass_g, \n                     color = island)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\nOn visual inspection, I believe this code should create a scatter plot of penguins flipper lengths (on x-axis) vs. body mass (on y-axis), with the dots coloured by islands on the penguins. Further, a smoother line if fitted to show the relationship, with a separate smoother line for each island type. Thus, since we know there are three types of islands, we expect three smoother lines fitted to the plot, without the display of standard error intervals.\nNow, let us check our predictions with the code in the Figure 9 :--\n\n\n\n\n\nFigure 9: Plot generated from running the Code of Question 9\n\n\n\n\nWill these two graphs look different? Why/why not?\n\n# Code 1\nggplot(data = penguins,\n       mapping = aes(x = flipper_length_mm, \n                     y = body_mass_g)) +\n  geom_point() +\n  geom_smooth()\n\n# Code 2\nggplot() +\n  geom_point(data = penguins,\n             mapping = aes(x = flipper_length_mm, \n                           y = body_mass_g)\n  ) +\n  geom_smooth(data = penguins,\n              mapping = aes(x = flipper_length_mm, \n                            y = body_mass_g))\n\nYes, these two graphs should look the same. Since, the data and the aesthetics mapped are the same in both. Only difference is that the second code has redundancy.\nHere’s the visual confirmation for both codes in Figure 10.\n\n\n\n\n\nFigure 10: Comparison of the two plots produced by the codes in Question 10\n\n\n\n\n\n\n\n2.4.3 Exercises\n\nMake a bar plot of species of penguins, where you assign species to the y aesthetic. How is this plot different?\nWhen we assign species to the y-axis, we get a horizontal bar plot, instead of the vertical bar plot given in the textbook. The results are compared in Figure 11 .\n\np1 = penguins |&gt;\n      ggplot(aes(x = species)) +\n      geom_bar() +\n      labs(caption = \"Species on x-axis\")\np2 = penguins |&gt;\n      ggplot(aes(y = species)) +\n      geom_bar() +\n      labs(caption = \"Species on y-axis\")\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n\n\n\nFigure 11: Change in figure when species is assigned to y-axis\n\n\n\n\nHow are the following two plots different? Which aesthetic, color or fill, is more useful for changing the color of bars?\nThe output of the two plots is in Figure 12 .\n\ngridExtra::grid.arrange(\n\nggplot(penguins, aes(x = species)) + geom_bar(color = \"red\") +\n  labs(caption = \"Color = Red\"),\n\nggplot(penguins, aes(x = species)) + geom_bar(fill = \"red\") +\n  labs(caption = \"Fill = Red\"),\n\nncol = 2)\n\n\n\n\nFigure 12: The two plots produced by the code given, with red color vs. red fill\n\n\n\n\nThe two plots are different in where the colour red appears. As a color aesthetic, it appears only on the borders. But, as a fill aesthetic, it fills the entire bar(s).\nThus, the aesthetic fill is more useful in changing the colour of the bars.\nWhat does the bins argument in geom_histogram() do?\nThe bins argument tell the number of bins (i.e. number of bars) in the histogram to be plotted. The default value is 30. However, if the binwidth is also specified, then the binwidth argument over-rides the bins argument.\nMake a histogram of the carat variable in the diamonds dataset that is available when you load the tidyverse package. Experiment with different binwidths. What bin-width reveals the most interesting patterns?\n\ng1 = ggplot(diamonds, aes(x=carat)) + \n  geom_histogram(fill = \"white\", color = \"black\") + \n  theme_classic() + labs(x = NULL, y = NULL, \n                         subtitle = \"Default Bindwidth\")\ng2 = ggplot(diamonds, aes(x=carat)) + \n    geom_histogram(fill = \"white\", color = \"black\", \n                   binwidth = 0.1) + \n    theme_classic() + \n    labs(x = NULL, y = NULL, \n         subtitle = \"Bindwidth = 0.1\")\ng3 = ggplot(diamonds, aes(x=carat)) + \n    geom_histogram(fill = \"white\", color = \"black\", \n                   binwidth = 0.2) + \n    theme_classic() + \n    labs(x = NULL, y = NULL, \n         subtitle = \"Bindwidth = 0.2\")  \ng4 = ggplot(diamonds, aes(x=carat)) + \n    geom_histogram(fill = \"white\", color = \"black\", \n                   binwidth = 0.3) + \n    theme_classic() + \n    labs(x = NULL, y = NULL, \n         subtitle = \"Bindwidth = 0.3\")\ng5 = ggplot(diamonds, aes(x=carat)) + \n    geom_histogram(fill = \"white\", color = \"black\", \n                   binwidth = 0.5) + \n    theme_classic() + \n    labs(x = NULL, y = NULL, \n         subtitle = \"Bindwidth = 0.5\")\ng6 = ggplot(diamonds, aes(x=carat)) + \n    geom_histogram(fill = \"white\", color = \"black\", \n                   binwidth = 1) + \n    theme_classic() + \n    labs(x = NULL, y = NULL, \n         subtitle = \"Bindwidth = 1\")\n\ngridExtra::grid.arrange(g1, g2, g3, g4, g5, g6,\n                        ncol = 3, nrow = 2)\n\n\n\n\nFigure 13: Histogram with different bin-widths tried out to select the most relevant one\n\n\n\n\nThus, we see that best binwidth is either the default binwidth chosen by ggplot2 or the bind-width of 0.2 per bin, since it reveals the most interesting patterns.\n\n\n\n2.5.5 Exercises\n\nThe mpg data frame that is bundled with the ggplot2 package contains 234 observations collected by the US Environmental Protection Agency on 38 car models. Which variables in mpg are categorical? Which variables are numerical? (Hint: Type ?mpg to read the documentation for the dataset.) How can you see this information when you run mpg?\nThe code below displays the summary fo the mpg data-set. The following variables are categorical: manufacturer (manufacturer name), model (model name), trans (type of transmission), drv (the type of drive train: front, rear or 4-wheel), fl (fuel type), and class (type of car). The numerical variables are displ (engine displacement, in litres), year (year of manufacture) , cyl (number of cylinders) , cty (city miles per gallon) and hwy (highway miles per gallon). We can see these in the square parenthesis the column titled Variable in the output of the code below .\n\n# Visualize summary of the data frame\nmpg |&gt;\n  summarytools::dfSummary(plain.ascii  = FALSE, \n                          style = \"grid\", \n                          graph.magnif = 0.75, \n                          valid.col = FALSE,\n                          na.col = FALSE,\n                          headings = FALSE) |&gt;\n  view()\n\nIf we simply run mpg , we can still see this information in the R console output, by the terms &lt;chr&gt; (for categorical variables) ; and, &lt;dbl&gt; or &lt;int&gt; (for numerical variables)\n\nmpg\n## # A tibble: 234 × 11\n##    manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n##    &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n##  1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n##  2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n##  3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n##  4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n##  5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n##  6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n##  7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n##  8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n##  9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n## 10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n## # ℹ 224 more rows\n\nMake a scatterplot of hwy vs. displ using the mpg data frame. Next, map a third, numerical variable to color, then size, then both color and size, then shape. How do these aesthetics behave differently for categorical vs. numerical variables?\n\ng1 = mpg |&gt;\n      ggplot(aes(x = hwy, y = displ)) +\n      geom_point() +\n      theme_minimal() +\n      labs(caption = \"Original Plot\")\n# Using numerical variable 'cty' to map to colour, size\ng2 = mpg |&gt;\n      ggplot(aes(x = hwy, y = displ, color = cty)) +\n      geom_point() +\n      theme_minimal()+\n      labs(caption = \"cty mapped to color\")\ng3 = mpg |&gt;\n      ggplot(aes(x = hwy, y = displ, size = cty)) +\n      geom_point(alpha = 0.5) +\n      theme_minimal()+\n      labs(caption = \"cty mapped to size\")\ng4 = mpg |&gt;\n      ggplot(aes(x = hwy, y = displ, \n                 color = cty, size = cty)) +\n      geom_point() +\n      theme_minimal()+\n      labs(caption = \"cty mapped to size and color\")\ngridExtra::grid.arrange(g1, g2, g3, g4, ncol = 2)\n\n\n\n\nFigure 14: Scatterplots of different kinds for different aesthetic mappings\n\n\n\n\nSo, we see that we can map a numerical variable to color or size aesthetics, and ggplot2 will itself make a scale and display the output with a legend. However, numerical variables (i.e., continuous variables) don’t map to shape aesthetic, as there cannot be any continuum amongst shapes. Accordingly, when mapped to shape, the code throws an error as below:---\n\n# Using numerical variable 'cty' to map to size aesthetic \n  mpg |&gt;\n      ggplot(aes(x = hwy, \n                 y = displ, \n                 shape = cty)) +\n      geom_point()\n\nError in `geom_point()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error in `scale_f()`:\n! A continuous variable cannot be mapped to the shape aesthetic\nℹ choose a different aesthetic or use `scale_shape_binned()`\n\n\nThus, the shape aesthetic works only with categorical variables, whereas color works with both numerical and categorical variables; and, by definition size aesthetic should be used only with numerical variables (it can work with categorical variables, but then the sizes are assigned arbitrarily to different categories).\nIn the scatter-plot of hwy vs. displ, what happens if you map a third variable to linewidth?\n\nmpg |&gt;\n      ggplot(aes(x = hwy, y = displ,\n                 linewidth = cty)) +\n      geom_point() +\n      theme_minimal()\n\n\n\n\nFigure 15: Experiment with mapping line width to a third variable\n\n\n\n\nAs we see, nothing changes with addition of the linewidth argument in Figure 15 . This is because the linewidth argument “scales the width of lines and polygon strokes.” in ggplot2 documentation. Since we are only plotting point geoms, and no lines, the argument is useless and not used to produce the output.\nWhat happens if you map the same variable to multiple aesthetics?\nWe can map the same variable to multiple aesthetics, and the output will display its variations in all such aesthetics. But it is redundant, and make plot cluttery with too much visual input.\nFor example, Figure 16 shows a poorly understandable plot where class of the vehicle is mapped to size, shape and color. It works, but there’s too much information redundancy.\n\nmpg |&gt;\n  ggplot(aes(x=hwy, y = cty,\n             size = class,\n             color = class,\n             shape = class)) +\n  geom_point(alpha = 0.5) +\n  theme_classic()\n\n\n\n\nFigure 16: A messy plot with Mutliple aesthetics defined by the same variable\n\n\n\n\nMake a scatterplot of bill_depth_mm vs. bill_length_mm and color the points by species. What does adding coloring by species reveal about the relationship between these two variables? What about faceting by species?\nThe Figure 17 shows the importance of coloring or faceting by species. This allows us to detect a fairly strong positive correlation which was not apparent in the simple scatter plot. This, perhaps, can be called an example of negative confounding (Mehio-Sibai et al. 2005) of the relation between bill depth and bill length by the species type.\n\np1 = penguins |&gt;\n  ggplot(mapping = aes(x = bill_length_mm,\n                       y = bill_depth_mm)) +\n  geom_point() +\n  geom_smooth(se = FALSE,\n              method = \"lm\") +\n  theme_classic() +\n  labs(x = \"Bill Length (mm)\", y = \"Bill Depth (mm)\",\n       subtitle = \"No relation is apparent\")\n\np2 = penguins |&gt;\n  ggplot(mapping = aes(x = bill_length_mm,\n                       y = bill_depth_mm,\n                       col = species)) +\n  geom_point() +\n  geom_smooth(se = FALSE,\n              method = \"lm\") +\n  theme_classic() +\n  labs(x = \"Bill Length (mm)\", y = \"Bill Depth (mm)\",\n       subtitle = \"Colouring  by species reveals relations\")\n\np3 = penguins |&gt;\n  ggplot(mapping = aes(x = bill_length_mm,\n                       y = bill_depth_mm)) +\n  geom_point() +\n  geom_smooth(se = FALSE,\n              method = \"lm\") +\n  facet_wrap(~species) +\n  theme_classic() +\n  labs(x = \"Bill Length (mm)\", y = \"Bill Depth (mm)\",\n       subtitle = \"Faceting also reveals the relations\")\n\nlay = rbind(c(1,1,2,2,2),\n            c(3,3,3,3,3))\ngridExtra::grid.arrange(p1, p2, p3, layout_matrix = lay)\n\n\n\n\nFigure 17: Adding color by species reveals a strong relationship\n\n\n\n\nWhy does the following yield two separate legends? How would you fix it to combine the two legends?\n\n\nggplot(data = penguins,   \n       mapping = aes(x = bill_length_mm, \n                     y = bill_depth_mm,      \n                     color = species, \n                     shape = species)) +   \n  geom_point() +   \n  labs(color = \"Species\")\n\nThis code presents a plot with two legends because in the last line, we have forced ggplot2 to name out “Color” legend as the string “Species”. Thus, ggplot2 differentiates between “species” and “Species”.\nWe can correct this issue in either of the following two ways:--\n\nggplot(data = penguins,   \n       mapping = aes(x = bill_length_mm, \n                     y = bill_depth_mm,      \n                     color = species, \n                     shape = species)) +   \n  geom_point()\n\nor,\n\nggplot(data = penguins,   \n       mapping = aes(x = bill_length_mm, \n                     y = bill_depth_mm,      \n                     color = species, \n                     shape = species)) +   \n  geom_point()\n\n\nCreate the two following stacked bar plots. Which question can you answer with the first one? Which question can you answer with the second one?\nThe plots are produced in Figure 18 .\n\ng1 = ggplot(penguins, aes(x = island, \n                     fill = species)) +   \n  geom_bar(position = \"fill\") +\n  labs(subtitle = \"Sub-figure A\")\n\ng2 = ggplot(penguins, aes(x = species, \n                     fill = island)) +   \n  geom_bar(position = \"fill\") +\n  labs(subtitle = \"Sub-figure B\")\n\ngridExtra::grid.arrange(g1, g2, ncol = 2)\n\n\n\n\nFigure 18: The two stacked bar plots produced by the code\n\n\n\n\nThe Sub-Figure A answers the question, that “On each of the three islands, what proportion of penguins belong to which species?”\nThe Sub-Figure B answers the question reg. distribution of the population of each species of penguins, that is, “For each of the penguin species’, what proportion of each species total population is found on which island?”\n\n\n\n2.6.1 Exercises\n\nRun the following lines of code. Which of the two plots is saved as mpg-plot.png? Why?\nggplot(mpg, aes(x = class)) +   \ngeom_bar() \n\nggplot(mpg, aes(x = cty, y = hwy)) +   \ngeom_point() \n\nggsave(\"mpg-plot.png\")\nThe second plot, i.e., the scatter plot is saved into the file “mpg-plot.png” in the working directory, because the function ggsave() saves only the most recent plot into the file.\nWhat do you need to change in the code above to save the plot as a PDF instead of a PNG? How could you find out what types of image files would work in ggsave()?\nTo save the plot as a PDF file, we will need to add the arguments device  = \"pdf\" to the ggsave() function call. We can find out the types of image files that would work by using the help for ggsave() function by running the code ?ggsave at the command prompt.\nThe documentation for the device argument within ggsave() function tells us that following image document types work with it:--\n\na device function (e.g. png), or\none of “eps”, “ps”, “tex” (pictex), “pdf”, “jpeg”, “tiff”, “png”, “bmp”, “svg” or “wmf” (windows only).\n\n\n\n\n\n\n\nReferences\n\nGorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” Edited by André Chiaradia. PLoS ONE 9 (3): e90081. https://doi.org/10.1371/journal.pone.0090081.\n\n\nHorst, Allison M, Alison Presmanes Hill, and Kristen B Gorman. 2020. Allisonhorst/Palmerpenguins: V0.1.0. Zenodo. https://doi.org/10.5281/ZENODO.3960218.\n\n\nIannone, Richard, Joe Cheng, Barret Schloerke, Ellis Hughes, Alexandra Lauer, and JooYoung Seo. 2023. “Gt: Easily Create Presentation-Ready Display Tables.” https://CRAN.R-project.org/package=gt.\n\n\nMehio-Sibai, Abla, Manning Feinleib, Tarek A. Sibai, and Haroutune K. Armenian. 2005. “A Positive or a Negative Confounding Variable? A Simple Teaching Aid for Clinicians and Students.” Annals of Epidemiology 15 (6): 421–23. https://doi.org/10.1016/j.annepidem.2004.10.004.\n\n\nMock, Thomas. 2022. “gtExtras: Extending ’Gt’ for Beautiful HTML Tables.” https://CRAN.R-project.org/package=gtExtras."
  },
  {
    "objectID": "Chapter30.html",
    "href": "Chapter30.html",
    "title": "Chapter 30",
    "section": "",
    "text": "library(DT)\nlibrary(tidyverse)\nlibrary(knitr)\n\n\n\nLet us try to create an interactive map in HTML below Figure 1:—̥\n\nlibrary(leaflet)\nleaflet() |&gt;\n  setView(76.801175, 30.761403, zoom = 14) |&gt; \n  addTiles() |&gt;\n  addMarkers(76.801175, 30.761403, \n             popup = \"Haryana Civil Secretariat\")  |&gt;\n  addMarkers(76.803773534,\n             30.752910586,\n             popup = \"Rock Garden\")\n\n\n\n\nFigure 1: A map of Chandigarh, India using Leaflet\n\n\n\n\n\nAn example of using DT for an interactive table is at Figure 2 :---\n\ndiamonds |&gt;\n  filter(carat &gt; 3) |&gt;\n  datatable(colnames = c(\"Carat\", \"Cut\", \"Color\",\n                         \"Clarity\", \"Depth\", \"Table\",\n                         \"Price\", \"X\", \"Y\", \"Z\"),\n            rownames = FALSE)\n\n\n\n\n\nFigure 2: A visually pleasing table produced using DT package"
  },
  {
    "objectID": "Chapter30.html#htmlwidgets",
    "href": "Chapter30.html#htmlwidgets",
    "title": "Chapter 30",
    "section": "",
    "text": "Let us try to create an interactive map in HTML below Figure 1:—̥\n\nlibrary(leaflet)\nleaflet() |&gt;\n  setView(76.801175, 30.761403, zoom = 14) |&gt; \n  addTiles() |&gt;\n  addMarkers(76.801175, 30.761403, \n             popup = \"Haryana Civil Secretariat\")  |&gt;\n  addMarkers(76.803773534,\n             30.752910586,\n             popup = \"Rock Garden\")\n\n\n\n\nFigure 1: A map of Chandigarh, India using Leaflet\n\n\n\n\n\nAn example of using DT for an interactive table is at Figure 2 :---\n\ndiamonds |&gt;\n  filter(carat &gt; 3) |&gt;\n  datatable(colnames = c(\"Carat\", \"Cut\", \"Color\",\n                         \"Clarity\", \"Depth\", \"Table\",\n                         \"Price\", \"X\", \"Y\", \"Z\"),\n            rownames = FALSE)\n\n\n\n\n\nFigure 2: A visually pleasing table produced using DT package"
  },
  {
    "objectID": "Chapter4.html",
    "href": "Chapter4.html",
    "title": "Chapter 4",
    "section": "",
    "text": "4.2.5 Exercises\n\nlibrary(tidyverse) \nlibrary(nycflights13) \nlibrary(gt)\ndata(\"flights\")\n\n\nIn a single pipeline for each condition, find all flights that meet the condition:\n\nHad an arrival delay of two or more hours\n\nflights |&gt;   \n  filter(arr_delay &gt;= 120)\n\n# A tibble: 10,200 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      811            630       101     1047            830\n 2  2013     1     1      848           1835       853     1001           1950\n 3  2013     1     1      957            733       144     1056            853\n 4  2013     1     1     1114            900       134     1447           1222\n 5  2013     1     1     1505           1310       115     1638           1431\n 6  2013     1     1     1525           1340       105     1831           1626\n 7  2013     1     1     1549           1445        64     1912           1656\n 8  2013     1     1     1558           1359       119     1718           1515\n 9  2013     1     1     1732           1630        62     2028           1825\n10  2013     1     1     1803           1620       103     2008           1750\n# ℹ 10,190 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nFlew to Houston (IAH or HOU)\n\nflights |&gt;     \n  filter(dest %in% c(\"IAH\", \"HOU\"))\n\n# A tibble: 9,313 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      623            627        -4      933            932\n 4  2013     1     1      728            732        -4     1041           1038\n 5  2013     1     1      739            739         0     1104           1038\n 6  2013     1     1      908            908         0     1228           1219\n 7  2013     1     1     1028           1026         2     1350           1339\n 8  2013     1     1     1044           1045        -1     1352           1351\n 9  2013     1     1     1114            900       134     1447           1222\n10  2013     1     1     1205           1200         5     1503           1505\n# ℹ 9,303 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nWere operated by United, American, or Delta\n\nflights |&gt;   \n  filter(carrier %in% c(\"UA\", \"AA\", \"DL\"))\n\n# A tibble: 139,504 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      554            600        -6      812            837\n 5  2013     1     1      554            558        -4      740            728\n 6  2013     1     1      558            600        -2      753            745\n 7  2013     1     1      558            600        -2      924            917\n 8  2013     1     1      558            600        -2      923            937\n 9  2013     1     1      559            600        -1      941            910\n10  2013     1     1      559            600        -1      854            902\n# ℹ 139,494 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nDeparted in summer (July, August, and September)\n\nflights |&gt;   \n  filter(month %in% c(7, 8, 9))\n\n# A tibble: 86,326 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     7     1        1           2029       212      236           2359\n 2  2013     7     1        2           2359         3      344            344\n 3  2013     7     1       29           2245       104      151              1\n 4  2013     7     1       43           2130       193      322             14\n 5  2013     7     1       44           2150       174      300            100\n 6  2013     7     1       46           2051       235      304           2358\n 7  2013     7     1       48           2001       287      308           2305\n 8  2013     7     1       58           2155       183      335             43\n 9  2013     7     1      100           2146       194      327             30\n10  2013     7     1      100           2245       135      337            135\n# ℹ 86,316 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nArrived more than two hours late, but didn’t leave late\n\nflights |&gt;     \n  filter(dep_delay &lt;= 0) \n\n# A tibble: 200,089 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      544            545        -1     1004           1022\n 2  2013     1     1      554            600        -6      812            837\n 3  2013     1     1      554            558        -4      740            728\n 4  2013     1     1      555            600        -5      913            854\n 5  2013     1     1      557            600        -3      709            723\n 6  2013     1     1      557            600        -3      838            846\n 7  2013     1     1      558            600        -2      753            745\n 8  2013     1     1      558            600        -2      849            851\n 9  2013     1     1      558            600        -2      853            856\n10  2013     1     1      558            600        -2      924            917\n# ℹ 200,079 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nWere delayed by at least an hour, but made up over 30 minutes in flight\n\nflights |&gt;     \n  filter(dep_delay - arr_delay &gt;= 30) \n\n# A tibble: 20,395 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      701            700         1     1123           1154\n 2  2013     1     1      820            820         0     1249           1329\n 3  2013     1     1      840            845        -5     1311           1350\n 4  2013     1     1      857            851         6     1157           1222\n 5  2013     1     1      909            810        59     1331           1315\n 6  2013     1     1     1025            951        34     1258           1302\n 7  2013     1     1     1153           1200        -7     1450           1529\n 8  2013     1     1     1245           1249        -4     1722           1800\n 9  2013     1     1     1610           1615        -5     1913           1948\n10  2013     1     1     1625           1550        35     2054           2050\n# ℹ 20,385 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nSort flights to find the flights with longest departure delays. Find the flights that left earliest in the morning.\nThe top 5 flights that has the longest departure delays are shown in Table 1 .\n\nflights |&gt;   \n  arrange(desc(dep_delay)) |&gt;   \n  head(n = 5) |&gt;\n  select(year, month, day, dep_time, sched_dep_time,\n         dep_delay, carrier, flight, origin) |&gt;\n  mutate(Day = as_date(paste(year, month, day, sep = \"-\"))) |&gt;\n  select(-year, -month, -day) |&gt;\n  relocate(Day, .before = dep_time) |&gt;\n  gt::gt()\n\n\n\n\n\nTable 1:  5 Flights with longest departure delays \n  \n\n    \n      Day\n      dep_time\n      sched_dep_time\n      dep_delay\n      carrier\n      flight\n      origin\n    \n  \n  \n    2013-01-09\n641\n900\n1301\nHA\n51\nJFK\n    2013-06-15\n1432\n1935\n1137\nMQ\n3535\nJFK\n    2013-01-10\n1121\n1635\n1126\nMQ\n3695\nEWR\n    2013-09-20\n1139\n1845\n1014\nAA\n177\nJFK\n    2013-07-22\n845\n1600\n1005\nMQ\n3075\nJFK\n  \n\n\n\n\n\n\n\nSort flights to find the fastest flights. (Hint: Try including a math calculation inside of your function.)\nThe speed of a flight can be found as distance/air_time . The Table 2 displays the 5 fastest flights.\n\nflights |&gt;\n  arrange(desc(distance/air_time)) |&gt;\n  slice_head(n=5) |&gt;\n  select(year, month, day, distance, air_time,\n         carrier, flight, origin, dest) |&gt;\n  gt()\n\n\n\n\n\nTable 2:  5 fastest Flights (by speed) \n  \n\n    \n      year\n      month\n      day\n      distance\n      air_time\n      carrier\n      flight\n      origin\n      dest\n    \n  \n  \n    2013\n5\n25\n762\n65\nDL\n1499\nLGA\nATL\n    2013\n7\n2\n1008\n93\nEV\n4667\nEWR\nMSP\n    2013\n5\n13\n594\n55\nEV\n4292\nEWR\nGSP\n    2013\n3\n23\n748\n70\nEV\n3805\nEWR\nBNA\n    2013\n1\n12\n1035\n105\nDL\n1902\nLGA\nPBI\n  \n\n\n\n\n\n\n\nWas there a flight on every day of 2013?\nYes, there was a flight on every day of 2013, because using distinct() function, we find that there are 365 unique combinations of year , month , and day .\n\nflights |&gt;\n  distinct(year, month, day) |&gt;\n  count() |&gt;\n  as.numeric()\n\n[1] 365\n\n\nWhich flights traveled the farthest distance? Which traveled the least distance?\nThe top 5 flights by the farthest distance traveled are shown in Table 3 .\n\nflights |&gt;\n  arrange(desc(distance)) |&gt;\n  select(origin, dest, distance, air_time, carrier) |&gt;\n  # Distinct added to remove same flight (on different days) repeating in top 5\n  distinct(origin, dest, .keep_all = TRUE) |&gt;\n  slice_head(n = 5) |&gt;\n  gt()\n\n\n\n\n\nTable 3:  5 longest distance flights \n  \n\n    \n      origin\n      dest\n      distance\n      air_time\n      carrier\n    \n  \n  \n    JFK\nHNL\n4983\n659\nHA\n    EWR\nHNL\n4963\n656\nUA\n    EWR\nANC\n3370\n418\nUA\n    JFK\nSFO\n2586\n366\nUA\n    JFK\nOAK\n2576\n330\nB6\n  \n\n\n\n\n\n\n\nThe 5 flights with least distance traveled are shown in\n\nflights |&gt;\n  arrange(distance) |&gt;\n  select(origin, dest, distance, air_time, carrier) |&gt;\n  # Distinct added to remove same flight (which runs\n  # on different days) repeating in top 5\n  distinct(origin, dest, .keep_all = TRUE) |&gt;\n  slice_head(n = 5) |&gt;\n  gt()\n\n\n\n\n\nTable 4:  5 shortest distance flights \n  \n\n    \n      origin\n      dest\n      distance\n      air_time\n      carrier\n    \n  \n  \n    EWR\nLGA\n17\nNA\nUS\n    EWR\nPHL\n80\n30\nEV\n    JFK\nPHL\n94\n35\n9E\n    LGA\nPHL\n96\n32\nUS\n    EWR\nBDL\n116\n25\nEV\n  \n\n\n\n\n\n\n\nDoes it matter what order you used filter() and arrange() if you’re using both? Why/why not? Think about the results and how much work the functions would have to do.\nAlthough, in terms of output received, it does not matter in which order we use them, because when we run the function filter() it removes the rows not required, but leaves the arrangement-ordering the same, i.e. the remaining rows move up.\nHowever, using arrange() before filter() means R will have to arrange all the rows, and then we filter out only a few rows - thus meaning that more work will have to be done computationally.\nFor computational efficiency, it would be better if we use filter() first, then run arrange() only on the subset of rows remaining.\nHere’s the proof for this, using system.time() function in R which tells how much time does an R expression take to run. Here, I compare both functions using the logical operator &gt; (greater than). The elapsed time comes TRUE, meaning that arranging first, and then filtering takes more time.\n\nsystem.time( flights |&gt;\n  arrange(distance) |&gt;\n  filter(air_time &lt; 60)\n  ) &gt; system.time(\n  flights |&gt;\n    filter(air_time &lt; 60) |&gt;\n    arrange(distance)\n)\n\n user.self   sys.self    elapsed user.child  sys.child \n     FALSE      FALSE       TRUE         NA         NA \n\n\n\n\n\n4.3.5 Exercises\n\nCompare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?\nWe would expect dep_delay = dep_time - sched_dep_time . Let us check this in the code, as well.\n\nflights |&gt;\n  mutate(calc = dep_time - sched_dep_time) |&gt;\n  mutate(match = calc == dep_delay, .keep = \"used\") |&gt;\n  summarise(Matching = sum(match, na.rm = TRUE),\n            Total = count(flights)) |&gt;\n  mutate(Percentage = 100*Matching/Total)\n\n# A tibble: 1 × 3\n  Matching Total$n Percentage$n\n     &lt;int&gt;   &lt;int&gt;        &lt;dbl&gt;\n1   228744  336776         67.9\n\n\nThe results indicate that 67.9% of the time, the comparison works out as expected. For others, there might be missing data issues (hence, we had to use na.rm = TRUE) or, any other data error.\nBrainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights.\n\n# Using variable names\nflights |&gt; \n  select(dep_time, dep_delay, arr_time, arr_delay)\n\n# Using starts_with()\nflights |&gt; \n  select(starts_with(\"dep\"), starts_with(\"arr\"))\n\n# Using column numbers\nflights |&gt;\n  select(4,6,7,9)\n\n# Using from, to, ie., \":\" along with \"!\" to remove sched_ \nflights |&gt; \n  select(dep_time:arr_delay) |&gt;\n  select(!starts_with(\"sched\"))\n\n# Using column numbers with :\nflights |&gt;\n  select(4:9, -5, -8)\n\nWhat happens if you specify the name of the same variable multiple times in a select() call?\nIf we specify the name of the same variable multiple times, the dplyr package understands the mistake, and only produces one copy of the variable in the output. The place of the variable is the one that first appears in the code within the select() function. Here are two examples:---\n\nflights |&gt;\n  select(dep_time, dep_time) |&gt;\n  slice_head(n=2)\n\n# A tibble: 2 × 1\n  dep_time\n     &lt;int&gt;\n1      517\n2      533\n\nflights |&gt;\n  select(dep_time:dep_delay, sched_dep_time) |&gt;\n  slice_head(n=2)\n\n# A tibble: 2 × 3\n  dep_time sched_dep_time dep_delay\n     &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;\n1      517            515         2\n2      533            529         4\n\n\nWhat does the any_of() function do? Why might it be helpful in conjunction with this vector?\nvariables &lt;- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")\nThe two functions, any_of() and all_of() are called selection helpers. They help select variables contained in a character vector, such as variables .\nIn present scenario, the any_of() can be used with variables vector to select these columns (or, remove these columns) from the flights data-set, as shown in the code below:---\n\nvariables &lt;- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")\n\nflights |&gt;\n  select(any_of(variables)) |&gt;\n  slice_head(n=2)\n\n# A tibble: 2 × 5\n   year month   day dep_delay arr_delay\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1  2013     1     1         2        11\n2  2013     1     1         4        20\n\n\nBut, there is a difference between any_of() and all_of() . As shown in R help, all_of() is for strict selection. If any of the variables in the character vector is missing, an error is thrown. But, any_of() doesn’t check for missing variables. It is especially useful with negative selections, when you would like to make sure a variable is removed. Here’s an example to demonstrate:---\n\n# Change \"day\" to \"date\" to delibertely cause a missing variable name\nvariables &lt;- c(\"year\", \"month\", \"date\", \"dep_delay\", \"arr_delay\")\n\n# all_of() should not work\nflights |&gt;\n  select(all_of(variables)) |&gt;\n  slice_head(n=2)\n\nError in `all_of()`:\n! Can't subset columns that don't exist.\n✖ Column `date` doesn't exist.\n\n# any_of() will still work\nflights |&gt;\n  select(any_of(variables)) |&gt;\n  slice_head(n=2)\n\n# A tibble: 2 × 4\n   year month dep_delay arr_delay\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1  2013     1         2        11\n2  2013     1         4        20\n\n\nDoes the result of running the following code surprise you? How do the select helpers deal with upper and lower case by default? How can you change that default? flights |&gt; select(contains(\"TIME\"))\nYes, the running of this code surprises me because generally, R is very picky about upper-case vs. lower-case. Since \"TIME\" is not contained in any variable name, I expected it to throw an error. Yet, it returns all variables which contain \"time\".\nThus, this means that the following select helpers from tidyselect package ignore the case of the match provided by default.\n\nstarts_with(): Starts with an exact prefix.\nends_with(): Ends with an exact suffix.\ncontains(): Contains a literal string.\nmatches(): Matches a regular expression.\n\nTo change this, we can set the argument ignore.case = FALSE.\nRename air_time to air_time_min to indicate units of measurement and move it to the beginning of the data frame.\nThe following code does the job, as shown in the output:--\n\nflights |&gt;\n  rename(air_time_min = air_time) |&gt;\n  relocate(air_time_min)\n\n# A tibble: 336,776 × 19\n   air_time_min  year month   day dep_time sched_dep_time dep_delay arr_time\n          &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1          227  2013     1     1      517            515         2      830\n 2          227  2013     1     1      533            529         4      850\n 3          160  2013     1     1      542            540         2      923\n 4          183  2013     1     1      544            545        -1     1004\n 5          116  2013     1     1      554            600        -6      812\n 6          150  2013     1     1      554            558        -4      740\n 7          158  2013     1     1      555            600        -5      913\n 8           53  2013     1     1      557            600        -3      709\n 9          140  2013     1     1      557            600        -3      838\n10          138  2013     1     1      558            600        -2      753\n# ℹ 336,766 more rows\n# ℹ 11 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nWhy doesn’t the following work, and what does the error mean?\nflights |&gt;    \n  select(tailnum) |&gt;    \n  arrange(arr_delay) \n\n#&gt; Error in `arrange()`: #&gt; ℹ In argument: `..1 = arr_delay`.\n#&gt; Caused by error: #&gt; ! object 'arr_delay' not found\nThe above code does not work because the select(tailnum) has removed all other variables (columns) from the tibble. Thus, when arrange(arr_delay) runs, it is unable to find any variable by the name of arr_delay in the tibble.\nThe error means that object (i.e. variable) 'arr_delay' has not been found in the tibble by the arrange() function.\n\n\n\n4.5.7 Exercises\n\nWhich carrier has the worst average delays? Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights |&gt; group_by(carrier, dest) |&gt; summarize(n()))\nThe overall carrier with worst average delays is F9, as seen from the code below:\n\nflights |&gt;\n  group_by(carrier) |&gt;\n  summarise(avg_delay = mean(arr_delay, na.rm = TRUE)) |&gt;\n  slice_max(order_by = avg_delay, n = 1)\n\n# A tibble: 1 × 2\n  carrier avg_delay\n  &lt;chr&gt;       &lt;dbl&gt;\n1 F9           21.9\n\n\nYes, we can disentangle the effect of bad airports vs. bad carriers using the code below:---\n\nflights |&gt;\n  group_by(dest, carrier) |&gt;\n  summarise(avg_delay = mean(arr_delay, na.rm = TRUE)) |&gt;\n  # taking the highest average delay flight at each airport\n  slice_max(order_by = avg_delay, n = 1) |&gt;\n  ungroup() |&gt;\n  # for each airline, summarize the number of airports where it is\n  # the most delayed airline\n  summarise(n = n(), .by = carrier) |&gt;\n  slice_head(n=5)|&gt;\n  arrange(desc(n)) |&gt;\n  rename(Carrier = carrier,\n         `Number of Airports` = n) |&gt;\n  gt()\n\n\n\n\n\nTable 5:  The airlines which have highest average delay at the maximum number\nof Airports \n  \n\n    \n      Carrier\n      Number of Airports\n    \n  \n  \n    EV\n42\n    B6\n20\n    UA\n14\n    AA\n6\n    FL\n2\n  \n\n\n\n\n\n\n\nFind the flights that are most delayed upon departure from each destination.\nWe can use the following codes (either one works, and the code checks this equality of results) to find the flight with highest departure delay (dep_delay ) for each destination (Table 6).\n\n\nCode\n# Option 1: Group by \"dest\" and then use slice_max\na = flights |&gt;\n  group_by(dest) |&gt;\n  slice_max(n=1, order_by = dep_delay, na_rm = TRUE) |&gt;\n  select(dest, flight, origin, dep_delay, month, day) |&gt;\n  arrange(desc(dep_delay), desc(flight))\n# Option 2: Directly use slice_max() with \"by\" argument\nb = flights |&gt;\n  slice_max(n=1, order_by = dep_delay, by = dest, na_rm = TRUE) |&gt;\n  select(dest, flight, origin, dep_delay, month, day) |&gt;\n  arrange(desc(dep_delay), desc(flight))\n# Check results\nsum(a != b)\n\n\n[1] 0\n\n\nCode\n# Display results\nb |&gt; \n  slice_head(n=5) |&gt; \n  gt() |&gt;\n  cols_label(dest = \"Destination\", \n             flight = \"Flight\", \n             origin = \"Origin Airport\",\n             dep_delay = \"Departure Depay (minutes)\", \n             month = \"Month\", \n             day = \"Date\") |&gt;\n  cols_align(align = \"center\")\n\n\n\n\n\n\nTable 6:  Flights with highest departure delay; displayed here only for 5\ndestinations highest departure delay \n  \n\n    \n      Destination\n      Flight\n      Origin Airport\n      Departure Depay (minutes)\n      Month\n      Date\n    \n  \n  \n    HNL\n51\nJFK\n1301\n1\n9\n    CMH\n3535\nJFK\n1137\n6\n15\n    ORD\n3695\nEWR\n1126\n1\n10\n    SFO\n177\nJFK\n1014\n9\n20\n    CVG\n3075\nJFK\n1005\n7\n22\n  \n\n\n\n\n\n\n\nHow do delays vary over the course of the day. Illustrate your answer with a plot.\nThe following graph of average delay (on y-axis) plotted against scheduled departure time (on x-axis) shows the overall trend that the average delays rise over the course of the day to hit a peak around 6 pm.\nNote: The scheduled departure time is not accurate in the data-set, since it is written in hhmm format, and thus is not continuous variable. For example, 1:59 am is 159, and then 2:00 am is 200. So there are no values in 60s, 70s, 80s, 90s. I rectified this using mathematical operators %/% and %% to obtain hours and minutes, and then combined them. Now, the result is a smoother graph.\n\nflights |&gt;\n  group_by(sched_dep_time) |&gt;\n  summarise(avg_delay = mean(dep_delay, na.rm = TRUE)) |&gt;\n  mutate(hour = sched_dep_time %/% 100,\n         minute = sched_dep_time %% 100) |&gt;\n  mutate(time_hr = hour + minute/60) |&gt;\n  ggplot(aes(x = time_hr, y = avg_delay)) +\n  geom_line() +\n  geom_smooth(color = \"red\", se = FALSE) +\n  theme_bw() + \n  labs(x = \"Scheduled Departure time (in Hrs.)\",\n       y = \"Average delay in minutes\") +\n  scale_x_continuous(breaks = seq(from = 0, to = 24, by = 4))\n\n\n\nFigure 1: Graph showing average delays over the course of the day at various scheduled times\n\n\n\n\n\nWhat happens if you supply a negative n to slice_min() and friends?\nThe inbuilt R help tells me that “A negative value of n will be subtracted from the group size. For example, n = -2 with a group of 5 rows will select 5 - 2 = 3 rows.”\nHere’s an example to explain. First, I create a tibble a (shown in Table 7) to contain the average departure delay from JFK airport to 10 destinations.\n\na = flights |&gt;\n  filter(origin == \"JFK\") |&gt;\n  group_by(origin, dest) |&gt;\n  summarise(avg_delay = mean(dep_delay, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_delay)) |&gt;\n  slice_head(n = 10) |&gt;\n  ungroup()\ngt(a) |&gt;\n  fmt_number(decimals = 2)\n\n\n\n\n\nTable 7:  The average departure delay from JFK airport to 10 destinations \n  \n\n    \n      origin\n      dest\n      avg_delay\n    \n  \n  \n    JFK\nCVG\n27.35\n    JFK\nSDF\n23.98\n    JFK\nEGE\n23.44\n    JFK\nSAT\n23.41\n    JFK\nMCI\n23.09\n    JFK\nCMH\n22.02\n    JFK\nORD\n21.55\n    JFK\nMSP\n21.33\n    JFK\nDEN\n20.10\n    JFK\nSTL\n20.00\n  \n\n\n\n\n\n\n\nNow, I use slice_min function with arguments n = 2 and then, with arguments n = -2 to show the difference in output. The first code n = 2 displays the two rows with minimum average delay. The second code, n = -2 displays the (total rows minus 2), i.e., 8 rows with minimum average delay.\n\n# n=2 displays the two rows with minimum average delay\na |&gt;\n  slice_min(n = 2, order_by = avg_delay)\n\n# A tibble: 2 × 3\n  origin dest  avg_delay\n  &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;\n1 JFK    STL        20  \n2 JFK    DEN        20.1\n\n# n=-2 displays the (total rows minus 2), i.e., 8 rows with minimum average delay\na |&gt;\n  slice_min(n = -2, order_by = avg_delay)\n\n# A tibble: 8 × 3\n  origin dest  avg_delay\n  &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;\n1 JFK    STL        20  \n2 JFK    DEN        20.1\n3 JFK    MSP        21.3\n4 JFK    ORD        21.6\n5 JFK    CMH        22.0\n6 JFK    MCI        23.1\n7 JFK    SAT        23.4\n8 JFK    EGE        23.4\n\n\nExplain what count() does in terms of the dplyr verbs you just learned. What does the sort argument to count() do?\nInstead of using the group_by() and summarize() verbs, the count() function can be used as a shortcut to quickly compute the number of unique values of each combination of a variable occurring in the data-set. Thus, count() helps us to calculate the number of values (rows) for each unique combination of variables which have been used as an argument in the count() function.\nThe inbuilt help in R tells us that df %&gt;% count(a, b) is roughly equivalent to df %&gt;% group_by(a, b) %&gt;% summarise(n = n()) .\nFurther, the sort = TRUE argument in count() tells R to display the largest groups (by count, i.e., n) to be displayed at the top.\nHere’s an example. The following code displays the 5 routes with maximum number of flights. For example, JFK to LAX had 11,262 flights in 2013. We can achieve this by using group_by(), summarize(), arrange() and ungroup(). Or, we can simply achieve the same result with a single function count().\n\nflights |&gt;\n  group_by(origin, dest) |&gt;\n  summarise(n = n()) |&gt;\n  arrange(desc(n)) |&gt;\n  ungroup() |&gt;\n  slice_head(n = 5)\n\n# A tibble: 5 × 3\n  origin dest      n\n  &lt;chr&gt;  &lt;chr&gt; &lt;int&gt;\n1 JFK    LAX   11262\n2 LGA    ATL   10263\n3 LGA    ORD    8857\n4 JFK    SFO    8204\n5 LGA    CLT    6168\n\nflights |&gt;\n  count(origin, dest, sort = TRUE) |&gt;\n  slice_head(n = 5)\n\n# A tibble: 5 × 3\n  origin dest      n\n  &lt;chr&gt;  &lt;chr&gt; &lt;int&gt;\n1 JFK    LAX   11262\n2 LGA    ATL   10263\n3 LGA    ORD    8857\n4 JFK    SFO    8204\n5 LGA    CLT    6168\n\n\nSuppose we have the following tiny data frame:\n\ndf &lt;- tibble(x = 1:5,   \n             y = c(\"a\", \"b\", \"a\", \"a\", \"b\"),   \n             z = c(\"K\", \"K\", \"L\", \"L\", \"K\") )\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what group_by() does.\ndf |&gt;   \n  group_by(y)\nIn my understanding, the output should look the same as df except that on top of it, a line mentioning that data is grouped by y should appear. When we run the code, it shows the following header # A tibble: 5 X 3 and #Groups: y[2] . Thus, there are two groups formed by two unique values of variable y , i.e., a and b .\n\ndf |&gt;\n  group_by(y)\n\n# A tibble: 5 × 3\n# Groups:   y [2]\n      x y     z    \n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 a     K    \n2     2 b     K    \n3     3 a     L    \n4     4 a     L    \n5     5 b     K    \n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what arrange() does. Also comment on how it’s different from the group_by() in part (a)?\ndf |&gt;   arrange(y)\nThe function arrange() re-orders the data-frame rows in ascending order of the variable mentioned, i.e. y . So, I expect the output to be the df tibble with ascending order of variable y . The ties will be arranged in the same order as they appeared in the original data-frame.\n\ndf |&gt;\n  arrange(y)\n\n# A tibble: 5 × 3\n      x y     z    \n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 a     K    \n2     3 a     L    \n3     4 a     L    \n4     2 b     K    \n5     5 b     K    \n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what the pipeline does.\ndf |&gt;   \n  group_by(y) |&gt;   \n  summarize(mean_x = mean(x))\nThe output should display the mean values of x for different values of y . For y = a , I expect mean_x = (1+3+4)/3 = 2.67 and for y = b , I expect mean_x = (2+5)/2 = 3.5 . I expect the output to be a 2 X 2 tibble with first column y and second column mean_x .\n\ndf |&gt;   \n  group_by(y) |&gt;   \n  summarize(mean_x = mean(x))\n\n# A tibble: 2 × 2\n  y     mean_x\n  &lt;chr&gt;  &lt;dbl&gt;\n1 a       2.67\n2 b       3.5 \n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what the pipeline does. Then, comment on what the message says.\ndf |&gt;   \n  group_by(y, z) |&gt;   \n  summarize(mean_x = mean(x))\nNow, I expect R to form groups of various combinations of y and z , and then display average value of x for each combination. The output should be a tibble of 3 X 3, and still containing two groups of y .\n\ndf |&gt;   \n  group_by(y, z) |&gt;   \n  summarize(mean_x = mean(x))\n\n# A tibble: 3 × 3\n# Groups:   y [2]\n  y     z     mean_x\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 a     K        1  \n2 a     L        3.5\n3 b     K        3.5\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what the pipeline does. How is the output different from the one in part (d).\ndf |&gt;   \n  group_by(y, z) |&gt;   \n  summarize(mean_x = mean(x), .groups = \"drop\")\nI think the output will still be a 3 X 3 tibble with same values as answer from Question 6 (d), i.e. displaying average values of x for different combinations of y and z . But, now the remaining grouping, i.e., of y will be dropped from the output. So the output is visually the same, but now it is an un-grouped tibble, rather than the grouped tibble output of Question 6 (d).\n\ndf |&gt;   \n  group_by(y, z) |&gt;   \n  summarize(mean_x = mean(x), .groups = \"drop\")\n\n# A tibble: 3 × 3\n  y     z     mean_x\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 a     K        1  \n2 a     L        3.5\n3 b     K        3.5\n\n\nWrite down what you think the outputs will look like, then check if you were correct, and describe what each pipeline does. How are the outputs of the two pipelines different?\n# Code Chunk 1\ndf |&gt;   \n  group_by(y, z) |&gt;   \n  summarize(mean_x = mean(x))  \n\n# Code Chunk 2\ndf |&gt;   \n  group_by(y, z) |&gt;   \n  mutate(mean_x = mean(x))\nThe answers should be different because summarize() collapses all the rows for a unique combination of grouped variables to produce one summary row. On the other hand, mutate() preserves each row of the original data-frame (or, tibble) and produces and additional variable with mean of x to be entered in each row.\nThus, I expect the # Code Chunk 1 to generate a tibble of 3 X 3 (like the output in Question 6(d), while I expect the # Code Chunk 2 to generate a tibble of 5 X 4, with the 4th column of mean_x having different values for each unique combination of y and z .\nFurther, I expect that the # Code Chunk 1 will re-order the output in ascending of order grouping variables. But, the # Code Chunk 2 will preserve the original ordering of the rows as in the original df tibble.\n\n# Code Chunk 1\ndf |&gt;   \n  group_by(y, z) |&gt;   \n  summarize(mean_x = mean(x))  \n\n# A tibble: 3 × 3\n# Groups:   y [2]\n  y     z     mean_x\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 a     K        1  \n2 a     L        3.5\n3 b     K        3.5\n\n# Code Chunk 2\ndf |&gt;   \n  group_by(y, z) |&gt;   \n  mutate(mean_x = mean(x))\n\n# A tibble: 5 × 4\n# Groups:   y, z [3]\n      x y     z     mean_x\n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1     1 a     K        1  \n2     2 b     K        3.5\n3     3 a     L        3.5\n4     4 a     L        3.5\n5     5 b     K        3.5\n\n\nNote: It is only by chance that the mean_x is 3.5 for both combinations:\n\ny=b, z=K , mean_x = (2+5)/2 = 3.5\ny=a, z=L , mean_x = (3+4)/2 = 3.5"
  },
  {
    "objectID": "Chapter3.html",
    "href": "Chapter3.html",
    "title": "Chapter 3",
    "section": "",
    "text": "Some important tips:\n\nUse Alt + - to write the assignment operator &lt;- in R.\nIn the comments, i.e, text written after # in code, explain the WHY of your code, not the WHAT or HOW.\n\n\n\n3.5 Exercises\n\nWhy does this code not work?\nmy_variable &lt;- 10 \nmy_varıable \n#&gt; Error in eval(expr, envir, enclos): object 'my_varıable' not found\nLook carefully! (This may seem like an exercise in pointlessness, but training your brain to notice even the tiniest difference will pay off when programming.)\nThe code does not work because of the minor spelling difference, i.e., i vs. ī .\nTweak each of the following R commands so that they run correctly:\nlibary(todyverse)  \nggplot(dTA = mpg) +    \n  geom_point(maping = aes(x = displ y = hwy)) +   \n  geom_smooth(method = \"lm)\nThe corrected code is as follows:---\n\nlibrary(tidyverse)  \nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +    \n  geom_point() +   \n  geom_smooth(method = \"lm\")\n\nPress Option + Shift + K / Alt + Shift + K. What happens? How can you get to the same place using the menus?\nThe Alt + Shift + K shortcut brings up the Keyboard Shortcut Quick Reference. We could get to the same using menus as Help –&gt; Keyboard Shortcuts Help.\nLet’s revisit an exercise from the Section 2.6. Run the following lines of code. Which of the two plots is saved as mpg-plot.png? Why?\nmy_bar_plot &lt;- ggplot(mpg, aes(x = class)) +   \n  geom_bar() \n\nmy_scatter_plot &lt;- ggplot(mpg, aes(x = cty, y = hwy)) +\n  geom_point() \n\nggsave(filename = \"mpg-plot.png\", plot = my_bar_plot)\nThis time, the bar plot, i.e. my_bar_plot is saved into the file mpg-plot.png because in the arguments to the function ggsave() we have specified the name of the plot. The plot argument tells ggsave() the Plot to save, and by default, it goes to the last plot displayed."
  },
  {
    "objectID": "Chapter5.html",
    "href": "Chapter5.html",
    "title": "Chapter 5",
    "section": "",
    "text": "5.6 Exercises\n\nRestyle the following pipelines following the guidelines above.\nflights|&gt;filter(dest==\"IAH\")|&gt;group_by(year,month,day)|&gt;summarize(n=n(),\ndelay=mean(arr_delay,na.rm=TRUE))|&gt;filter(n&gt;10)\n\nflights|&gt;filter(carrier==\"UA\",dest%in%c(\"IAH\",\"HOU\"),sched_dep_time&gt;\n0900,sched_arr_time&lt;2000)|&gt;group_by(flight)|&gt;summarize(delay=mean(\narr_delay,na.rm=TRUE),cancelled=sum(is.na(arr_delay)),n=n())|&gt;filter(n&gt;10)\nThe restyled code is as below:---\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nflights |&gt;\n  filter(dest == \"IAH\") |&gt;\n  group_by(year, month, day) |&gt;\n  summarize(\n    n = n(),\n    delay = mean(arr_delay, na.rm = TRUE)\n  ) |&gt;\n  filter(n &gt; 10)\n\nflights |&gt;\n  filter(\n    carrier == \"UA\",\n    dest %in% c(\"IAH\", \"HOU\"),\n    sched_dep_time &gt; 0900,\n    sched_arr_time &lt; 2000\n  ) |&gt;\n  group_by(flight) |&gt;\n  summarize(\n    delay = mean(arr_delay, na.rm = TRUE),\n    cancelled = sum(is.na(arr_delay)), n = n()\n  ) |&gt;\n  filter(n &gt; 10)\n\nLet us try to use the styler package for the same task, using Ctrl + Shift + P . The styled code using styler is shown below:---\n\nflights |&gt;\n  filter(dest == \"IAH\") |&gt;\n  group_by(year, month, day) |&gt;\n  summarise(\n    n = n(),\n    delay = mean(arr_delay, na.rm = TRUE)\n  ) |&gt;\n  filter(n &gt; 10)\n\nflights |&gt;\n  filter(carrier == \"UA\", dest %in% c(\"IAH\", \"HOU\"), sched_dep_time &gt;\n    0900, sched_arr_time &lt; 2000) |&gt;\n  group_by(flight) |&gt;\n  summarise(delay = mean(\n    arr_delay,\n    na.rm = TRUE\n  ), cancelled = sum(is.na(arr_delay)), n = n()) |&gt;\n  filter(n &gt; 10)"
  },
  {
    "objectID": "Chapter6.html",
    "href": "Chapter6.html",
    "title": "Chapter 6",
    "section": "",
    "text": "6.2.1 Exercises\n\nQuestion 1\nFor each of the sample tables, describe what each observation and each column represents.\nFor table1 , the following columns represent:--\n\nCountry\nYear of the observation of cases and population\nNumber of cases\nTotal Population for that year\n\nFor table1 , each observation represents number of cases and total population for a country in a given year.\n\nFor table2 , the following columns represent:--\n\nCountry\nYear of the Observation\nWhich type of variable is represented in column 4 - i.e., cases or population. Thus, this column in itself is not a variable. Thus, this data-set is not tidy.\nThe actual value (i.e. observation) of the variable mentioned in Column 3.\n\nFor table2 , each observation represents either the number of cases or the total population for a country in a given year.\n\nFor table3 , the columns represent the following:--\n\nCountry\nYear of the observation\nThe ratio of two observations, i.e. rate = cases divided by the population. Thus, the column 3 represents two observations, not one. Hence, the data is not tidy.\n\nFor table3 , each observation is a rate, i.e., actually it is a ratio of two observations, namely, cases and population.\n\n\nQuestion 2\nSketch out the process you’d use to calculate the rate for table2 and table3. You will need to perform four operations:\n\na.\nExtract the number of TB cases per country per year.\nFor table2 , we will have to filter out rows where type == \"cases\" .\nFor table3 , we will have to extract the numerator from rate variable for each row.\n\n\nb.\nExtract the matching population per country per year.\nFor table2 , we will have to filter out rows where type == \"population\" .\nFor table3 , we will have to extract the denominator from rate variable for each row.\n\n\nc.\nDivide cases by population, and multiply by 10000.\nFor table2 , we will have to divide the observations from question 2 (a) by observations from question 2(b). We might also want to check that the year and country match, row by row.\nFor table3 , we can divide the numerator by denominator, and multiply by 10,000. Or simply calculate the expression in rate column, as a numeric.\n\n\nd.\nStore back in the appropriate place.\nFor table2 , we will have to re-save the rates in a new set of rows, where type == \"rate\" and count will be the calculated rate. Thus, the table2 will have 6 new rows.\nFor table3 , we will have to convert rate column into numeric, to get the ratio per 10,000. But we will end up losing information, i.e. the cases and population of each country for different years will be lost if data is reported directly as rate .\n\n\n\n\nSection 6.3.4\nData and variable names in the column headers\nHere’s an attempt to recreate \".values\" argument method in R :---\n\nlibrary(tidyverse)\nhousehold |&gt;\n  pivot_longer(\n    cols = !family,\n    names_to = c(\".value\", \"child\"),\n    names_sep = \"_\",\n    values_to = \"Value\",\n    values_drop_na = TRUE\n  )\n\n# A tibble: 9 × 4\n  family child  dob        name  \n   &lt;int&gt; &lt;chr&gt;  &lt;date&gt;     &lt;chr&gt; \n1      1 child1 1998-11-26 Susan \n2      1 child2 2000-01-29 Jose  \n3      2 child1 1996-06-22 Mark  \n4      3 child1 2002-07-11 Sam   \n5      3 child2 2004-04-05 Seth  \n6      4 child1 2004-10-10 Craig \n7      4 child2 2009-08-27 Khai  \n8      5 child1 2000-12-05 Parker\n9      5 child2 2005-02-28 Gracie\n\n\n\n\nSection 6.4.1\nHow does the pivot_wider() work?\nHere I try to understand what is the output from pivot_wider() is there are more than 1 unique values for a measurement, i.e.. there are two bp1 ’s for A .\n\ndf &lt;- tribble(\n  ~id, ~measurement, ~value,\n  \"A\",        \"bp1\",    100,\n  \"A\",        \"bp1\",    102,\n  \"A\",        \"bp2\",    120,\n  \"B\",        \"bp1\",    140, \n  \"B\",        \"bp2\",    115\n)\ndf |&gt;\n  pivot_wider(\n    id_cols = id,\n    names_from = measurement,\n    values_from = value\n  )\n## Warning: Values from `value` are not uniquely identified; output will contain list-cols.\n## • Use `values_fn = list` to suppress this warning.\n## • Use `values_fn = {summary_fun}` to summarise duplicates.\n## • Use the following dplyr code to identify duplicates.\n##   {data} %&gt;%\n##   dplyr::group_by(id, measurement) %&gt;%\n##   dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %&gt;%\n##   dplyr::filter(n &gt; 1L)\n## # A tibble: 2 × 3\n##   id    bp1       bp2      \n##   &lt;chr&gt; &lt;list&gt;    &lt;list&gt;   \n## 1 A     &lt;dbl [2]&gt; &lt;dbl [1]&gt;\n## 2 B     &lt;dbl [1]&gt; &lt;dbl [1]&gt;\n\n# Using the Code given by R in Warning to find out the \n# duplicate observation\ndf |&gt;\n  group_by(id, measurement) |&gt;\n  summarise(n = n(), .groups = \"drop\") |&gt;\n  filter(n &gt; 1)\n## # A tibble: 1 × 3\n##   id    measurement     n\n##   &lt;chr&gt; &lt;chr&gt;       &lt;int&gt;\n## 1 A     bp1             2"
  },
  {
    "objectID": "Chapter7.html",
    "href": "Chapter7.html",
    "title": "Chapter 7",
    "section": "",
    "text": "7.3 Exercises\n\nQuestion 1\nGo to the RStudio Tips Twitter account, https://twitter.com/rstudiotips and find one tip that looks interesting. Practice using it!\nOne tip that I found interesting is the use of Ctrl + Shift + P to open the R-Studio Command Palette. I have embedded the tweet here by inserting a simple HTML block, and pasting the HTML code from public.twitter.com link.\nAccess it with Ctrl + Shift + P (Windows / Linux) or Cmd + Shift + P (macOS)!#rstats https://t.co/pWAYHGCWRr— RStudio Tips (@rstudiotips) October 26, 2022 \n\n\nQuestion 2\nWhat other common mistakes will RStudio diagnostics report? Read https://support.posit.co/hc/en-us/articles/205753617-Code-Diagnostics to find out.\nSome of the common mistakes that RStudio diagnostics will report are:--\n\nIt can detect if a variable named in a function has not yet been defined (i.e. the variable used has no definition in scope), or is misspelt.\nIt can detect if a variable has been defined, but is not being used within a function.\nIt can detect the missing punctuation, i.e. a missing comma or missing brackets.\nIt can detect whether the call within a function can work or not, i.e., whether the arguments to a function are matched, partially matched or unmatched.\nIt can detect if an essential argument to a function is missing.\nIt can provide us R code style diagnostics, e.g. white-space etc.\nIt can even detect mistakes in other languages such as C , C++ , JavaScript and Python ."
  },
  {
    "objectID": "Chapter8.html",
    "href": "Chapter8.html",
    "title": "Chapter 8",
    "section": "",
    "text": "8.2.4 Exercises\n\nlibrary(tidyverse)\nlibrary(gt)\n\n\nQuestion 1\nWhat function would you use to read a file where fields were separated with “|”?\nLet us first create a data set with “|” delimiter. I used ChatGPT to create a random data set and named it as the data to be imported, i.e., imp_df .\nI will use the function read_delim() to read a file where fields were separated with “|”, as shown below. The output is\n\n#| label: tbl-q1-Ex-8.2.4\n#| tbl-cap: \"Imported data using read_delim() function\"\n\nimport_df = \"Name|Age|Gender|City|Salary\nJohn|28|Male|New York|75000\nEmily|22|Female|Los Angeles|60000\nMichael|31|Male|Chicago|80000\nJessica|25|Female|Houston|65000\nWilliam|29|Male|Miami|70000\nSophia|27|Female|San Francisco|75000\nDaniel|24|Male|Seattle|72000\nOlivia|30|Female|Boston|78000\nJames|26|Male|Dallas|67000\nAva|23|Female|Atlanta|62000\"\n\ndf = read_delim(import_df, delim = \"|\")\n\ndf |&gt;\n  gt()\n\n\n\n\n\n  \n    \n    \n      Name\n      Age\n      Gender\n      City\n      Salary\n    \n  \n  \n    John\n28\nMale\nNew York\n75000\n    Emily\n22\nFemale\nLos Angeles\n60000\n    Michael\n31\nMale\nChicago\n80000\n    Jessica\n25\nFemale\nHouston\n65000\n    William\n29\nMale\nMiami\n70000\n    Sophia\n27\nFemale\nSan Francisco\n75000\n    Daniel\n24\nMale\nSeattle\n72000\n    Olivia\n30\nFemale\nBoston\n78000\n    James\n26\nMale\nDallas\n67000\n    Ava\n23\nFemale\nAtlanta\n62000\n  \n  \n  \n\n\n\n\nNote: The same read_delim() function will even work without the argument delim = \"|\" because it has in-built capacity to identify the delimiter.\n\n\nQuestion 2\nApart from file, skip, and comment, what other arguments do read_csv() and read_tsv() have in common?\nWhile read_csv() works for comma separated files, read_tsv() works for tab-separated files. The arguments for each of them are as follows:-\n\n\n\nArguments for read_csv\n\nfile,\ncol_names = TRUE,\ncol_types = NULL,\ncol_select = NULL,\nid = NULL,\nlocale = default_locale(),\nna = c(\"\", \"NA\"),\nquoted_na = TRUE,\nquote = \"\\\"\",\ncomment = \"\",\ntrim_ws = TRUE,\nskip = 0,\nn_max = Inf,\nguess_max = min(1000, n_max),\nname_repair = \"unique\",\nnum_threads = readr_threads(),\nprogress = show_progress(),\nshow_col_types = should_show_types(),\nskip_empty_rows = TRUE,\nlazy = should_read_lazy()\n\n\n\nArguments for read_tsv\n\nfile,\ncol_names = TRUE,\ncol_types = NULL,\ncol_select = NULL,\nid = NULL,\nlocale = default_locale(),\nna = c(\"\", \"NA\"),\nquoted_na = TRUE,\nquote = \"\\\"\",\ncomment = \"\",\ntrim_ws = TRUE,\nskip = 0,\nn_max = Inf,\nguess_max = min(1000, n_max),\nname_repair = \"unique\",\nnum_threads = readr_threads(),\nprogress = show_progress(),\nshow_col_types = should_show_types(),\nskip_empty_rows = TRUE,\nlazy = should_read_lazy()\n\n\n\n\nThus, all the arguments to both the functions are common, and have the exact same role.\n\n\nQuestion 3\nWhat are the most important arguments to read_fwf()?\nThe fixed width files are very fast to parse because each field will be in exact sample place in each line. However, this means, we must know the exact width of each column. Hence, the most important argument to read_fwf() is the cols_position = , which can take the following values:\n\nfwf_empty() - Guesses based on the positions of empty columns.\nfwf_widths() - Supply the widths of the columns.\nfwf_positions() - Supply paired vectors of start and end positions.\nfwf_cols() - Supply named arguments of paired start and end positions or column widths.\n\nAlso, another important argument is cols_types which will tell whether each column will be of which class - character, integer, factor etc.\nHere’s an example shown in Table 1 .\n\nimport_fwf_data = \"John    Smith   35  New York\nAlice   Johnson 28  Los Angeles\nMichael Williams 42  Chicago\"\n\ndf2 = read_fwf(\n  import_fwf_data,\n  col_positions = fwf_widths(c(8, 8, 3, 12))\n) \n\ncolnames(df2) = c(\"Name\", \"Surname\", \"Age\", \"City\")\n\ndf2 |&gt;\n  gt()\n\n\n\n\n\nTable 1:  Fixed Width File Data parsed using read_fwf \n  \n    \n    \n      Name\n      Surname\n      Age\n      City\n    \n  \n  \n    John\nSmith\n35\nNew York\n    Alice\nJohnson\n28\nLos Angeles\n    Michael\nWilliams\n42\nChicago\n  \n  \n  \n\n\n\n\n\n\n\nQuestion 4\nSometimes strings in a CSV file contain commas. To prevent them from causing problems, they need to be surrounded by a quoting character, like ” or ’. By default, read_csv() assumes that the quoting character will be “. To read the following text into a data frame, what argument to read_csv() do you need to specify?\n\"x,y\\n1,'a,b'\"\nTo read a data above text into a data-frame, we will need to used the argument quote = \"'\" . Here’s an example in Table 2 .\n\nimport_quote = \"x,y\\n1,'a,b'\"\n\nread_csv(\n  import_quote,\n  quote = \"'\",\n  col_names = FALSE\n) |&gt;\n  gt()\n\n\n\n\n\nTable 2:  A data-frame imported from csv file with different quotes \n  \n    \n    \n      X1\n      X2\n    \n  \n  \n    x\ny\n    1\na,b\n  \n  \n  \n\n\n\n\n\n\n\nQuestion 5\nIdentify what is wrong with each of the following inline CSV files. What happens when you run the code?\n\nread_csv(\"a,b\\n1,2,3\\n4,5,6\") : This data is not rectangular, there are only two columns in first row, but three in other two rows. Thus, R ends up reading only two columns by default and joins the second and third column values for the two observations.\n\nread_csv(\"a,b\\n1,2,3\\n4,5,6\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 2 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): a\nnum (1): b\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 2\n      a     b\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1    23\n2     4    56\n\n\nread_csv(\"a,b,c\\n1,2\\n1,2,3,4\"): This data is again not rectangular, there are three columns (column names) in first row, but two values in second row, and four in the third row. Thus, R ends up reading three columns by default, creates an NA and joins the second and third column values for the second row.\n\nread_csv(\"a,b,c\\n1,2\\n1,2,3,4\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 2 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): a, b\nnum (1): c\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 3\n      a     b     c\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     2    NA\n2     1     2    34\n\n\nread_csv(\"a,b\\n\\\"1\"): This data is entered wrong, as the double quotes don’t match up in pairs, i.e., there are three double quotes (\"), so R will read only the data between first two, i.e, a and b as variable names, and the data-frame will be empty. An error with also be displayed, as shown below:---\n\nread_csv(\"a,b\\n\\\"1\")\n\nRows: 0 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): a, b\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 0 × 2\n# ℹ 2 variables: a &lt;chr&gt;, b &lt;chr&gt;\n\n\nread_csv(\"a,b\\n1,2\\na,b\"): This data is rectangular, but the names of the columns are repeated in the second row, i.e. the second observation. Further, the data in each column is not of a single type, i.e. either &lt;chr&gt; or &lt;dbl&gt; or &lt;int&gt; etc. Thus, each column is not a variable, and each row is not an observation. The data is not tidy.\n\nread_csv(\"a,b\\n1,2\\na,b\")\n\nRows: 2 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): a, b\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 2\n  a     b    \n  &lt;chr&gt; &lt;chr&gt;\n1 1     2    \n2 a     b    \n\n\nread_csv(\"a;b\\n1;3\"): This data is wrong coded, i.e. it is not a comma-separated data, rather it is a semi-colon-separated data. Thus, read_csv() will end up reading a;b as a single string (i.e., column name) and the 1;3 as the single observation, i.e. first row. Instead, we should have used read_csv2() here.\n\nread_csv(\"a;b\\n1;3\")\n\nRows: 1 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): a;b\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 1 × 1\n  `a;b`\n  &lt;chr&gt;\n1 1;3  \n\n\nUsing read_csv2() , the data is read-in correctly:---\n\nread_csv2(\"a;b\\n1;3\")\n\n# A tibble: 1 × 2\n      a     b\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     3\n\n\n\n\n\nQuestion 6\nPractice referring to non-syntactic names in the following data frame by:\n\nannoying &lt;- tibble(\n  `1` = 1:10,\n  `2` = `1` * 2 + rnorm(length(`1`))\n)\nannoying\n\n# A tibble: 10 × 2\n     `1`   `2`\n   &lt;int&gt; &lt;dbl&gt;\n 1     1  4.42\n 2     2  5.82\n 3     3  6.75\n 4     4  8.48\n 5     5 10.3 \n 6     6 12.9 \n 7     7 14.4 \n 8     8 16.7 \n 9     9 16.8 \n10    10 18.6 \n\n\n\nExtracting the variable called 1.\n\nannoying |&gt;\n  pull(`1`)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nPlotting a scatterplot of 1 vs. 2.\n\nannoying |&gt;\n  ggplot(aes(x = `1`, y = `2`)) +\n  geom_point() +\n  theme_classic()\n\n\n\n\nCreating a new column called 3, which is 2 divided by 1.\n\nannoying = annoying |&gt;\n  mutate(`3` = `2` / `1`)\nannoying\n\n# A tibble: 10 × 3\n     `1`   `2`   `3`\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1  4.42  4.42\n 2     2  5.82  2.91\n 3     3  6.75  2.25\n 4     4  8.48  2.12\n 5     5 10.3   2.05\n 6     6 12.9   2.14\n 7     7 14.4   2.06\n 8     8 16.7   2.09\n 9     9 16.8   1.86\n10    10 18.6   1.86\n\n\nRenaming the columns to one, two, and three.\n\nannoying |&gt;\n  rename(\n    \"one\" = `1`,\n    \"two\" = `2`,\n    \"three\" = `3`\n  )\n\n# A tibble: 10 × 3\n     one   two three\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1  4.42  4.42\n 2     2  5.82  2.91\n 3     3  6.75  2.25\n 4     4  8.48  2.12\n 5     5 10.3   2.05\n 6     6 12.9   2.14\n 7     7 14.4   2.06\n 8     8 16.7   2.09\n 9     9 16.8   1.86\n10    10 18.6   1.86\n\n\n\n\n\n\n8.3 Controlling column types\nHere’s an example data set to use the arguments col_types and na associated with the powerful read_csv() function.\n\nraw_df1 = \"Name,Age,Value,DateTime,Flag\nJohn Doe,25,123.45,2023-08-07 10:30:00,True\nJane Smith,42,987.65,2023-08-06 15:45:00,False\nBob Johnson,32,543.21,2023-08-05 08:00:00,True\nMary Williams,28,.,2023-08-04 12:15:00,False\nMichael Brown,,789.01,2023-08-03 18:30:00,True\nEmily Davis,38,234.56,,False\nDavid Lee,50,.,2023-08-01 09:45:00,True\n.,22,345.67,2023-07-31 14:00:00,False\"\n\nread_csv(raw_df1)\n\n# A tibble: 8 × 5\n  Name            Age Value  DateTime            Flag \n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;  &lt;dttm&gt;              &lt;lgl&gt;\n1 John Doe         25 123.45 2023-08-07 10:30:00 TRUE \n2 Jane Smith       42 987.65 2023-08-06 15:45:00 FALSE\n3 Bob Johnson      32 543.21 2023-08-05 08:00:00 TRUE \n4 Mary Williams    28 .      2023-08-04 12:15:00 FALSE\n5 Michael Brown    NA 789.01 2023-08-03 18:30:00 TRUE \n6 Emily Davis      38 234.56 NA                  FALSE\n7 David Lee        50 .      2023-08-01 09:45:00 TRUE \n8 .                22 345.67 2023-07-31 14:00:00 FALSE\n\n\nHere, we see that by default, read_csv() does an amazing job. It identifies most column types, but fails to understand that “.” is a missing value in Value variable, which is otherwise numerical.\nLet’s improve this behavior.\n\nread_csv(\n  raw_df1,\n  na = \".\"\n)\n\n# A tibble: 8 × 5\n  Name            Age Value DateTime            Flag \n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt;              &lt;lgl&gt;\n1 John Doe         25  123. 2023-08-07 10:30:00 TRUE \n2 Jane Smith       42  988. 2023-08-06 15:45:00 FALSE\n3 Bob Johnson      32  543. 2023-08-05 08:00:00 TRUE \n4 Mary Williams    28   NA  2023-08-04 12:15:00 FALSE\n5 Michael Brown    NA  789. 2023-08-03 18:30:00 TRUE \n6 Emily Davis      38  235. NA                  FALSE\n7 David Lee        50   NA  2023-08-01 09:45:00 TRUE \n8 &lt;NA&gt;             22  346. 2023-07-31 14:00:00 FALSE\n\n\nNow, let’s use col_types argument to force some variables into certain classes we desire. Here I will try to force Age into an integer, Value into a number (i.e., &lt;dbl&gt;), and DateTime into a character, and Flag into a character.\n\nread_csv(\n  raw_df1,\n  na = \".\",\n  col_types = list(\n    Name = col_character(),\n    Age = col_integer(),\n    Value = col_double(),\n    DateTime = col_character()\n  )\n)\n\n# A tibble: 8 × 5\n  Name            Age Value DateTime              Flag \n  &lt;chr&gt;         &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;                 &lt;lgl&gt;\n1 John Doe         25  123. \"2023-08-07 10:30:00\" TRUE \n2 Jane Smith       42  988. \"2023-08-06 15:45:00\" FALSE\n3 Bob Johnson      32  543. \"2023-08-05 08:00:00\" TRUE \n4 Mary Williams    28   NA  \"2023-08-04 12:15:00\" FALSE\n5 Michael Brown    NA  789. \"2023-08-03 18:30:00\" TRUE \n6 Emily Davis      38  235. \"\"                    FALSE\n7 David Lee        50   NA  \"2023-08-01 09:45:00\" TRUE \n8 &lt;NA&gt;             22  346. \"2023-07-31 14:00:00\" FALSE\n\n\nNow, I will try to read-in only a few columns to save on memory space in R by using col_skip().\n\nread_csv(\n  raw_df1,\n  na = \".\",\n  col_types = list(\n    Name = col_character(),\n    Age = col_integer(),\n    Value = col_double(),\n    DateTime = col_skip(),\n    Flag = col_skip()\n  )\n)\n\n# A tibble: 8 × 3\n  Name            Age Value\n  &lt;chr&gt;         &lt;int&gt; &lt;dbl&gt;\n1 John Doe         25  123.\n2 Jane Smith       42  988.\n3 Bob Johnson      32  543.\n4 Mary Williams    28   NA \n5 Michael Brown    NA  789.\n6 Emily Davis      38  235.\n7 David Lee        50   NA \n8 &lt;NA&gt;             22  346.\n\n\nNow, I will repeat this using cols_only() function:---\n\nread_csv(\n  raw_df1,\n  na = \".\",\n  col_types = cols_only(\n    Name = col_character(), \n    Age = col_integer(), \n    Value = col_double())\n)\n\n# A tibble: 8 × 3\n  Name            Age Value\n  &lt;chr&gt;         &lt;int&gt; &lt;dbl&gt;\n1 John Doe         25  123.\n2 Jane Smith       42  988.\n3 Bob Johnson      32  543.\n4 Mary Williams    28   NA \n5 Michael Brown    NA  789.\n6 Emily Davis      38  235.\n7 David Lee        50   NA \n8 &lt;NA&gt;             22  346.\n\n\nNow, let’s try to read-in data from three different files at the same time..\n\nread_csv(\n  c(\n    \"https://pos.it/r4ds-01-sales\", \n    \"https://pos.it/r4ds-02-sales\", \n    \"https://pos.it/r4ds-03-sales\"\n  ),\n  na = \".\",\n  id = \"file\"\n) |&gt;\n  slice_head(n = 5)\n\n# A tibble: 5 × 6\n  file                         month    year brand  item     n\n  &lt;chr&gt;                        &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 https://pos.it/r4ds-01-sales January  2019     1  1234     3\n2 https://pos.it/r4ds-01-sales January  2019     1  8721     9\n3 https://pos.it/r4ds-01-sales January  2019     1  1822     2\n4 https://pos.it/r4ds-01-sales January  2019     2  3333     1\n5 https://pos.it/r4ds-01-sales January  2019     2  2156     9"
  },
  {
    "objectID": "Chapter9.html#making-a-reprex",
    "href": "Chapter9.html#making-a-reprex",
    "title": "Chapter 9",
    "section": "9.2 Making a reprex",
    "text": "9.2 Making a reprex\nHere, I try to create a reprex for a deliberate mistake I am making in ggplot2 :--\nFirst, I create the faulty ggplot2 code:--\n\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(gt)\ndata(\"gtcars\")\ngtcars |&gt;\n  slice_head(n = 3, \n             by = ctry_origin) |&gt;\n  ggplot(aes(x = hp,\n             y = mpg_h,\n             color = ctry_origin,\n             label = model)) +\n  geom_point() +\n  geom_text_repel(force = 2) +\n  theme_classic() +\n  labs(x = \"Horse Power\",\n       y = \"Miles per Gallon (Highway)\") +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n\n\n# Copy the code above\n\n# Then create reprex data set\ngtcars |&gt;\n  slice_head(n = 3, \n             by = ctry_origin) |&gt;\n  dput()\n\nHere’s the reprex:\n\ngtcars = \nstructure(list(mfr = c(\"Ford\", \"Chevrolet\", \"Dodge\", \"Ferrari\", \n\"Ferrari\", \"Ferrari\", \"Acura\", \"Nissan\", \"Bentley\", \"Aston Martin\", \n\"Aston Martin\", \"BMW\", \"BMW\", \"BMW\"), model = c(\"GT\", \"Corvette\", \n\"Viper\", \"458 Speciale\", \"458 Spider\", \"458 Italia\", \"NSX\", \"GT-R\", \n\"Continental GT\", \"DB11\", \"Rapide S\", \"6-Series\", \"i8\", \"M4\"), \n    year = c(2017, 2016, 2017, 2015, 2015, 2014, 2017, 2016, \n    2016, 2017, 2016, 2016, 2016, 2016), trim = c(\"Base Coupe\", \n    \"Z06 Coupe\", \"GT Coupe\", \"Base Coupe\", \"Base\", \"Base Coupe\", \n    \"Base Coupe\", \"Premium Coupe\", \"V8 Coupe\", \"Base Coupe\", \n    \"Base Sedan\", \"640 I Coupe\", \"Mega World Coupe\", \"Base Coupe\"\n    ), bdy_style = c(\"coupe\", \"coupe\", \"coupe\", \"coupe\", \"convertible\", \n    \"coupe\", \"coupe\", \"coupe\", \"coupe\", \"coupe\", \"sedan\", \"coupe\", \n    \"coupe\", \"coupe\"), hp = c(647, 650, 645, 597, 562, 562, 573, \n    545, 500, 608, 552, 315, 357, 425), hp_rpm = c(6250, 6400, \n    5000, 9000, 9000, 9000, 6500, 6400, 6000, 6500, 6650, 5800, \n    5800, 5500), trq = c(550, 650, 600, 398, 398, 398, 476, 436, \n    487, 516, 465, 330, 420, 406), trq_rpm = c(5900, 3600, 5000, \n    6000, 6000, 6000, 2000, 3200, 1700, 1500, 5500, 1400, 3700, \n    1850), mpg_c = c(11, 15, 12, 13, 13, 13, 21, 16, 15, 15, \n    14, 20, 28, 17), mpg_h = c(18, 22, 19, 17, 17, 17, 22, 22, \n    25, 21, 21, 30, 29, 24), drivetrain = c(\"rwd\", \"rwd\", \"rwd\", \n    \"rwd\", \"rwd\", \"rwd\", \"awd\", \"awd\", \"awd\", \"rwd\", \"rwd\", \"rwd\", \n    \"awd\", \"rwd\"), trsmn = c(\"7a\", \"7m\", \"6m\", \"7a\", \"7a\", \"7a\", \n    \"9a\", \"6a\", \"8am\", \"8am\", \"8am\", \"8am\", \"6am\", \"6m\"), ctry_origin = c(\"United States\", \n    \"United States\", \"United States\", \"Italy\", \"Italy\", \"Italy\", \n    \"Japan\", \"Japan\", \"United Kingdom\", \"United Kingdom\", \"United Kingdom\", \n    \"Germany\", \"Germany\", \"Germany\"), msrp = c(447000, 88345, \n    95895, 291744, 263553, 233509, 156000, 101770, 198500, 211195, \n    205300, 77300, 140700, 65700)), class = c(\"spec_tbl_df\", \n\"tbl_df\", \"tbl\", \"data.frame\"), row.names = c(NA, -14L), spec = structure(list(\n    cols = list(mfr = structure(list(), class = c(\"collector_character\", \n    \"collector\")), model = structure(list(), class = c(\"collector_character\", \n    \"collector\")), year = structure(list(), class = c(\"collector_double\", \n    \"collector\")), trim = structure(list(), class = c(\"collector_character\", \n    \"collector\")), bdy_style = structure(list(), class = c(\"collector_character\", \n    \"collector\")), hp = structure(list(), class = c(\"collector_double\", \n    \"collector\")), hp_rpm = structure(list(), class = c(\"collector_double\", \n    \"collector\")), trq = structure(list(), class = c(\"collector_double\", \n    \"collector\")), trq_rpm = structure(list(), class = c(\"collector_double\", \n    \"collector\")), mpg_c = structure(list(), class = c(\"collector_double\", \n    \"collector\")), mpg_h = structure(list(), class = c(\"collector_double\", \n    \"collector\")), drivetrain = structure(list(), class = c(\"collector_character\", \n    \"collector\")), trsmn = structure(list(), class = c(\"collector_character\", \n    \"collector\")), ctry_origin = structure(list(), class = c(\"collector_character\", \n    \"collector\")), msrp = structure(list(), class = c(\"collector_double\", \n    \"collector\"))), default = structure(list(), class = c(\"collector_guess\", \n    \"collector\")), skip = 1), class = \"col_spec\"))\n\nreprex::reprex()\n\n``` r\nlibrary(tidyverse)\nlibrary(ggrepel)\n\ngtcars |&gt;\n  slice_head(n = 3, \n             by = ctry_origin) |&gt;\n  ggplot(aes(x = hp,\n             y = mpg_h,\n             color = ctry_origin,\n             label = model)) +\n  geom_point() +\n  geom_text_repel(force = 2) +\n  theme_classic() +\n  labs(x = \"Horse Power\",\n       y = \"Miles per Gallon (Highway)\") +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n\n```\n\n&lt;sup&gt;Created on 2023-08-13 with [reprex v2.0.2](https://reprex.tidyverse.org)&lt;/sup&gt;"
  },
  {
    "objectID": "Chapter21.html",
    "href": "Chapter21.html",
    "title": "Chapter 21",
    "section": "",
    "text": "21.2.9 Exercises\nThe two main packages for reading data from and writing data to excel spreadsheets are readxl and writexl . But they are not core-tidyverse, so let us load them first.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(writexl)\n\nThe few important functions we will use are:\n\nread_excel()\nexcel_sheets()\n\n\nQuestion 1.\nIn an Excel file, create the following dataset and save it as survey.xlsx. Alternatively, you can download it as an Excel file from here. Then, read it into R, with survey_id as a character variable and n_pets as a numerical variable.\n\nsurvey_df = tibble(\n  survey_id = c(1:6),\n  n_pets = c(0,1,\"N/A\", \"two\", 2, \"\")\n)\n\nsurvey_df |&gt;\n  write_xlsx(\"docs/survey.xlsx\")\n\ndf = read_excel(\n  path = \"docs/survey.xlsx\",\n  col_names = TRUE,\n  col_types = c(\"text\", \"text\"),\n  na = c(\"N/A\", \"\")\n  ) |&gt;\n  mutate(\n    n_pets = ifelse(n_pets == \"two\", 2, n_pets),\n    n_pets = parse_number(n_pets)\n    )\n\ndf\n\n# A tibble: 6 × 2\n  survey_id n_pets\n  &lt;chr&gt;      &lt;dbl&gt;\n1 1              0\n2 2              1\n3 3             NA\n4 4              2\n5 5              2\n6 6             NA\n\n\n\n\nQuestion 2.\nIn another Excel file, create the following data-set and save it as roster.xlsx. Alternatively, you can download it as an Excel file from here. Then, read it into R. The resulting data frame should be called roster and should look like the following.\nThere are two ways of doing this:\n\nUsing the read_excel() function with fill() function of the tidyr package.\nUsing the package openxlsx and its function read.xlsx() which has an argument fillMergedCells = TRUE to do the same task in one go. However, the output is a data.frame, which we must then convert to a tibble.\n\n\n# Use readxl package with fill() from tidyr\nread_excel(\n  path = \"docs/roster.xlsx\"\n  ) |&gt;\n  fill(group, subgroup)\n\n# A tibble: 12 × 3\n   group subgroup    id\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1     1 A            1\n 2     1 A            2\n 3     1 A            3\n 4     1 B            4\n 5     1 B            5\n 6     1 B            6\n 7     1 B            7\n 8     2 A            8\n 9     2 A            9\n10     2 B           10\n11     2 B           11\n12     2 B           12\n\n# Option 2: using the openxlsx package\nlibrary(openxlsx)\nread.xlsx(\n  xlsxFile = \"docs/roster.xlsx\",\n  fillMergedCells = TRUE\n  ) |&gt;\n  as_tibble()\n\n# A tibble: 12 × 3\n   group subgroup    id\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1     1 A            1\n 2     1 A            2\n 3     1 A            3\n 4     1 B            4\n 5     1 B            5\n 6     1 B            6\n 7     1 B            7\n 8     2 A            8\n 9     2 A            9\n10     2 B           10\n11     2 B           11\n12     2 B           12\n\n\n\n\nQuestion 3.\nIn a new Excel file, create the following dataset and save it as sales.xlsx. Alternatively, you can download it as an Excel file from here.\n\nRead sales.xlsx in and save as sales. The data frame should look like the following, with id and n as column names and with 9 rows.\n\nsales = read_excel(\n  \"docs/sales.xlsx\",\n  skip = 4,\n  col_names = c(\"id\", \"n\")\n)\nsales\n\n# A tibble: 9 × 2\n  id      n    \n  &lt;chr&gt;   &lt;chr&gt;\n1 Brand 1 n    \n2 1234.0  8.0  \n3 8721.0  2.0  \n4 1822.0  3.0  \n5 Brand 2 n    \n6 3333.0  1.0  \n7 2156.0  3.0  \n8 3987.0  6.0  \n9 3216.0  5.0  \n\n\nModify sales further to get it into the following tidy format with three columns (brand, id, and n) and 7 rows of data. Note that id and n are numeric, brand is a character variable.\n\nsales |&gt;\n  mutate(\n    brand = ifelse(str_detect(id, \"Brand\"), id, NA),\n    id = parse_number(id),\n    n = parse_number(n, na = \"n\")) |&gt;\n  fill(brand) |&gt;\n  drop_na() |&gt;\n  relocate(brand)\n\n# A tibble: 7 × 3\n  brand      id     n\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Brand 1  1234     8\n2 Brand 1  8721     2\n3 Brand 1  1822     3\n4 Brand 2  3333     1\n5 Brand 2  2156     3\n6 Brand 2  3987     6\n7 Brand 2  3216     5\n\n\n\n\n\nQuestion 4.\nRecreate the bake_sale data frame, write it out to an Excel file using the write.xlsx() function from the openxlsx package.\n\nbake_sale = tibble(\n  item = factor(c(\"brownie\", \"cupcake\", \"cookie\")),\n  quantity = c(10, 5, 8)\n)\nbake_sale |&gt;\n  write.xlsx(\"docs/bake_sale.xlsx\")\n\n\n\nQuestion 5.\nIn Chapter 8 you learned about the janitor::clean_names() function to turn columns names into snake case. Read the students.xlsx file that we introduced earlier in this section and use this function to “clean” the column names.\n\n# Option 1: Read in data from the google sheets\n# library(googlesheets4)\n# id = \"1V1nPp1tzOuutXFLb3G9Eyxi3qxeEhnOXUzL5_BcCQ0w\"\n\n# For an easy reprex, write in the data now, suing:--\n# read_sheet(id) |&gt;\n#   dput()\n\nraw_data = structure(list(`Student ID` = c(1, 2, 3, 4, 5, 6), `Full Name` = c(\"Sunil Huffmann\", \"Barclay Lynn\", \"Jayendra Lyne\", \"Leon Rossini\", \"Chidiegwu Dunkel\",\"Güvenç Attila\"), favourite.food = c(\"Strawberry yoghurt\", \"French fries\", \"N/A\", \"Anchovies\", \"Pizza\", \"Ice cream\"), mealPlan = c(\"Lunch only\", \"Lunch only\", \"Breakfast and lunch\", \"Lunch only\", \"Breakfast and lunch\", \"Lunch only\"), AGE = list(4, 5, 7, NULL, \"five\", 6)), class = c(\"tbl_df\", \"tbl\", \"data.frame\"), row.names = c(NA, -6L))\n\nraw_data |&gt;\n  janitor::clean_names() |&gt;\n  as_tibble()\n\n# A tibble: 6 × 5\n  student_id full_name        favourite_food     meal_plan           age      \n       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;list&gt;   \n1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          &lt;dbl [1]&gt;\n2          2 Barclay Lynn     French fries       Lunch only          &lt;dbl [1]&gt;\n3          3 Jayendra Lyne    N/A                Breakfast and lunch &lt;dbl [1]&gt;\n4          4 Leon Rossini     Anchovies          Lunch only          &lt;NULL&gt;   \n5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch &lt;chr [1]&gt;\n6          6 Güvenç Attila    Ice cream          Lunch only          &lt;dbl [1]&gt;\n\n\n\n\nQuestion 6.\nWhat happens if you try to read in a file with .xlsx extension with read_xls()?\nIf we try to open a *.xlsx file with read_xls() , an error is displayed that Error: filepath libxls error: Unable to open file\n\nread_xls(\"docs/sales.xlsx\")\n\n\n\n\n21.3.6 Exercises\n\nQuestion 1.\nRead the students data set from earlier in the chapter from Excel and also from Google Sheets, with no additional arguments supplied to the read_excel() and read_sheet() functions. Are the resulting data frames in R exactly the same? If not, how are they different?\nThe two resulting data frames are not exactly the same. The data frame created from read_excel() , i.e. df_xl has the has variable AGE saved as character because one of the values is written in characters, instead of a number. Whenever some data is numeric and some data is character , read_excel() converts all data within a column into character format.\nOn the other hand, the data frame created from read_sheet() of googlesheets4 package, i.e. df_gs has this variable stored as a “list”, which contains both numeric and character types of data.\n\ndf_xl = read_excel(\"docs/students.xlsx\")\n\nlibrary(googlesheets4)\nurl_id = \"1V1nPp1tzOuutXFLb3G9Eyxi3qxeEhnOXUzL5_BcCQ0w\"\n\ndf_gs = read_sheet(url_id)\n# Comparing the types of columns in the two data.frames\nsapply(df_xl, class) == sapply(df_gs, class)\n\n    Student ID      Full Name favourite.food       mealPlan            AGE \n          TRUE           TRUE           TRUE           TRUE          FALSE \n\nclass(df_xl$AGE)\n\n[1] \"character\"\n\nclass(df_gs$AGE)\n\n[1] \"list\"\n\nsapply(df_gs$AGE, class)\n\n[1] \"numeric\"   \"numeric\"   \"numeric\"   \"NULL\"      \"character\" \"numeric\"  \n\n\n\n\nQuestion 2.\nRead the Google Sheet titled survey from https://pos.it/r4ds-survey, with survey_id as a character variable and n_pets as a numerical variable.\nWhen we read Google sheets, using col_types argument, we introduce NAs by coercion.\n\nurl_gs = \"https://docs.google.com/spreadsheets/d/1yc5gL-a2OOBr8M7B3IsDNX5uR17vBHOyWZq6xSTG2G8/edit#gid=0\"\n\nread_sheet(\n  ss = url_gs,\n  col_types = \"cd\")\n\n# A tibble: 6 × 2\n  survey_id n_pets\n  &lt;chr&gt;      &lt;dbl&gt;\n1 1              0\n2 2              1\n3 3             NA\n4 4             NA\n5 5              2\n6 6             NA\n\n\n\n\nQuestion 3.\nRead the Google Sheet titled roster from https://pos.it/r4ds-roster. The resulting data frame should be called roster and should look like the following.\n\nurl_gs1 = \"https://docs.google.com/spreadsheets/d/1LgZ0Bkg9d_NK8uTdP2uHXm07kAlwx8-Ictf8NocebIE/edit#gid=0\"\n\nread_sheet(\n  ss = url_gs1\n) |&gt;\nfill(group, subgroup)  \n\n# A tibble: 12 × 3\n   group subgroup    id\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1     1 A            1\n 2     1 A            2\n 3     1 A            3\n 4     1 B            4\n 5     1 B            5\n 6     1 B            6\n 7     1 B            7\n 8     2 A            8\n 9     2 A            9\n10     2 B           10\n11     2 B           11\n12     2 B           12"
  },
  {
    "objectID": "Chapter9.html",
    "href": "Chapter9.html",
    "title": "Chapter 9",
    "section": "",
    "text": "We can copy any error; paste and search it on google.com"
  },
  {
    "objectID": "Chapter9.html#google-is-your-friend",
    "href": "Chapter9.html#google-is-your-friend",
    "title": "Chapter 9",
    "section": "",
    "text": "We can copy any error; paste and search it on google.com"
  },
  {
    "objectID": "Chapter10.html",
    "href": "Chapter10.html",
    "title": "Chapter 10",
    "section": "",
    "text": "“The greatest value of a picture is when it forces us to notice what we never expected to see.” — John Tukey\n\n\nggplot2 will use maximum 6 shapes at a time. The 7th shape is treated as a missing value.\nUsing alpha aesthetic for a discrete variable is not advised.\nThe shapes used in ggplot2 are as follows(Wickham 2016) :--\n\n\n\nShapes available to use in ggplot2.\n\n\nThe best place to explore ggplot2 extensions and graphs is the ggplot2 extensions gallery.\nBest place to search for and understand the geoms within ggplot2 is ggplot2 Function Reference.\n\n\nlibrary(tidyverse)\nlibrary(gt)\ndata(\"mpg\")\ndata(\"diamonds\")"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nHarvard University | Boston, MA MPH in Global Health | Aug 2021 - May 2022\nAll India Institute of Medical Sciences | New Delhi, India MBBS in Medicine | Aug 2005-Dec 2010\nIndira Gandhi National Open University | New Delhi, India MA in Public Policy | Jan 2012 - Dec 2013"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nIndian Administrative Service | Director | Aug 2011 - present\nNeuro-Radiology, AIIMS New Delhi | Junior Resident Doctor | Jan 2011 - Aug 2011"
  },
  {
    "objectID": "Chapter10.html#footnotes",
    "href": "Chapter10.html#footnotes",
    "title": "Chapter 10",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWritten with help from ChatGPT 3.5. OpenAI. (2023). ChatGPT (Aug 22, 2023 version) [Large language model]. https://chat.openai.com↩︎\nThis portion of answer was created using help from ChatGPT 3.5. OpenAI. (2023). ChatGPT (Aug 22, 2023 version) [Large language model]. https://chat.openai.com↩︎\nggplot2, Map projections. https://ggplot2.tidyverse.org/reference/coord_map.html↩︎"
  },
  {
    "objectID": "Chatper11.html",
    "href": "Chatper11.html",
    "title": "Chapter 11",
    "section": "",
    "text": "library(tidyverse)\nlibrary(gt)\n\n\n11.3.3 Exercises\n\nQuestion 1\nExplore the distribution of each of the x, y, and z variables in diamonds. What do you learn? Think about a diamond and how you might decide which dimension is the length, width, and depth.\nUpon exploratory data analysis (code shown below), I learn the following insights: —\n\nThere are outliers in distribution of x , there are eight diamonds with zero value of x, but no outliers on higher side.\nThere are outliers in distribution of y , there are eight diamonds with zero value of y, and 2 outliers on higher side.\nThere are outliers in distribution of z , there are 20 diamonds with zero value of z, and 1 outlier on higher side.\nThe correlation between the variables show that x , y , and z are strongly positively correlated amongst themselves and with the weight (carat).\nThe mean values of x , y and z are 5.73, 5.73 and 3.54. Thus, it is possible that x and y represent either of length and width, while z represents depth.\nNow, upon visualizing the density plots of x , y and z , we see that x and y are similar distributed so, they must be length and breadth, but z is smaller in value. So, z must be depth.\n\n\n\nCode\ndata(\"diamonds\")\ndiamonds |&gt;\n  ggplot(aes(x = x,\n             fill = (x ==  0 | x &gt; 12))) +\n  geom_histogram(binwidth = 0.1) +\n  coord_cartesian(ylim = c(0,10))\n\n\ndiamonds |&gt;\n  ggplot(aes(x = y,\n             fill = (y ==  0 | y &gt; 12))) +\n  geom_histogram(binwidth = 0.1) +\n  coord_cartesian(ylim = c(0,10))\n\ndiamonds |&gt;\n  ggplot(aes(x = z,\n             fill = (z ==  0 | z &gt; 12))) +\n  geom_histogram(binwidth = 0.1) +\n  coord_cartesian(ylim = c(0,20))\n\ndiamonds |&gt;\n  summarize(x = mean(x),\n            y = mean(y),\n            z = mean(z))\n\ndiamonds |&gt;\n  filter(x == 0 | z == 0 | y == 0)\n\ndiamonds |&gt;\n  select(x, y, z) |&gt;\n  pivot_longer(cols = everything(),\n               names_to = \"dimension\",\n               values_to = \"value\") |&gt;\n  ggplot() +\n  geom_density(aes(x = value,\n                   col = dimension)) +\n  theme_classic() +\n  theme(legend.position = \"bottom\") +\n  coord_cartesian(xlim = c(0, 10))\n\n\n\n\nQuestion 2\nExplore the distribution of price. Do you discover anything unusual or surprising? (Hint: Carefully think about the binwidth and make sure you try a wide range of values.)\nThe distribution of price shows a surprising fact in Figure 1 that there are no diamonds priced between $1,450 and $1,550.\n\ndiamonds |&gt;\n  ggplot(aes(price)) +\n  geom_histogram(binwidth = 10, \n                 fill = \"lightgrey\", \n                 color = \"darkgrey\") + \n  coord_cartesian(xlim = c(500, 2000)) + \n  scale_x_continuous(breaks = seq(from = 500, to = 2000, by = 100)) +\n  theme_minimal()\n\n\n\n\nFigure 1: The hisotgram of diamonds’ prices, focussed in area around $1500 price tag\n\n\n\n\n\n\nQuestion 3\nHow many diamonds are 0.99 carat? How many are 1 carat? What do you think is the cause of the difference?\nThere are only 23 diamonds of 0.99 carat, but 1,558 diamonds of 1 carat.\nThe possible cause of this difference is that the weight recorder or measurement instrument rounded off to the nearest integer, especially if carat was 0.99.\n\ndiamonds |&gt;\n  select(carat) |&gt;\n  filter(carat == 0.99 | carat == 1) |&gt;\n  group_by(carat) |&gt;\n  count()\n\n# A tibble: 2 × 2\n# Groups:   carat [2]\n  carat     n\n  &lt;dbl&gt; &lt;int&gt;\n1  0.99    23\n2  1     1558\n\n\n\n\nQuestion 4\nCompare and contrast coord_cartesian() vs. xlim() or ylim() when zooming in on a histogram. What happens if you leave binwidth unset? What happens if you try and zoom so only half a bar shows?\nBoth coord_cartesian() and xlim() / ylim() serve a similar purpose of adjusting the visible range of data in a plot, but they do so in slightly different ways. The Figure 2 shows the difference.\n\nxlim() and ylim() are functions in R that directly modify the data range that is displayed on the x-axis and y-axis, respectively. The xlim() / ylim() replace all values outside the range into NAs . They remove the data outside the limits. They can be used to zoom in on specific portions of the plot.\n\nPros:\n\nCan help emphasize specific details or patterns in the data after removal of outliers.\n\nCons:\n\nData points outside the specified range are removed from the plot, potentially leading to a loss of context.\nIf used improperly, it can distort the visual representation of the data, making it appear more or less significant than it actually is.\n\n\n\ncoord_cartesian() allows us to adjust the visible range of data without altering the underlying data.\n\nPros:\n\nIt does not remove any data points from the plot; it only changes the visible range.\nUseful when you want to focus on a specific part of the plot while still having access to the full data context.\n\nCons:\n\nIf there are outliers or extreme values, they might still affect the appearance of the plot.\n\n\nComparison\n\n\n\n\n\n\n\n\nAspect\ncoord_cartesian()\nxlim()/ylim()\n\n\n\n\nPurpose\nAdjust visible range without altering data\nSet specific data range to be displayed, removes data outside the range\n\n\nData Integrity\nMaintains original data and scaling\nCan exclude data points outside range\n\n\nContext\nPreserves overall data context\nMay lose context due to excluded data; or reveal new insights upon removal of outliers.\n\n\nImpact on Plot\nAdjusts only the visible area\nAlters axes scaling and data representation\n\n\nHandling outliers\nKeeps outliers within context\nRemove outliers outside the specified range\n\n\nControl over Range\nLimited control over axes scaling\nPrecise control over displayed range\n\n\nSuitability for Histograms\nRecommended for maintaining bin sizes\nCan distort histogram representation\n\n\n\n\ngridExtra::grid.arrange(\ndiamonds |&gt;\n  ggplot(aes(x = y)) +\n  geom_histogram(binwidth = 0.1) + \n  ylim(0, 1000) +\n  xlim(0, 10) +\n  labs(subtitle = \"xlim and ylim remove data outside the limits, \\neg. counts &gt; 1000; or the observation at zero\"),\n\ndiamonds |&gt;\n  ggplot(aes(x = y)) +\n  geom_histogram(binwidth = 0.1) + \n  coord_cartesian(ylim = c(0, 1000),\n                  xlim = c(0, 10)) +\n  labs(subtitle = \"coord_cartesian preserves data outside the limits, \\neg. counts &gt; 1000; or the observation at zero\"),\n\nncol = 2)\n\n\n\n\nFigure 2: Difference between coord_cartesian() and xlim()/ylim()\n\n\n\n\n\n\n\n11.4.1 Exercises\n\nQuestion 1\nWhat happens to missing values in a histogram? What happens to missing values in a bar chart? Why is there a difference in how missing values are handled in histograms and bar charts?\nIn a histogram, missing values are typically ignored. If there are missing values in your data, they won’t be placed into any bin and won’t contribute to the creation of bars in the histogram. Thus, histogram only shows the distribution of the non-missing values.\nIn a bar chart, which is used to display categorical data, missing values are treated as a distinct category. When you create a bar chart using ggplot2, each unique category in your data is represented by a bar. If there are missing values, ggplot2 will include a separate bar to represent the missing category, often labeled as “NA” or “Missing”.\nThe difference in how missing values are handled in histograms and bar charts arises from their underlying purposes:\n\nHistograms are primarily used to visualize the distribution of continuous or numeric data. Since missing values don’t have a specific numeric value to be placed into bins, it’s common practice to exclude them.\nBar charts, on the other hand, are used to compare the frequency or count of different categories. Missing values are treated as a category themselves.\n\nIn summary, the distinction in handling missing values is based on the type of data being visualized and the purpose of each plot. Histograms focus on the distribution of non-missing numeric data, while bar charts emphasize the comparison of categorical data, including missing values as a separate category.\n\n# Set a random seed for reproducibility\nset.seed(123)\n\n# Create a sample dataset with missing values\nn = 200\ndf = data.frame(\n  Category = sample(x = c(\"A\", \"B\", \"C\", \"D\"), \n                    size = n, \n                    replace = TRUE),\n  Value = rnorm(n)\n)\n\n# Introduce missing values\ndf$Value[sample(1:n, 40)] = NA\ndf$Category[sample(1:n, 40)] = NA\n\n# Create plots to demonstrate\ngridExtra::grid.arrange(\n  ggplot(df, aes(x = Value)) +\n    geom_histogram(col = \"grey\", fill = \"lightgrey\") +\n    theme_minimal() +\n    labs(subtitle = \"Histogram drops the missing values\"),\n\n  ggplot(df, aes(x = Category)) +\n    geom_bar(col = \"grey\", fill = \"lightgrey\") + \n    theme_minimal() +\n    labs(subtitle = \"Bar Chart includes missing values as a category\"),\n  \n  ncol = 2)\n\n\n\n\n\n\nQuestion 2\nWhat does na.rm = TRUE do in mean() and sum()?\nWhen na.rm is set to TRUE, the function will remove any NA values from the input vector before performing the calculation. This means that the resulting mean or sum will only consider the non-missing values.\nThis is important because in R , NAs cannot be added or subtrated or operated upon, for example, NA + 1 = NA. Thus, even if one observation is NA, the mean or sum of the entire vector will be NA . Hence, using na.rm = TRUE is important.\n\nmean(df$Value)\n\n[1] NA\n\nmean(df$Value, na.rm = TRUE)\n\n[1] 0.006409991\n\nsum(df$Value)\n\n[1] NA\n\nsum(df$Value, na.rm = TRUE)\n\n[1] 1.025599\n\n\n\n\nQuestion 3\nRecreate the frequency plot of scheduled_dep_time colored by whether the flight was cancelled or not. Also facet by the cancelled variable. Experiment with different values of the scales variable in the faceting function to mitigate the effect of more non-cancelled flights than cancelled flights.\nThe best value of scales to use is scales = \"free_y\" so that the two facets’ y-axis are completely free and we can compare the distribution of cancelled flights vs. non-cancelled flights in Figure 3.\n\nnycflights13::flights |&gt; \n  mutate(\n    cancelled = is.na(dep_time),\n    sched_hour = sched_dep_time %/% 100,\n    sched_min = sched_dep_time %% 100,\n    sched_dep_time = sched_hour + (sched_min / 60)\n  ) |&gt;\n  # Create nice names for \"cancelled\" to show in the eventual plot\n  mutate(cancelled = as_factor(ifelse(cancelled, \n                                      \"Cancelled Flights\",\n                                      \"Flights Not Cancelled\"))) |&gt;\n  ggplot(aes(x = sched_dep_time)) +\n  geom_freqpoly(lwd = 1) +\n  theme_minimal() + \n  facet_wrap(~cancelled, \n             scales = \"free_y\") +\n  labs(x = \"Scheduled Departure Time (in hrs)\",\n       y = \"Number of flights\") +\n  scale_x_continuous(breaks = seq(0, 24, 4))\n\n\n\n\nFigure 3: Comparison of cancelled vs. non-cancelled flights by faceting\n\n\n\n\n\n\n\n11.5.1.1 Exercises\n\nQuestion 1\nUse what you’ve learned to improve the visualization of the departure times of cancelled vs. non-cancelled flights.\nThe Figure 4 shows an example to demonstrate exploratory data analysis in missing values in data-set flights of the package nycflights13 . It shows that as the day progresses, more flights get cancelled. Evening flights are more likely to get cancelled than morning flights.\n\nnycflights13::flights |&gt; \n  mutate(\n    cancelled = is.na(dep_time),\n    sched_hour = sched_dep_time %/% 100,\n    sched_min = sched_dep_time %% 100,\n    sched_dep_time = sched_hour + (sched_min / 60)\n  ) |&gt;\n  ggplot(aes(x = sched_dep_time,\n             y = after_stat(density))) +\n  geom_freqpoly(aes(col = cancelled),\n                lwd = 1) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"Scheduled Departure Time (in hrs)\",\n       y = \"Proportion of flights departed\",\n       color = \"Whether the flight was cancelled?\",\n       subtitle = \"Comparison of density frequency polygons of cancelled vs. non-cancelled flights\") +\n  scale_x_continuous(breaks = seq(0,24, by = 2))\n\n\n\n\nFigure 4: Visualizing departure times of cancelled vs. non-cancelled flights\n\n\n\n\nAnother method to visualize it is shown using stat = \"density\" argument in the geom_freqpoly() in Figure 5 below.\n\n\nCode\nnycflights13::flights |&gt; \n  mutate(\n    cancelled = is.na(dep_time),\n    sched_hour = sched_dep_time %/% 100,\n    sched_min = sched_dep_time %% 100,\n    sched_dep_time = sched_hour + (sched_min / 60)\n  ) |&gt;\n  ggplot(aes(x = sched_dep_time)) +\n  geom_freqpoly(aes(col = cancelled),\n                stat = \"density\",\n                lwd = 1) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"Scheduled Departure Time (in hrs)\",\n       y = \"Proportion of flights departed\",\n       color = \"Whether the flight was cancelled?\",\n       subtitle = \"Comparison of density frequency polygons of cancelled vs. non-cancelled flights\") +\n  scale_x_continuous(breaks = seq(0,24, by = 2))\n\n\n\n\n\nFigure 5: Another method of visualizing departure times of cancelled vs. non-cancelled flights\n\n\n\n\nLastly, we can also work on the data set, compute the percentage of flights that are cancelled within each hour and plot the percentage as shown in Figure 6 .\n\n\nCode\nnycflights13::flights |&gt; \n  mutate(\n    cancelled = is.na(dep_time),\n    sched_hour = sched_dep_time %/% 100\n  ) |&gt;\n  group_by(sched_hour) |&gt;\n  summarise(\n    cancelled = sum(cancelled),\n    total = n()\n  ) |&gt;\n  mutate (prop_cancelled = cancelled/total) |&gt;\n  ggplot(aes(x = sched_hour,\n             y = prop_cancelled*100)) +\n  geom_line() +\n  geom_point() +\n  xlim(4,24) +\n  ylim(0, 5) +\n  scale_x_continuous(breaks = seq(4, 24, 2)) +\n  coord_cartesian(xlim = c(4, 24)) +\n  labs(x = \"Scheduled Departure Time (in hrs)\",\n       y = \"Percentage of flights that were cancelled\",\n       subtitle = \"Percentage of cancelled flights over different scheduled departure times\") +\n  theme_minimal()\n\n\n\n\n\nFigure 6: Percentage of flights cancelled each hour\n\n\n\n\n\n\nQuestion 2\nBased on EDA, what variable in the diamonds dataset appears to be most important for predicting the price of a diamond? How is that variable correlated with cut? Why does the combination of those two relationships lead to lower quality diamonds being more expensive?\n\n\nQuestion 3\nInstead of exchanging the x and y variables, add coord_flip() as a new layer to the vertical boxplot to create a horizontal one. How does this compare to exchanging the variables?\n\n\nQuestion 4\nOne problem with boxplots is that they were developed in an era of much smaller datasets and tend to display a prohibitively large number of “outlying values”. One approach to remedy this problem is the letter value plot. Install the lvplot package, and try using geom_lv() to display the distribution of price vs. cut. What do you learn? How do you interpret the plots?\n\n\nQuestion 5\nCreate a visualization of diamond prices vs. a categorical variable from the diamonds data-set using geom_violin(), then a faceted geom_histogram(), then a colored geom_freqpoly(), and then a colored geom_density(). Compare and contrast the four plots. What are the pros and cons of each method of visualizing the distribution of a numerical variable based on the levels of a categorical variable?\n\n\nQuestion 6\nIf you have a small data-set, it’s sometimes useful to use geom_jitter() to avoid overplotting to more easily see the relationship between a continuous and categorical variable. The ggbeeswarm package provides a number of methods similar to geom_jitter(). List them and briefly describe what each one does."
  },
  {
    "objectID": "Chapter11.html",
    "href": "Chapter11.html",
    "title": "Chapter 11",
    "section": "",
    "text": "library(tidyverse)\nlibrary(gt)\nlibrary(RColorBrewer)\ndata(\"diamonds\")"
  },
  {
    "objectID": "Chapter11.html#footnotes",
    "href": "Chapter11.html#footnotes",
    "title": "Chapter 11",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis my my interpretation, and needs confirmation. Please comment or leave a pull request / issue on GitHub.↩︎"
  },
  {
    "objectID": "Chapter12.html",
    "href": "Chapter12.html",
    "title": "Chapter 12",
    "section": "",
    "text": "Important lessons from R for Data Science 2nd Edition, Chapter 12: –\nlibrary(tidyverse)   # the tidyverse\nlibrary(scales)      # to adjust display of numbers\nlibrary(ggrepel)     # to clearly position text labels\nlibrary(patchwork)   # to display multiple plots\nlibrary(gt)          # to display beautiful tables in Quarto"
  },
  {
    "objectID": "Chapter12.html#footnotes",
    "href": "Chapter12.html#footnotes",
    "title": "Chapter 12",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\nNote: The BODMAS rules (also known as PEMDAS or PEDMAS in different regions) are a set of rules that dictate the order of operations to be followed when evaluating mathematical expressions. These rules ensure that mathematical expressions are evaluated consistently and accurately. BODMAS stands for:\n\nBrackets: Evaluate expressions inside brackets first.\nOrders: Evaluate exponential or power operations.\nDivision and Multiplication: Perform division and multiplication.\nAddition and Subtraction: Perform addition and subtraction operations from left to right.\n\n\n↩︎"
  },
  {
    "objectID": "Chapter13.html",
    "href": "Chapter13.html",
    "title": "Chapter 13",
    "section": "",
    "text": "Some important take-away points\nlibrary(tidyverse)\nlibrary(nycflights13)\ndata(\"flights\")\nlibrary(gt)"
  },
  {
    "objectID": "Chapter13.html#question-1",
    "href": "Chapter13.html#question-1",
    "title": "Chapter 13",
    "section": "Question 1",
    "text": "Question 1\nHow does dplyr::near() work? Type near to see the source code. Is sqrt(2)^2 near 2?\ndplyr::near() is used for testing whether two numeric values are close to each other within a certain tolerance. This is useful when working with floating-point numbers, where exact equality can be problematic due to precision limitations. It takes three arguments:\n\nx: The first numeric value to compare.\ny: The second numeric value to compare.\ntol: Tolerance level, which is a small positive numeric value that defines how close x and y need to be to be considered “near.” By default, it uses .Machine$double.eps^0.5, which is a good default for most cases.\n\nNote 1: The variable .Machine contains data concerning the numerical attributes of the computer system where R is currently operating. This information encompasses details like the maximum values for double, and integer types, as well as the machine’s precision.\nNote 2: The double.eps value represents the smallest positive floating-point number x for which the equation 1 + x != 1 holds true. Its calculation involves the base of the double data type and the number of significant digits (ulp.digits). Specifically, if the base is 2 or the rounding method is 0, it equals double.base ^ ulp.digits. In other cases, it is (double.base ^ double.ulp.digits) / 2. Typically, this value is approximately 2.220446e-16.\n\nnear\n## function (x, y, tol = .Machine$double.eps^0.5) \n## {\n##     abs(x - y) &lt; tol\n## }\n## &lt;bytecode: 0x000001b2a68444a8&gt;\n## &lt;environment: namespace:dplyr&gt;\n.Machine$double.eps\n## [1] 2.220446e-16\n.Machine$double.eps^0.5\n## [1] 1.490116e-08\n\nThus, dplyr::near() works by checking whether the absolute value of the difference between x and y is less that the square-root of .Machine$double.eps or not.\nAs shown in the code below, yes, sqrt(2)^2 is near 2.\n\nnear(sqrt(2)^2, 2)\n## [1] TRUE"
  },
  {
    "objectID": "Chapter13.html#question-2",
    "href": "Chapter13.html#question-2",
    "title": "Chapter 13",
    "section": "Question 2",
    "text": "Question 2\nUse mutate(), is.na(), and count() together to describe how the missing values in dep_time, sched_dep_time and dep_delay are connected.\nFirst, let us try to compute the number of rows where, as it should be, sched_dep_time - dep_time == dep_delay. As we see below, the results is NA since NAs are contagious in addition, the result returns NA .\n\nflights |&gt;\n  select(dep_time, sched_dep_time, dep_delay) |&gt;\n  mutate(check1 = sched_dep_time - dep_time == dep_delay) |&gt;\n  summarise(\n    n = n(),\n    equal = sum(check1)\n  )\n\n# A tibble: 1 × 2\n       n equal\n   &lt;int&gt; &lt;int&gt;\n1 336776    NA\n\n\nNow, let us rework the maths with using is.na() to remove missing values of departure time, i.e. cancelled flights. We can use filter(!is.na(dep_time)) . The results indicate that 69.% of flights, the departure delay is equal to difference between departure time and scheduled departure time.\n\nflights |&gt;\n  select(dep_time, sched_dep_time, dep_delay) |&gt;\n  filter(!is.na(dep_time)) |&gt;\n  mutate(check1 = dep_time - sched_dep_time == dep_delay) |&gt;\n  summarise(\n    total = n(),\n    equal = sum(check1)\n  ) |&gt;\n  mutate(perc_equal = (equal*100)/total)\n\n# A tibble: 1 × 3\n   total  equal perc_equal\n   &lt;int&gt;  &lt;int&gt;      &lt;dbl&gt;\n1 328521 228744       69.6\n\n\nNow, onto checking the relation between missing values. We observe that none of the Scheduled Departure Time values are missing. There are 8,255 missing Departure Time values, which indicate a cancelled flight. There are also 8,255 missing Departure Delay values, and we show below that these are the exact same flights for which the departure time is missing. Thus, the missing values in dep_delay and dep_time are connected and exaclty occurring for same rows.\n\n# Number of row with missing Scheduled Departure Time\nflights |&gt;\n  filter(is.na(sched_dep_time)) |&gt;\n  count() |&gt;\n  as.numeric()\n\n[1] 0\n\n# The number of rows with missing Departure Time\nflights |&gt;\n  filter(is.na(dep_time)) |&gt;\n  count() |&gt;\n  as.numeric()\n\n[1] 8255\n\n# The number of rows with missing Departure Delay\nflights |&gt;\n  filter(is.na(dep_delay)) |&gt;\n  count() |&gt;\n  as.numeric()\n\n[1] 8255\n\n# Checking whether the exact same rows have missing values\n# for Departure Time and Departure Delay\nsum(which(is.na(flights$dep_time)) != which(is.na(flights$dep_delay)))\n\n[1] 0"
  },
  {
    "objectID": "Chapter13.html#question-1-1",
    "href": "Chapter13.html#question-1-1",
    "title": "Chapter 13",
    "section": "Question 1",
    "text": "Question 1\nFind all flights where arr_delay is missing but dep_delay is not. Find all flights where neither arr_time nor sched_arr_time are missing, but arr_delay is.\nThe following code displays all flights where arr_delay is missing but dep_delay is not in Figure 1. There are 1,175 such flights.\n\nflights |&gt;\n  filter(is.na(arr_delay) & !is.na(dep_delay)) |&gt;\n  select(flight, arr_delay, dep_delay) |&gt;\n  gt() |&gt;\n  cols_label(flight = \"Flight Number\",\n             arr_delay = \"Arrival Delay (in min)\",\n             dep_delay = \"Departure Delay (in min)\") |&gt;\n  opt_interactive(pagination_type = \"jump\")\n\n\n\n\n\n\n\n\nFigure 1: Table of all flights where arr_delay is missing but dep_delay is not\n\n\n\nThe following code displays all flights where neither arr_time nor sched_arr_time are missing, but arr_delay is missing reflected in Figure 2. There are 717 such flights.\n\nflights |&gt;\n  filter(!is.na(arr_time) & !is.na(sched_arr_time) & is.na(arr_delay)) |&gt;\n  select(flight, arr_time, sched_arr_time, arr_delay) |&gt;\n  gt() |&gt;\n  cols_label(flight = \"Flight Number\",\n             arr_delay = \"Arrival Delay (in min)\",\n             arr_time = \"Arrival Time (hrs)\",\n             sched_arr_time = \"Scheduled Arrival Time (hrs)\") |&gt;\n  opt_interactive(pagination_type = \"jump\")\n\n\n\n\n\n\n\n\nFigure 2: Table of all flights where arr_delay is missing but dep_delay is not"
  },
  {
    "objectID": "Chapter13.html#question-2-1",
    "href": "Chapter13.html#question-2-1",
    "title": "Chapter 13",
    "section": "Question 2",
    "text": "Question 2\nHow many flights have a missing dep_time? What other variables are missing in these rows? What might these rows represent?\nThe following code shows us that 8,255 flights have a missing dep_time . Further, Figure 3 shows us that these flights have missing dep_delay , arr_time , arr_delay and air_time. Further, around 30% of these have missing tailnum.\nThus, these rows most likely represent cancelled flights.\n\nflights |&gt;\n  filter(is.na(dep_time)) |&gt;\n  count() |&gt; as.numeric()\n\n[1] 8255\n\nflights |&gt;\n  filter(is.na(dep_time)) |&gt;\n  naniar::vis_miss() +\n  theme(axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.text.x.top = element_text(angle = 90))\n\n\n\n\nFigure 3: Visualization of missing values in flights with missing dep_time"
  },
  {
    "objectID": "Chapter13.html#question-3",
    "href": "Chapter13.html#question-3",
    "title": "Chapter 13",
    "section": "Question 3",
    "text": "Question 3\nAssuming that a missing dep_time implies that a flight is cancelled, look at the number of cancelled flights per day. Is there a pattern? Is there a connection between the proportion of cancelled flights and the average delay of non-cancelled flights?\nAs shown in Figure 4 below, when seen over the entire course of year, no specific pattern emerges. However, certain months show more number of spikes - there is one day with large number of cancellations (over 400) in February. When seen month-wise, as in Figure 5 , it becomes apparent that most flight cancellations occur in December - March; or in June-July.\n\n\nCode\nflights |&gt;\n  filter(is.na(dep_time)) |&gt;\n  mutate(date = make_date(year = year,\n                          month = month,\n                          day = day)) |&gt;\n  group_by(date) |&gt;\n  count() |&gt;\n  ggplot(aes(x = date, y = n)) +\n  geom_line() +\n  ggthemes::theme_fivethirtyeight() +\n  labs(title = \"Cancelled flight numbers show spikes on certain days\",\n       subtitle = \"There is consistent cancellation in December - may be due to snowstorms!\") +\n  theme(\n    plot.background = element_rect(fill = \"white\"), \n    panel.background = element_rect(fill = \"white\", colour = \"white\")\n    )\n\n\n\n\n\nFigure 4: ?(caption)\n\n\n\n\n\n\nCode\nmths_lab = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\",\n             \"May\", \"Jun\", \"Jul\", \"Aug\",\n             \"Sep\", \"Oct\", \"Nov\", \"Dec\")\ngridExtra::grid.arrange(\n  flights |&gt;\n    filter(is.na(dep_time)) |&gt;\n    group_by(month) |&gt;\n    count() |&gt;\n    ggplot(aes(x = month, y = n)) +\n    geom_line() +\n    ggthemes::theme_fivethirtyeight() +\n    labs(y = \"No. of cancelled flights\",\n         title = \"Flight cancellations occur most \\nin a few months\",\n         subtitle = \"December-Feruary, June-July see maximum cancelled flights. \\nOctober-November have the least cancellations\") +\n    scale_x_continuous(breaks = 1:12,\n                       labels = mths_lab) +\n    theme(title = element_text(size = 8)) +\n    theme(\n      plot.background = element_rect(fill = \"white\"), \n      panel.background = element_rect(fill = \"white\", colour = \"white\")\n    ),\n  \n  flights |&gt;\n    filter(is.na(dep_time)) |&gt;\n    group_by(day) |&gt;\n    count() |&gt;\n    ggplot(aes(x = day, y = n)) +\n    geom_line() +\n    geom_point(col = \"darkgrey\", size = 2) +\n    ggthemes::theme_fivethirtyeight() +\n    labs(y = \"No. of cancelled flights\",\n         title = \"Flight cancellations spike on 8th-9th days \\nof the months\",\n         subtitle = \"This is unlikely to be a pattern since \\nthis is a sum, influenced by a few high values\") + \n    scale_x_continuous(breaks = seq(1, 31, 5)) +\n    theme(title = element_text(size = 8)) +\n    theme(\n      plot.background = element_rect(fill = \"white\"), \n      panel.background = element_rect(fill = \"white\", colour = \"white\")\n    ),\n\nncol = 2)\n\n\n\n\n\nFigure 5: ?(caption)\n\n\n\n\nThe following Figure 6 depicts the connection between the proportion of cancelled flights and the average delay of non-cancelled flights. It shows that there is high average delay on the days where there are more cancelled flights, perhaps because bad weather causes both.\nNote: Here, I used geom_smooth() instead of geom_line() to make the pattern more easily perceptible.\n\n\nCode\ncoeff = 0.15\n\nflights |&gt;\n  mutate(date = make_date(year = year,\n                          month = month,\n                          day = day)) |&gt;\n  group_by(month, date) |&gt;\n  summarise(\n    n = n(),\n    cancelled = sum(is.na(dep_time)),\n    prop_cancelled = sum(is.na(dep_time)) / n(),\n    avg_delay = mean(dep_delay, na.rm = TRUE)\n  ) |&gt;\n  ggplot(aes(x = date)) +\n  geom_smooth(aes(y = prop_cancelled * 100), \n              method = \"loess\",\n              span = coeff,\n              col = \"red\",\n              se = FALSE) +\n  geom_smooth(aes(y = avg_delay), , \n              method = \"loess\",\n              span = coeff,\n              col = \"blue\",\n              se = FALSE) +  \n  theme_minimal() +\n  scale_y_continuous(\n    name = \"Percentage Cancelled Flights (%)\",\n    sec.axis = sec_axis(trans = ~ .*0.5,\n                        name = \"Avg. Delay (min.)\")) +\n  labs(x = NULL) +\n  theme(axis.text.y.left = element_text(color = \"red\"),\n        axis.title.y.left = element_text(color = \"red\"),\n        axis.text.y.right = element_text(color = \"blue\"),\n        axis.title.y.right = element_text(color = \"blue\")) +\n  labs(title = \"Flight Cancellations and average delays are correlated\",\n       subtitle = \"Higher average delay occurs on same days as more flight cancellations\")\n\n\n\n\n\nFigure 6: Correlation between proportion of cancelled flights and the average delay of non-cancelled flights"
  },
  {
    "objectID": "Chapter13.html#question-1-2",
    "href": "Chapter13.html#question-1-2",
    "title": "Chapter 13",
    "section": "Question 1",
    "text": "Question 1\nWhat will sum(is.na(x)) tell you? How about mean(is.na(x))?\nThe expression sum(is.na(x)) tells us the number of missing values in the vector x. The expression mean(is.na(x)) tells us the proportion of missing values in the vector x .\nThis is because is.na(x) is a function or operation used to determine which elements of a vector or data structure x are missing or NA (Not Available) values. NA values typically represent missing or undefined data points. Here’s what the two expressions you provided mean:\n\nsum(is.na(x)):\n\nThis expression will count the number of NA (missing) values in the vector or data structure x. It calculates the total count of NA values in the entire dataset.\nThe result will be an integer representing the count of NA values.\n\nmean(is.na(x)):\n\nThis expression will calculate the proportion of NA (missing) values in the vector or data structure x. It calculates the average of a binary vector where each element is either 1 (if NA) or 0 (if not NA).\nThe result will be a numeric value between 0 and 1, representing the fraction of missing values in the dataset. It can be interpreted as the percentage of missing values when multiplied by 100.\n\n\nHere’s a simple example in R to illustrate these concepts:\n\n# Sample vector with missing values \nx &lt;- c(1, NA, 3, NA, 5, 6)  \n\n# Count of missing values \ncount_missing &lt;- sum(is.na(x)) \ncat(\"Count of missing values:\", count_missing, \"\\n\")  \n\nCount of missing values: 2 \n\n# Proportion of missing values \nprop_missing &lt;- mean(is.na(x)) \ncat(\"Proportion of missing values:\", prop_missing, \"\\n\")\n\nProportion of missing values: 0.3333333"
  },
  {
    "objectID": "Chapter13.html#question-2-2",
    "href": "Chapter13.html#question-2-2",
    "title": "Chapter 13",
    "section": "Question 2",
    "text": "Question 2\nWhat does prod() return when applied to a logical vector? What logical summary function is it equivalent to? What does min() return when applied to a logical vector? What logical summary function is it equivalent to? Read the documentation and perform a few experiments.\n\n# A logical vector with random TRUE and FALSE\nrandom &lt;- sample(c(TRUE, FALSE),\n            size = 10,\n            replace = TRUE)\nrandom\n##  [1]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE\n# A logical vector with all TRUE\nall_true &lt;- rep(TRUE, 10)\nall_true\n##  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n# A logical vector with all FALSE\nall_false &lt;- rep(FALSE, 10)\nall_false\n##  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\nprod(random)\n## [1] 0\nprod(all_true)\n## [1] 1\nprod(all_false)\n## [1] 0\n\nmin(random)\n## [1] 0\nmin(all_true)\n## [1] 1\nmin(all_false)\n## [1] 0\n\nIn R, when we apply the prod() function to a logical vector, it treats TRUE as 1 and FALSE as 0, and then computes the product of the elements in the vector. Essentially, it multiplies all the elements together. This can be useful when we want to check if all elements in a logical vector are TRUE, as the product will be 1 if all are TRUE and 0 if any of them is FALSE.\nThis is equivalent to using the all() function, which checks if all elements in a logical vector are TRUE. The all() function returns TRUE if all elements are TRUE and FALSE otherwise.\nNow, when we apply the min() function to a logical vector, it also treats TRUE as 1 and FALSE as 0, and then computes the minimum value. Since 0 represents FALSE and 1 represents TRUE, the minimum value in a logical vector is FALSE (0). Therefore, when we use min() on a logical vector with even one value FALSE, it will return FALSE.\nIn summary, prod() and min() applied to logical vectors have specific behavior related to the interpretation of TRUE and FALSE, and they are equivalent to the all().\nNote: max() will act as equivalent to the any() function."
  },
  {
    "objectID": "Chapter13.html#question-1-3",
    "href": "Chapter13.html#question-1-3",
    "title": "Chapter 13",
    "section": "Question 1",
    "text": "Question 1\nA number is even if it's divisible by two, which in R you can find out with x %% 2 == 0. Use this fact and if_else() to determine whether each number between 0 and 20 is even or odd.\nThe easiest way to do this would be if_else(x %% 2 == 0, true = \"even\", false = \"odd\"), as shown below: –\n\nx = 0:20\nif_else(x %% 2 == 0,\n        true = \"even\",\n        false = \"odd\")\n\n [1] \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\" \n[11] \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\" \n[21] \"even\""
  },
  {
    "objectID": "Chapter13.html#question-2-3",
    "href": "Chapter13.html#question-2-3",
    "title": "Chapter 13",
    "section": "Question 2",
    "text": "Question 2\nGiven a vector of days like x &lt;- c(\"Monday\", \"Saturday\", \"Wednesday\"), use an ifelse() statement to label them as weekends or weekdays.\nThe code shown below does the job: –\n\ndays = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\",\n      \"Friday\", \"Saturday\", \"Sunday\")\nweeknd = c(\"Saturday\", \"Sunday\")\n\nx = sample(days, size = 10, replace = TRUE)\n\ncbind(x,\n      if_else(x %in% weeknd,\n              \"Weekends\",\n              \"Weekdays\", \n              \"NA\")) |&gt;\n  as_tibble() |&gt; gt()\n\n\n\n\n\n  \n    \n    \n      x\n      V2\n    \n  \n  \n    Friday\nWeekdays\n    Friday\nWeekdays\n    Thursday\nWeekdays\n    Friday\nWeekdays\n    Saturday\nWeekends\n    Thursday\nWeekdays\n    Sunday\nWeekends\n    Thursday\nWeekdays\n    Wednesday\nWeekdays\n    Sunday\nWeekends"
  },
  {
    "objectID": "Chapter13.html#question-3-1",
    "href": "Chapter13.html#question-3-1",
    "title": "Chapter 13",
    "section": "Question 3",
    "text": "Question 3\nUse ifelse() to compute the absolute value of a numeric vector called x.\nWe can use the code if_else(x &lt; 0, true = -x, false = x, missing = 0) to do so, as shown below: –\n\nx = sample(x = -10:10,\n           replace = TRUE,\n           size = 100)\n\ntibble(\n  x = x,\n  abs_x = if_else(x &lt; 0,\n                  true = -x,\n                  false = x,\n                  missing = 0)\n) |&gt;\n  gt() |&gt;\n  opt_interactive(use_pagination = TRUE,\n                  pagination_type = \"simple\")"
  },
  {
    "objectID": "Chapter13.html#question-4",
    "href": "Chapter13.html#question-4",
    "title": "Chapter 13",
    "section": "Question 4",
    "text": "Question 4\nWrite a case_when() statement that uses the month and day columns from flights to label a selection of important US holidays (e.g., New Years Day, 4th of July, Thanksgiving, and Christmas). First create a logical column that is either TRUE or FALSE, and then create a character column that either gives the name of the holiday or is NA.\nHere is the code: –\n\nflights |&gt;\n  mutate(holiday = case_when(\n    month == 1 & day == 1   ~ \"New Year’s Day\",\n    month == 6 & day == 19  ~ \"Juneteenth National Independence Day\",\n    month == 7 & day == 4   ~ \"Independence Day\",\n    month == 11 & day == 11 ~ \"Veterans’ Day\",\n    month == 12 & day == 25 ~ \"Christmas Day\",\n    .default = NA\n  ),\n  .keep = \"used\") |&gt;\n  mutate(is_holiday = if_else(!is.na(holiday),\n                              true = TRUE,\n                              false = FALSE))\n\n# A tibble: 336,776 × 4\n   month   day holiday        is_holiday\n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;          &lt;lgl&gt;     \n 1     1     1 New Year’s Day TRUE      \n 2     1     1 New Year’s Day TRUE      \n 3     1     1 New Year’s Day TRUE      \n 4     1     1 New Year’s Day TRUE      \n 5     1     1 New Year’s Day TRUE      \n 6     1     1 New Year’s Day TRUE      \n 7     1     1 New Year’s Day TRUE      \n 8     1     1 New Year’s Day TRUE      \n 9     1     1 New Year’s Day TRUE      \n10     1     1 New Year’s Day TRUE      \n# ℹ 336,766 more rows\n\n\nAnother way to code this creating the logical column first is as follows: –\n\nflights |&gt;\n  mutate(\n    is_holiday = case_when(\n      (month == 1 & day == 1) ~ TRUE,        # New Year's Day\n      (month == 7 & day == 4) ~ TRUE,        # 4th of July\n      (month == 12 & day == 25) ~ TRUE,      # Christmas\n      TRUE ~ FALSE                           # Not a holiday\n    ),\n    holiday_name = case_when(\n      is_holiday ~ case_when(\n        (month == 1 & day == 1) ~ \"New Year's Day\",\n        (month == 7 & day == 4) ~ \"4th of July\",\n        (month == 12 & day == 25) ~ \"Christmas\",\n        TRUE ~ NA_character_\n      ),\n      TRUE ~ NA_character_\n    ),\n    .keep = \"used\"\n  )\n\n# A tibble: 336,776 × 4\n   month   day is_holiday holiday_name  \n   &lt;int&gt; &lt;int&gt; &lt;lgl&gt;      &lt;chr&gt;         \n 1     1     1 TRUE       New Year's Day\n 2     1     1 TRUE       New Year's Day\n 3     1     1 TRUE       New Year's Day\n 4     1     1 TRUE       New Year's Day\n 5     1     1 TRUE       New Year's Day\n 6     1     1 TRUE       New Year's Day\n 7     1     1 TRUE       New Year's Day\n 8     1     1 TRUE       New Year's Day\n 9     1     1 TRUE       New Year's Day\n10     1     1 TRUE       New Year's Day\n# ℹ 336,766 more rows"
  },
  {
    "objectID": "Chapter14.html",
    "href": "Chapter14.html",
    "title": "Chapter 14",
    "section": "",
    "text": "library(tidyverse)\nlibrary(gt)\nlibrary(janitor)\nlibrary(nycflights13)\ndata(\"flights\")"
  },
  {
    "objectID": "Chapter14.html#question-1",
    "href": "Chapter14.html#question-1",
    "title": "Chapter 14",
    "section": "Question 1",
    "text": "Question 1\nHow can you use count() to count the number rows with a missing value for a given variable?\nWe can use count() with weights as is.na(var_name) to the number of rows with missing values, as shown below\n\nflights |&gt;\n  group_by(month) |&gt;\n  summarise(total = n(),\n            missing = sum(is.na(dep_time))) |&gt;\n  gt()\nflights |&gt;\n  group_by(month) |&gt;\n  count(wt = is.na(dep_time)) |&gt;\n  ungroup() |&gt;\n  gt()\n\n\n\n\n\n\n\n  \n    \n    \n      month\n      total\n      missing\n    \n  \n  \n    1\n27004\n521\n    2\n24951\n1261\n    3\n28834\n861\n    4\n28330\n668\n    5\n28796\n563\n    6\n28243\n1009\n    7\n29425\n940\n    8\n29327\n486\n    9\n27574\n452\n    10\n28889\n236\n    11\n27268\n233\n    12\n28135\n1025\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n      month\n      n\n    \n  \n  \n    1\n521\n    2\n1261\n    3\n861\n    4\n668\n    5\n563\n    6\n1009\n    7\n940\n    8\n486\n    9\n452\n    10\n236\n    11\n233\n    12\n1025"
  },
  {
    "objectID": "Chapter14.html#question-2",
    "href": "Chapter14.html#question-2",
    "title": "Chapter 14",
    "section": "Question 2",
    "text": "Question 2\nExpand the following calls to count() to instead use group_by(), summarize(), and arrange():\n\nflights |&gt; count(dest, sort = TRUE)\n\nflights |&gt;\n  group_by(dest) |&gt;\n  summarise(n = n()) |&gt;\n  arrange(desc(n))\n\n# A tibble: 105 × 2\n   dest      n\n   &lt;chr&gt; &lt;int&gt;\n 1 ORD   17283\n 2 ATL   17215\n 3 LAX   16174\n 4 BOS   15508\n 5 MCO   14082\n 6 CLT   14064\n 7 SFO   13331\n 8 FLL   12055\n 9 MIA   11728\n10 DCA    9705\n# ℹ 95 more rows\n\n\nflights |&gt; count(tailnum, wt = distance)\n\nflights |&gt;\n  group_by(tailnum) |&gt;\n  summarise(n = sum(distance))\n\n# A tibble: 4,044 × 2\n   tailnum      n\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 D942DN    3418\n 2 N0EGMQ  250866\n 3 N10156  115966\n 4 N102UW   25722\n 5 N103US   24619\n 6 N104UW   25157\n 7 N10575  150194\n 8 N105UW   23618\n 9 N107US   21677\n10 N108UW   32070\n# ℹ 4,034 more rows"
  },
  {
    "objectID": "Chapter14.html#question-1-1",
    "href": "Chapter14.html#question-1-1",
    "title": "Chapter 14",
    "section": "Question 1",
    "text": "Question 1\nExplain in words what each line of the code used to generate Figure 14.1 does.\nThe explanation is given as annotation in the code below (i.e, lines starting with #): –\n\n# Load in the data-set flights from package nycflights13\nflights |&gt; \n  # Create a variable hour, which is the quotient of the division of\n  # sched_dep_time by 100. Further, group the dataset by this newly \n  # created variable \"hour\" to get data into 24 groups - one for each\n  # hour.\n  group_by(hour = sched_dep_time %/% 100) |&gt; \n  \n  # For each gropu, i.e. all flights scheduled to depart in that\n  # hour, compute the NAs, i.e. cancelled flights, then compute their\n  # mean, i.e. proportion of cancelled flights; and also create a \n  # variable n, which is the total number of flights\n  summarize(prop_cancelled = mean(is.na(dep_time)), n = n()) |&gt; \n  \n  # Remove the flights departing between 12 midnight and 1 am, since\n  # these are very very few, and all are cancelled leading to a highly\n  # skewed and uninformative graph\n  filter(hour &gt; 1) |&gt; \n  \n  # Start a ggplot call, plotting the hour on the x-axis, and \n  # proportion of cancelled flights on the y-axis\n  ggplot(aes(x = hour, y = prop_cancelled)) +\n  \n  # Create a line graph,which joins the proportion of cancelled \n  # flights for each hour. Also, put in in dark grey colour\n  geom_line(color = \"grey50\") + \n  \n  # Add points for each hour, whose size varies with the total number\n  # of flights in that hour\n  geom_point(aes(size = n))"
  },
  {
    "objectID": "Chapter14.html#question-2-1",
    "href": "Chapter14.html#question-2-1",
    "title": "Chapter 14",
    "section": "Question 2",
    "text": "Question 2\nWhat trigonometric functions does R provide? Guess some names and look up the documentation. Do they use degrees or radians?\nR provides several trigonometric functions, most of which operate in radians. Here’s a table listing some of the commonly used trigonometric functions in R, along with short descriptions and information about whether they use degrees or radians:\n\n\n\n\n\n\n\n\nFunction\nDescription\nAngle Measure\n\n\n\n\nsin(x)\nSine function: Computes the sine of the angle x.\nRadians\n\n\ncos(x)\nCosine function: Computes the cosine of the angle x.\nRadians\n\n\ntan(x)\nTangent function: Computes the tangent of the angle x.\nRadians\n\n\nasin(x) or acos(x)\nInverse sine or inverse cosine: Computes the angle whose sine or cosine is x.\nRadians\n\n\natan(x) or atan2(y, x)\nInverse tangent or arctangent: Computes the angle whose tangent is x or the angle between the point (x, y) and the origin.\nRadians\n\n\nsinh(x)\nHyperbolic sine function: Computes the hyperbolic sine of x.\nRadians\n\n\ncosh(x)\nHyperbolic cosine function: Computes the hyperbolic cosine of x.\nRadians\n\n\ntanh(x)\nHyperbolic tangent function: Computes the hyperbolic tangent of x.\nRadians\n\n\n\nIn R, trigonometric functions like sin, cos, and tan expect angles to be in radians by default. However, we can convert between degrees and radians using the deg2rad and rad2deg functions. For example, to compute the sine of an angle in degrees, you can use sin(deg2rad(angle)).\n\n\nCode\ndf = tibble(x = seq(from = -5, to = +5, by = 0.1))\ng = ggplot(df, aes(x = x)) + \n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  labs(x = NULL, y = NULL) +\n  scale_x_continuous(breaks = -5:5)\ngridExtra::grid.arrange(\n  g + geom_line(aes(y = sin(x))) + labs(title = \"sin(x)\"),\n  g + geom_line(aes(y = cos(x))) + labs(title = \"cos(x)\"),\n  g + geom_line(aes(y = tan(x))) + labs(title = \"tan(x)\"),\n  g + geom_line(aes(y = asin(x))) + labs(title = \"asin(x)\"),\n  g + geom_line(aes(y = acos(x))) + labs(title = \"acos(x)\"),\n  g + geom_line(aes(y = atan(x))) + labs(title = \"atan(x)\"),\n  g + geom_line(aes(y = sinh(x))) + labs(title = \"sinh(x)\"),\n  g + geom_line(aes(y = cosh(x))) + labs(title = \"cosh(x)\"),\n  \n  nrow = 2\n)\n\n\n\n\n\nFigure 1: Graphs from some common trigonometric functions"
  },
  {
    "objectID": "Chapter14.html#question-3",
    "href": "Chapter14.html#question-3",
    "title": "Chapter 14",
    "section": "Question 3",
    "text": "Question 3\nCurrently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. You can see the basic problem by running the code below: there’s a gap between each hour.\nflights |&gt;    \n  filter(month == 1, day == 1) |&gt;    \n  ggplot(aes(x = sched_dep_time, y = dep_delay)) +   \n  geom_point()\nConvert them to a more truthful representation of time (either fractional hours or minutes since midnight).\nWe can correct the sched_dep_time() using the two arithmetical functions %/% and %% to generate the decimal representation of time in hours as shown in the code for the graph on the right hand side in\n\n\nCode\ngridExtra::grid.arrange(\n  flights |&gt;    \n    filter(month == 1, day == 1) |&gt;    \n    ggplot(aes(x = sched_dep_time, y = dep_delay)) +   \n    geom_point(size = 0.5) +\n    labs(subtitle = \"Incorrect scheduled departure time\"),\n\n  flights |&gt;\n    mutate(\n      hour_dep = sched_dep_time %/% 100,\n      min_dep  = sched_dep_time %%  100,\n      time_dep = hour_dep + (min_dep/60)\n    ) |&gt;\n    filter(month == 1, day == 1) |&gt;    \n    ggplot(aes(x = time_dep, y = dep_delay)) +   \n    geom_point(size = 0.5) +\n    labs(subtitle = \"Improved and accurate scheduled departure time\",\n         x = \"Scheduled Departure Time (in hrs)\") +\n    scale_x_continuous(breaks = seq(0,24,4)),\n  \n  ncol = 2)\n\n\n\n\n\nFigure 2: Improved represenation of scheduled departure time to remove breaks in the data owing to represention of time as hhmm"
  },
  {
    "objectID": "Chapter14.html#question-4",
    "href": "Chapter14.html#question-4",
    "title": "Chapter 14",
    "section": "Question 4",
    "text": "Question 4\nRound dep_time and arr_time to the nearest five minutes.\n\nattach(flights)\nflights |&gt;\n  slice_head(n = 50) |&gt;\n  mutate(\n    dep_time_5 = round(dep_time/5) * 5,\n    arr_time_5 = round(arr_time/5) * 5,\n    .keep = \"used\"\n  ) |&gt;\n  gt() |&gt;\n  opt_interactive()"
  },
  {
    "objectID": "Chapter14.html#question-1-2",
    "href": "Chapter14.html#question-1-2",
    "title": "Chapter 14",
    "section": "Question 1",
    "text": "Question 1\nFind the 10 most delayed flights using a ranking function. How do you want to handle ties? Carefully read the documentation for min_rank().\nWe can find the 10 most delayed flights by using min_rank() since it gives ties equal rank, while creating gaps, thus keeping the total number of the flights to 10.\n\nflights |&gt;\n  select(sched_dep_time, dep_time, dep_delay, tailnum, dest, carrier) |&gt;\n  mutate(rank_delay = min_rank(desc(dep_delay))) |&gt;\n  arrange(rank_delay) |&gt;\n  slice_head(n = 10) |&gt;\n  gt()\n\n\n\n\n\n  \n    \n    \n      sched_dep_time\n      dep_time\n      dep_delay\n      tailnum\n      dest\n      carrier\n      rank_delay\n    \n  \n  \n    900\n641\n1301\nN384HA\nHNL\nHA\n1\n    1935\n1432\n1137\nN504MQ\nCMH\nMQ\n2\n    1635\n1121\n1126\nN517MQ\nORD\nMQ\n3\n    1845\n1139\n1014\nN338AA\nSFO\nAA\n4\n    1600\n845\n1005\nN665MQ\nCVG\nMQ\n5\n    1900\n1100\n960\nN959DL\nTPA\nDL\n6\n    810\n2321\n911\nN927DA\nMSP\nDL\n7\n    1900\n959\n899\nN3762Y\nPDX\nDL\n8\n    759\n2257\n898\nN6716C\nATL\nDL\n9\n    1700\n756\n896\nN5DMAA\nMIA\nAA\n10"
  },
  {
    "objectID": "Chapter14.html#question-2-2",
    "href": "Chapter14.html#question-2-2",
    "title": "Chapter 14",
    "section": "Question 2",
    "text": "Question 2\nWhich plane (tailnum) has the worst on-time record?\nUsing the code below, we can find the that the flight with tail number N844MH has the highest average departure delay of 32 minutes. However, this flight flew only once, so we might want to re-look for this delay statistic by removing the flights that flew less than 5 times.\nThus, amongst the flights that flew 5 times or more, the highest mean departure delay of the flight with tail number N665MQ.\n\n# The flight tailnum with the highest average delay\nflights |&gt;\n  group_by(tailnum) |&gt;\n  summarize(mean_delay = mean(dep_delay, na.rm = TRUE),\n            n = n()) |&gt;\n  arrange(desc(mean_delay)) |&gt;\n  slice_head(n = 5)\n\n# A tibble: 5 × 3\n  tailnum mean_delay     n\n  &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;\n1 N844MH         297     1\n2 N922EV         274     1\n3 N587NW         272     1\n4 N911DA         268     1\n5 N851NW         233     1\n\n# The flight tailnum with the highest average delay amongst \n# the flights that flew atleast 5 times or more\nflights |&gt;\n  group_by(tailnum) |&gt;\n  summarize(mean_delay = mean(dep_delay, na.rm = TRUE),\n            nos_of_flights = n()) |&gt;\n  filter(nos_of_flights &gt;= 5) |&gt;\n  arrange(desc(mean_delay)) |&gt;\n  slice_head(n = 5)\n\n# A tibble: 5 × 3\n  tailnum mean_delay nos_of_flights\n  &lt;chr&gt;        &lt;dbl&gt;          &lt;int&gt;\n1 N665MQ       177                6\n2 N276AT        84.8              6\n3 N652SW        79.5              6\n4 N919FJ        78                6\n5 N396SW        69.7              7"
  },
  {
    "objectID": "Chapter14.html#question-3-1",
    "href": "Chapter14.html#question-3-1",
    "title": "Chapter 14",
    "section": "Question 3",
    "text": "Question 3\nWhat time of day should you fly if you want to avoid delays as much as possible?\nAs shown in the Figure 4 , we should fly with the early morning flights, particularly 5 am to 7 am, to avoid delays as much as possible.\n\nflights |&gt;\n  mutate(sched_dep_hour = sched_dep_time %/% 100) |&gt;\n  group_by(sched_dep_hour) |&gt;\n  summarize(\n    mean_dep_delay = mean(dep_delay, na.rm = TRUE),\n    nos_of_flights = n()\n  ) |&gt;\n  drop_na() |&gt;\n  ggplot(aes(x = sched_dep_hour,\n             y = mean_dep_delay)) +\n  geom_line() +\n  geom_point(aes(size = nos_of_flights), \n             alpha = 0.5) +\n  theme_light() +\n  labs(x = \"Depature Hour\", y = \"Average departure delay (min)\",\n       title = \"Early morning flights have the least delay\",\n       size = \"Number of flights\") +\n  scale_x_continuous(breaks = seq(5, 24, 2)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 4: Graph of average departure delay vs. scheduled departure time"
  },
  {
    "objectID": "Chapter14.html#question-4-1",
    "href": "Chapter14.html#question-4-1",
    "title": "Chapter 14",
    "section": "Question 4",
    "text": "Question 4\nWhat does flights |&gt; group_by(dest) |&gt; filter(row_number() &lt; 4) do? What does flights |&gt; group_by(dest) |&gt; filter(row_number(dep_delay) &lt; 4) do?\nThe code flights |&gt; group_by(dest) |&gt; filter(row_number() &lt; 4) displays only three flights for each destination, selected on the basis of the order in which they appear in the flights data-set.\n\nflights |&gt; \n  # reducing the number of columns for easy display\n  select(carrier, dest, sched_dep_time, month, day) |&gt;\n  group_by(dest) |&gt; \n  # arrange(dest, sched_dep_time) |&gt;\n  filter(row_number() &lt; 4)\n\n# A tibble: 311 × 5\n# Groups:   dest [105]\n   carrier dest  sched_dep_time month   day\n   &lt;chr&gt;   &lt;chr&gt;          &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 UA      IAH              515     1     1\n 2 UA      IAH              529     1     1\n 3 AA      MIA              540     1     1\n 4 B6      BQN              545     1     1\n 5 DL      ATL              600     1     1\n 6 UA      ORD              558     1     1\n 7 B6      FLL              600     1     1\n 8 EV      IAD              600     1     1\n 9 B6      MCO              600     1     1\n10 AA      ORD              600     1     1\n# ℹ 301 more rows\n\n\nOn the other hand, the code flights |&gt; group_by(dest) |&gt; filter(row_number(dep_delay) &lt; 4) will display the three flights with the least departure delays for each destination. Further, in case of ties, it will select the flight which appears earlier in the data-set, i.e. on a earlier date.\n\nflights |&gt; \n  # reducing the number of columns for easy display\n  select(carrier, dest, sched_dep_time, month, day, dep_delay) |&gt;\n  group_by(dest) |&gt; \n  filter(row_number(dep_delay) &lt; 4)\n\n# A tibble: 310 × 6\n# Groups:   dest [104]\n   carrier dest  sched_dep_time month   day dep_delay\n   &lt;chr&gt;   &lt;chr&gt;          &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n 1 UA      JAC              851     1     1        -3\n 2 EV      AVL              959     1     1       -13\n 3 DL      PWM             2159     1     4       -19\n 4 UA      MTJ              901     1     5        -2\n 5 EV      BTV             2159     1     5       -16\n 6 B6      SJC             1810     1     7        -9\n 7 FL      CAK             2030     1     7       -17\n 8 9E      PIT              750     1     8       -15\n 9 EV      BHM             1858     1     9       -11\n10 EV      MYR              827     1    10       -17\n# ℹ 300 more rows"
  },
  {
    "objectID": "Chapter14.html#question-5",
    "href": "Chapter14.html#question-5",
    "title": "Chapter 14",
    "section": "Question 5",
    "text": "Question 5\nFor each destination, compute the total minutes of delay. For each flight, compute the proportion of the total delay for its destination.\nThe following code does the calculation desired: –\n\nflights |&gt;\n  group_by(dest) |&gt;\n  mutate(\n    total_delay = sum(dep_delay, na.rm = TRUE),\n    prop_delay = dep_delay / total_delay,\n    .keep = \"used\"\n  )\n\n# A tibble: 336,776 × 4\n# Groups:   dest [105]\n   dep_delay dest  total_delay  prop_delay\n       &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1         2 IAH         77012  0.0000260 \n 2         4 IAH         77012  0.0000519 \n 3         2 MIA        103261  0.0000194 \n 4        -1 BQN         11032 -0.0000906 \n 5        -6 ATL        211391 -0.0000284 \n 6        -4 ORD        225840 -0.0000177 \n 7        -5 FLL        151933 -0.0000329 \n 8        -3 IAD         91555 -0.0000328 \n 9        -3 MCO        157661 -0.0000190 \n10        -2 ORD        225840 -0.00000886\n# ℹ 336,766 more rows"
  },
  {
    "objectID": "Chapter14.html#question-6",
    "href": "Chapter14.html#question-6",
    "title": "Chapter 14",
    "section": "Question 6",
    "text": "Question 6\nDelays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag(), explore how the average flight delay for an hour is related to the average delay for the previous hour.\nHere’s the code for the analysis with annotation at appropriate places: –\n\nflights |&gt;    \n  # Generate hour of departure\n  mutate(hour = dep_time %/% 100) |&gt;    \n  \n  # Creating groups by each hour of depature for different days\n  group_by(year, month, day, hour) |&gt;  \n  \n  # Remove 213 flights with NA as hour (i.e., cancelled flights)\n  # to improve the subsequent plotting\n  filter(!is.na(hour)) |&gt;\n  \n  # Average delay and the number of flights in each hour\n  summarize(     \n    dep_delay = mean(dep_delay, na.rm = TRUE),     \n    n = n(),     \n    .groups = \"drop\"   \n  ) |&gt;    \n  \n  # Removing hours in which there were less than 5 flights departing\n  filter(n &gt; 5) |&gt;\n  \n  # Grouping to prevent using 11 pm hour delays as previous delays of the\n  # next day's 5 am flights\n  group_by(year, month, day) |&gt;\n  \n  # A new variabe to show previous hour's average average departure delay\n  mutate(\n    prev_hour_delay = lag(dep_delay),\n    morning_flights = hour == 5\n  ) |&gt;\n  \n  # Plotting to see correlation\n  ggplot(aes(x = dep_delay,\n             y = prev_hour_delay,\n             col = morning_flights)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(se = FALSE) +\n  geom_abline(slope = 1, intercept = 0, col = \"grey\") +\n  theme_light() +\n  coord_fixed() +\n  scale_x_continuous(breaks = seq(0,300,60)) +\n  scale_y_continuous(breaks = seq(0,300,60)) +\n  scale_color_discrete(labels = c(\"Other flights\",\n                                  \"Early Morning flights (5 am to 6 am)\")) +\n  labs(x = \"Average Departure Delay in an hour (min)\",\n       y = \"Average Departure Delay in the previous hour (min)\",\n       col = NULL,\n       title = \"Departure Delay correlates with delay in previous hour\",\n       subtitle = \"The average departure delay in any hour is worse than previous hours's average delay. \\nFurther, the early morning flights' delay doesn't depend on previous nights' delay.\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 5: Plots of average departure delays in each hour vs. average departure delay in the previous hour"
  },
  {
    "objectID": "Chapter14.html#question-7",
    "href": "Chapter14.html#question-7",
    "title": "Chapter 14",
    "section": "Question 7",
    "text": "Question 7\nLook at each destination. Can you find flights that are suspiciously fast (i.e. flights that represent a potential data entry error)? Compute the air time of a flight relative to the shortest flight to that destination. Which flights were most delayed in the air?\nThe Table 1 computes the average air time to each destination, and then computes the ratio of each flight’s airtime to the average air time. There are four flights with this ratio less than 0.6. Out of these, the flight N666DN to ATL destination with air time of just 65 minutes (compared to average 113 minutes) appears to be suspiciously fast, and a potential data entry error.\n\nflights |&gt;\n  select(month, day, dest, tailnum, dep_time, arr_time, air_time) |&gt;\n  group_by(dest) |&gt;\n  mutate(\n    mean_air_time = mean(air_time, na.rm = TRUE),\n    fast_ratio = air_time/mean_air_time\n  ) |&gt;\n  relocate(air_time, mean_air_time, fast_ratio) |&gt;\n  ungroup() |&gt;\n  arrange(fast_ratio) |&gt;\n  filter(fast_ratio &lt; 0.6) |&gt;\n  gt() |&gt;\n  fmt_number(columns = c(mean_air_time, fast_ratio),\n    decimals = 2)\n\n\n\n\n\nTable 1:  Suspiciously fast flights compared to the average air time for that\ndestination \n  \n    \n    \n      air_time\n      mean_air_time\n      fast_ratio\n      month\n      day\n      dest\n      tailnum\n      dep_time\n      arr_time\n    \n  \n  \n    21\n38.95\n0.54\n3\n2\nBOS\nN947UW\n1450\n1547\n    65\n112.93\n0.58\n5\n25\nATL\nN666DN\n1709\n1923\n    55\n93.39\n0.59\n5\n13\nGSP\nN14568\n2040\n2225\n    23\n38.95\n0.59\n1\n25\nBOS\nN947UW\n1954\n2131\n  \n  \n  \n\n\n\n\n\nThe following code also computes the air time (air_time)of every flight relative to the shortest flight (min_air_time) for each destination in form a ratio (rel_ratio), and Figure 6 displays top 100 flights that were most delayed in the air. We observe that, of the 100 most delayed flights in-air: –\n\n83 of these are flights to Boston (BOS); with shortest air time of 21 minutes (a potential data entry) causing this over-representation of Boston flights in the current computation.\nOnce Boston is removed, most of top 100 delayed in-air flights belong to destinations PHL , DCA and ATL. These again either represent a erroneous data entry (flight N666DN to ATL destination with air time of just 65 minutes) or very short glifht destinations (PHL and DCA).\n\n\nflights |&gt;\n  select(month, day, dest, tailnum, dep_time, arr_time, air_time) |&gt;\n  group_by(dest) |&gt;\n  mutate(\n    min_air_time = min(air_time, na.rm = TRUE),\n    rel_ratio = air_time/min_air_time\n  ) |&gt;\n  relocate(air_time, min_air_time, rel_ratio) |&gt;\n  ungroup() |&gt;\n  arrange(desc(rel_ratio)) |&gt;\n  slice_head(n = 100) |&gt;\n  gt() |&gt;\n  fmt_number(columns = c(min_air_time, rel_ratio),\n    decimals = 2) |&gt;\n  opt_interactive()\n\n\n\n\n\n\n\n\nFigure 6: Flights that were most delayed in the air-time"
  },
  {
    "objectID": "Chapter14.html#question-8",
    "href": "Chapter14.html#question-8",
    "title": "Chapter 14",
    "section": "Question 8",
    "text": "Question 8\nFind all destinations that are flown by at least two carriers. Use those destinations to come up with a relative ranking of the carriers based on their performance for the same destination.\nFirst, as shown in Figure 7 , we find out the destinations which have at-least two or more carriers. There are 76 such destinations.\n\n\nCode\n# Computing the destinations with atleast two carriers\ndf1 = flights |&gt;\n  group_by(dest) |&gt;\n  summarize(no_of_carriers = n_distinct(carrier)) |&gt;\n  filter(no_of_carriers &gt; 1)\n\n# Extracting the names of these carriers as a vector to use in filtering\n# the data in subsequent steps, as a vector called \"dest2\"\ndest2 = df1 |&gt;\n  select(dest) |&gt;\n  as_vector() |&gt;\n  unname()\n\n# Displaying the table\ndf1 |&gt;\n  arrange(desc(no_of_carriers)) |&gt;\n  gt() |&gt;\n  opt_interactive(page_size_default = 5)\n\n\n\n\n\n\n\n\n\nFigure 7: Destinations with two or more carriers\n\n\n\nThen, in Figure 8 , we compute the three important parameters that we use to compare the performance of carriers: –\n\nAverage departure delay.\nProportion of flights cancelled.\nAverage delay in Air Time.\n\n\n\nCode\n# Ranking parameters can be based on: ---\n# 1. Average Departure Delay\n# 2. Proportion of flights cancelled\n# 3. Average Delay in Air Time\n# Note: I have not included arrival delay, as that will automatically\n#       include the delay in the departure, and penalize the airline \n#       twice for a departure delay.\n\ndf2 = flights |&gt;\n  filter(dest %in% dest2) |&gt;\n  group_by(dest, carrier) |&gt;\n  summarise(\n    mean_dep_delay = mean(dep_delay, na.rm = TRUE),\n    prop_cancelled = mean(is.na(dep_time)),\n    mean_air_time  = mean(air_time, na.rm = TRUE),\n    nos_of_flights = n()\n  ) |&gt;\n  drop_na()\n  \ndf2 |&gt;\n  ungroup() |&gt;\n  gt() |&gt;\n  fmt_number(decimals = 2,\n             columns = -nos_of_flights) |&gt;\n  opt_interactive()\n\n\n\n\n\n\n\n\n\nFigure 8: Table displaying the comparison parameters for each carrier at each destination\n\n\n\nThereafter, in Table 2 , we compute scores based on these three parameters for each carrier and rank them accordingly.\n\n\nCode\n# Ranking can be generated on basis of the least score calculated after\n# giving equal weightage to the following in formula for each carrier as\n# calculated by: (x - min(x)) / (max(x) - min(x))\n# 1. Average Departure Delay\n# 2. Proportion of flights cancelled\n# 3. Average Delay in Air Time\ndf3 = df2 |&gt;\n  mutate(\n    score_delay = (mean_dep_delay - min(mean_dep_delay))/(max(mean_dep_delay) - min(mean_dep_delay)),\n    score_cancel = (prop_cancelled - min(prop_cancelled)) / (max(prop_cancelled) - min(prop_cancelled)),\n    score_at = (mean_air_time - min(mean_air_time)) / (max(mean_air_time) - min(mean_air_time)),\n    total_score = score_delay + score_cancel + score_at,\n    rank_carrier = min_rank(total_score)\n  ) |&gt;\n  drop_na()\n\n# Displaying an example ranking for ATL and AUS\ndf3 |&gt;\n  filter(dest %in% c(\"ATL\", \"AUS\")) |&gt;\n  gt() |&gt;\n  fmt_number(columns = -c(rank_carrier, nos_of_flights)) |&gt;\n  tab_spanner(label = \"Scoring and Rank\",\n              columns = score_delay:rank_carrier) |&gt;\n  tab_spanner(label = \"Statistics\",\n              columns = mean_dep_delay:nos_of_flights) |&gt;\n  opt_stylize(style = 1)\n\n\n\n\n\n\nTable 2:  Computed rankings with the method of scoring displayed for two\ndestinations \n  \n    \n    \n      carrier\n      \n        Statistics\n      \n      \n        Scoring and Rank\n      \n    \n    \n      mean_dep_delay\n      prop_cancelled\n      mean_air_time\n      nos_of_flights\n      score_delay\n      score_cancel\n      score_at\n      total_score\n      rank_carrier\n    \n  \n  \n    \n      ATL\n    \n    9E\n0.96\n0.03\n120.11\n59\n0.00\n0.60\n0.75\n1.34\n4\n    DL\n10.41\n0.01\n112.16\n10571\n0.44\n0.15\n0.00\n0.59\n1\n    EV\n22.40\n0.06\n114.05\n1764\n1.00\n1.00\n0.18\n2.18\n7\n    FL\n18.45\n0.02\n114.44\n2337\n0.82\n0.36\n0.21\n1.39\n6\n    MQ\n9.35\n0.03\n113.70\n2322\n0.39\n0.56\n0.14\n1.10\n3\n    UA\n15.79\n0.00\n113.65\n103\n0.69\n0.00\n0.14\n0.83\n2\n    WN\n2.34\n0.02\n122.81\n59\n0.06\n0.30\n1.00\n1.36\n5\n    \n      AUS\n    \n    9E\n19.00\n0.00\n222.00\n2\n1.00\n0.00\n1.00\n2.00\n5\n    AA\n15.25\n0.02\n215.07\n365\n0.75\n1.00\n0.36\n2.11\n6\n    B6\n14.94\n0.00\n213.85\n747\n0.73\n0.24\n0.25\n1.22\n3\n    DL\n10.82\n0.01\n211.96\n357\n0.46\n0.51\n0.07\n1.04\n2\n    UA\n14.92\n0.01\n211.28\n670\n0.73\n0.54\n0.01\n1.28\n4\n    WN\n3.84\n0.01\n211.18\n298\n0.00\n0.61\n0.00\n0.61\n1\n  \n  \n  \n\n\n\n\n\nThe final ranking of the carriers for each destination is displayed below in Table 3 with names of destinations as column names, and within each column, the best carrier at the top, worst carrier at the bottom.\n\n\nCode\ndf3 |&gt;\n  arrange(dest, rank_carrier) |&gt;\n  select(dest, carrier, rank_carrier) |&gt;\n  ungroup() |&gt;\n  pivot_wider(names_from = rank_carrier,\n              values_from = carrier) |&gt;\n  t() |&gt;\n  as_tibble() |&gt;\n  janitor::row_to_names(row_number = 1) |&gt;\n  gt() |&gt;\n  sub_missing(missing_text = \"\") |&gt;\n  gtExtras::gt_theme_538()\n\n\n\n\n\n\nTable 3:  Ranks of carriers for each destination: Best carrier at top, worst\ncarrier at bottom \n  \n    \n    \n      ATL\n      AUS\n      AVL\n      BDL\n      BNA\n      BOS\n      BQN\n      BTV\n      BUF\n      BWI\n      CAE\n      CHS\n      CLE\n      CLT\n      CMH\n      CVG\n      DAY\n      DCA\n      DEN\n      DFW\n      DSM\n      DTW\n      EGE\n      FLL\n      GRR\n      GSO\n      GSP\n      HNL\n      HOU\n      IAD\n      IAH\n      IND\n      JAC\n      JAX\n      LAS\n      LAX\n      MCI\n      MCO\n      MEM\n      MHT\n      MIA\n      MKE\n      MSN\n      MSP\n      MSY\n      MVY\n      OMA\n      ORD\n      ORF\n      PBI\n      PDX\n      PHL\n      PHX\n      PIT\n      PWM\n      RDU\n      RIC\n      ROC\n      RSW\n      SAN\n      SAT\n      SDF\n      SEA\n      SFO\n      SJC\n      SJU\n      SLC\n      SRQ\n      STL\n      STT\n      SYR\n      TPA\n      TVC\n      TYS\n      XNA\n    \n  \n  \n    DL\nWN\n9E\nEV\nDL\nDL\nB6\n9E\nDL\nWN\n9E\nUA\nUA\nUS\nMQ\nDL\n9E\nUA\nDL\nEV\nEV\nDL\nUA\nDL\nEV\n9E\n9E\nHA\nB6\nUA\nUA\nUA\nDL\nDL\nVX\nDL\nDL\nDL\nDL\n9E\nAA\nWN\n9E\nUA\nUA\n9E\nUA\nUA\nEV\nUA\nDL\nDL\nDL\nUA\nDL\nUA\nEV\nB6\n9E\nDL\nUA\nUA\nAS\nDL\nVX\nDL\nDL\nEV\nDL\nDL\nB6\n9E\nMQ\nEV\nEV\n    UA\nDL\nEV\nUA\nWN\nAA\nUA\nB6\nEV\nEV\nEV\nB6\nMQ\nB6\nEV\n9E\nEV\nDL\nUA\nUA\n9E\nUA\nAA\nUA\n9E\nEV\nEV\nUA\nWN\nOO\nAA\nDL\nUA\nB6\nDL\nB6\n9E\nAA\n9E\nEV\nDL\nEV\nEV\nDL\nDL\nB6\nDL\nAA\nMQ\nDL\nUA\nYV\nUA\nDL\nB6\nB6\n9E\n9E\nB6\nB6\nDL\n9E\nDL\nUA\nB6\nB6\nB6\nB6\nWN\nAA\n9E\nEV\nEV\n9E\nMQ\n    MQ\nB6\n\n\nMQ\nUS\n\nEV\nB6\n9E\n\n9E\nOO\nMQ\n9E\nEV\n\nUS\nB6\n9E\n\nOO\n\nB6\n\n\n\n\n\nB6\n\nMQ\n\n9E\nB6\nUA\nEV\nUA\nEV\n\nUA\n9E\n\nOO\nWN\n\nEV\nEV\n9E\nB6\nB6\nUS\nUS\nMQ\nEV\nEV\n\nEV\nUA\nUA\n9E\nEV\nB6\nB6\n\nUA\n\nDL\nUA\nUA\nEV\nMQ\n\n\n\n    9E\nUA\n\n\n9E\nEV\n\n\n9E\nMQ\n\nEV\nEV\nUA\n\nMQ\n\n9E\nF9\nAA\n\nMQ\n\nAA\n\n\n\n\n\n9E\n\nEV\n\nEV\nUA\nAA\n\nB6\n\n\n\nFL\n\nMQ\nB6\n\n\nB6\n\nEV\n\nEV\nB6\nEV\n\nMQ\n\n\nDL\nAA\n\n\nUA\nVX\n\nAA\n\n9E\nAA\n\n\nUA\n\n\n\n    WN\n9E\n\n\nEV\nB6\n\n\n\n\n\n\n9E\nEV\n\n\n\nMQ\nWN\n\n\nEV\n\n\n\n\n\n\n\nEV\n\n9E\n\n\nAA\nVX\n\n\n\n\n\n\n\n9E\n9E\n\n\nMQ\n\nAA\n\n9E\nWN\n9E\n\n9E\n\n\n\n\n\n\nAA\nAA\n\n\n\n\nMQ\n\n\nDL\n\n\n\n    FL\nAA\n\n\n\nUA\n\n\n\n\n\n\n\nYV\n\n\n\nEV\n\n\n\n9E\n\n\n\n\n\n\n\nYV\n\n\n\n\n\n\n\n\n\n\n\n\n\nEV\nEV\n\n\n9E\n\n\n\n\n\nB6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEV\n\n\nAA\n\n\n\n    EV\n\n\n\n\n9E\n\n\n\n\n\n\n\n9E\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nB6"
  },
  {
    "objectID": "Chapter14.html#various-ranking-functions-in-r",
    "href": "Chapter14.html#various-ranking-functions-in-r",
    "title": "Chapter 14",
    "section": "Various Ranking functions in R",
    "text": "Various Ranking functions in R\n\n\n\n\n\n\n\n\nFunction\nDescription\nHandling Ties\n\n\n\n\nmin_rank()\nAssigns the minimum rank to each element. The way ranks are usually computed in sports.\nTied values get the same rank; next rank is skipped.\n\n\nrow_number()\nAssigns every input a unique rank. Ties are given ranks based on the order of appearance, i.e. row number.\nTied values get different ranks, depending on the order of appearance in the data set.\n\n\ndense_rank()\nAssigns the minimum rank to each element. And, assigns the same rank to tied values without gaps.\nTied values get the same rank; no gaps in ranks.\n\n\npercent_rank()\nComputes the rank as a percentage of the total number of elements. It counts the total number of values less than xi, and divides it by the number of observations minus 1.\nTied values get the same rank; no gaps in ranks.\n\n\ncume_dist()\nComputes the cumulative distribution function (CDF) of the data. It counts the total number of values less than or equal to xi, and divides it by the number of observations.\nTied values get the same rank; no gaps in ranks.\n\n\n\n\n\nCode\ndf &lt;- data.frame(Name = c(\"Alice\", \"Bob\", \"Carol\", \"David\", \"Eve\"),                  Score = c(85, 92, 85, 78, 92))   \n\n# Apply ranking functions and store the results in new columns   \ndf |&gt;      \n  mutate(Min_Rank = min_rank(Score),                    \n         Row_Number = row_number(Score),                    \n         Dense_Rank = dense_rank(Score),                    \n         Percent_Rank = percent_rank(Score),                    \n         Cumulative_Dist = cume_dist(Score)) |&gt;      \n  gt()\n\n\n\n\n\n\n\n  \n    \n    \n      Name\n      Score\n      Min_Rank\n      Row_Number\n      Dense_Rank\n      Percent_Rank\n      Cumulative_Dist\n    \n  \n  \n    Alice\n85\n2\n2\n2\n0.25\n0.6\n    Bob\n92\n4\n4\n3\n0.75\n1.0\n    Carol\n85\n2\n3\n2\n0.25\n0.6\n    David\n78\n1\n1\n1\n0.00\n0.2\n    Eve\n92\n4\n5\n3\n0.75\n1.0\n  \n  \n  \n\n\nFigure 3: Various Ranking Functions"
  },
  {
    "objectID": "Chapter14.html#question-1-3",
    "href": "Chapter14.html#question-1-3",
    "title": "Chapter 14",
    "section": "Question 1",
    "text": "Question 1\nBrainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights. When is mean() useful? When is median() useful? When might you want to use something else? Should you use arrival delay or departure delay? Why might you want to use data from planes?\nThe different ways to assess the delay characteristics of a group of flights in form of a tabular comparison of various measures of central tendency are shown below: –\n\n\n\n\n\n\n\n\n\n\n\nDefinition\nAppropriate Data Types\nRobustness to Outliers\nSuitability\n\n\n\n\nMean\nmean()\nThe sum of all values divided by the number of values.\nInterval, Ratio\nSensitive to outliers\nCommonly used for normally distributed data.\n\n\nMedian\nmedian()\nThe middle value in a dataset when values are ordered, i.e., 50th percentile.\nOrdinal, Interval, Ratio\nRobust to outliers\nSuitable for skewed data and data with outliers.\n\n\nTrimmed Mean\nmean(x, trim = 0.05)\nMean calculated after removing a certain percentage of extreme values.\nInterval, Ratio\nReduces the impact of outliers\nUseful for data containing outliers, i.e. flights with abnormally high delay\n\n\nWeighted Mean\nweighted.mean(x, weights)\nThe sum of the products of values and their associated weights divided by the sum of the weights.\nInterval, Ratio\nDepends on the weighting scheme\nAppropriate when different data points have different levels of importance. Example, if we give more importance to delay in short-haul flights, i.e., ratio of dep_delay to air_time\n\n\nInter-Quartile Range\nIQR()\nThe difference between 25th and 75th percentile values in a dataset.\nInterval, Ratio\nNot sensitive to outliers\nProvides a simple measure of central dispersion in the data.\n\n\nPercentile\nquantile()\nThe value below which a given percentage of data falls, e.g. 95th percentile\nInterval, Ratio\nNot influenced by outliers\nUseful for identifying specific positions within a dataset. We could find 95th percentile instead to maximum dep_delay to avoid data entry errors affecting our computation.\n\n\nWinsorized Mean\nDescTools::Winsorize()\nMean calculated after replacing extreme values with less extreme ones, say 5th and 95th percentile.\nInterval, Ratio\nReduces the impact of outliers\nUseful when we want to retain some information from outliers.\n\n\n\nOur choice of a measure depends on the characteristics of flights delay data and our specific analytical objectives.\n\nmean() is useful when: –\n\nWhen we want to find the typical or central value of a dataset, sensitive to all data points.\nWhen our data does not contain extreme outliers. That is, it is commonly used for normally distributed data or data that follows a symmetric bell-shaped curve.\n\nmedian() is useful when: –\n\nWhen we want to find the middle value of a dataset. We want this when our data contains extreme values and outliers, perhaps due to data entry error.\nMedian is robust to outliers and is a better choice when the data contains extreme values or is skewed.\nCaution: median() may not provide as much information about the distribution of data as the mean() does. It doesn’t consider the magnitude of differences between values.\n\nWhen to use something else:\n\nMode: The mode is useful when we want to identify the most frequently occurring value(s) in a dataset. It’s suitable for nominal data (categories or labels) and not for departure delay data.\nGeometric Mean: If we’re dealing with positive data that follows a multiplicative relationship (e.g., growth rates, financial returns), the geometric mean may be more appropriate than the arithmetic mean.\nTrimmed Mean: When dealing with data that contains outliers, we can calculate a trimmed mean by removing a certain percentage of extreme values from both ends of the dataset. This can help reduce the impact of outliers on the mean.\nWeighted Mean: If different data points have different levels of importance or weight, we can calculate a weighted mean to give more weight to specific values.\n\n\nWe should use departure delay instead of arrival delay, as most of the delay occurs only in departure. Once in air, the delay is minimal, and even if so, it is unavoidable due to safety reasons. The following table compares departure and arrival delays, and in my view, since our purpose of analysis is to compare airline carriers, we might use departure delay.\n\n\n\n\n\n\n\n\n\nDeparture Delay\nArrival Delay\n\n\n\n\nPros\nUseful for assessing punctuality in departing flights.\nImportant for passengers concerned about on-time departure.\nProvides a comprehensive view of overall travel delays.\nReflects the entire flight experience, including in-flight issues.\n\n\nCons\nDoesn’t account for delays occurring during the flight.\nLess relevant for passengers with long layovers or no connecting flights.\nMay not fully represent the passenger’s overall experience.\nDoesn’t provide specific insight into departure punctuality.\nMay not be as crucial for passengers with direct flights.\nMay not reflect the airline’s performance in terms of prompt departures.\n\n\n\nWe might use data from planes (to get flight aircraft make and year) and flights together as shown in the analysis below: –\n\n\nCode\n# Compute mean, median and trimmed mean for departure delay and delay\n# in the airtime (i.e., time lost in air calculated by arr_delay minus \n# dep_delay).\ndf1 = \nflights |&gt;\n  group_by(tailnum) |&gt;\n  summarize(\n    mean_dep_delay = mean(dep_delay, na.rm = TRUE),\n    median_dep_delay = median(dep_delay, na.rm = TRUE),\n    trim_mean_dep_delay = mean(dep_delay, na.rm = TRUE, trim = 0.025),\n    mean_air_time_loss = mean(arr_delay - dep_delay, na.rm = TRUE),\n    median_air_time_loss = median(arr_delay - dep_delay, na.rm = TRUE)\n  )\n\ndf1 = inner_join(df1, planes, by = \"tailnum\")\n\n\ndf1 |&gt;\n  group_by(engine) |&gt;\n  summarize(\n    median_delay = mean(median_dep_delay, na.rm = TRUE),\n    nos = n()\n  ) |&gt;\n  mutate(is_positive = if_else(median_delay &gt; 0, +1, -1)) |&gt;\n  ggplot(aes(x = median_delay, \n             y = engine,\n             fill = factor(is_positive),\n             label = paste0(\"Flights = \", nos))) +\n  geom_bar(stat = \"identity\") +\n  geom_text(hjust = 0.5) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(-3, 5, 1)) +\n  scale_fill_manual(values = c(\"#119644\", \"#ed523e\")) +\n  labs(x = \"Average of the Median Depature Delays of each airplane (in minutes)\",\n       y = NULL,\n       title = \"Turbo-jet aircrafts have highest departure delays on average\") +\n  theme(axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        legend.position = \"blank\",\n        panel.grid = element_blank())\n\n\n\n\n\nFigure 9: Bar plot of the average of median departure delays for each aircraft type (make)\n\n\n\n\n\n\nCode\ntop_m = df1 |&gt;\n  group_by(manufacturer) |&gt;\n  count() |&gt;\n  filter(n &gt; 100) |&gt;\n  select(manufacturer) |&gt;\n  as_vector() |&gt;\n  unname()\n\ndf1 |&gt;\n  filter(manufacturer %in% top_m) |&gt;\n  group_by(manufacturer) |&gt;\n  summarize(\n    nos = n(),\n    median_delay = mean(median_dep_delay, na.rm = TRUE)\n  ) |&gt;\n  mutate(is_positive = if_else(median_delay &gt; 0, +1, -1)) |&gt;\n  ggplot(aes(x = median_delay, \n             y = reorder(manufacturer, -median_delay),\n             fill = factor(is_positive),\n             label = paste0(\"Flights = \", nos))) +\n  geom_bar(stat = \"identity\") +\n  geom_text(position = \"fill\", hjust = 1.2) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(-4, 4, 1)) +\n  scale_fill_manual(values = c(\"#119644\", \"#ed523e\")) +\n  labs(x = \"Average of the Median Depature Delays of flights of the manufacturer's aircrafts (in minutes)\",\n       y = NULL,\n       title = \"Average Departure Delays vary by the aircrafts' manufacturers: Boeing's airplanes get delayed the most\") +\n  theme(axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        legend.position = \"blank\",\n        panel.grid = element_blank(),\n        plot.title.position = \"plot\")\n\n\n\n\n\nFigure 10: Bar plot of the average of median departure delays for each aircraft manufacturer"
  },
  {
    "objectID": "Chapter14.html#question-2-3",
    "href": "Chapter14.html#question-2-3",
    "title": "Chapter 14",
    "section": "Question 2",
    "text": "Question 2\nWhich destinations show the greatest variation in air speed?\nTo find the destinations that show the greatest variation in air speed, we can group the flights data-set by dest and then find the destinations which have the highest coefficient of variability, i.e. \\(CV = \\frac{sd}{\\text{mean}} \\cdot 100\\), expressed as a percentage. The top 5 destinations with the greatest variation in air speed are in Table 4 .\nThe \\(CV\\) is expressed as a percentage and is useful to compare the relative variability of different destinations’ air speed because the mean airspeed for each destination is different.\nThe usefulness of the coefficient of variation depends on the context and the nature of the data you are dealing with. Here are a few points to consider. A higher \\(CV\\) indicates greater relative variability compared to the mean, while a lower CV suggests less relative variability.\n\n\nCode\nflights |&gt;\n  mutate(speed = 60 * distance/air_time) |&gt;\n  group_by(dest) |&gt;\n  summarise(\n    nos = n(),\n    mean = mean(speed, na.rm = TRUE),\n    sd   = sd(speed, na.rm = TRUE),\n    CV   = sd/mean) |&gt;\n  arrange(desc(CV)) |&gt;\n  slice_head(n = 5) |&gt;\n  gt() |&gt;\n  fmt_number(columns = -nos, \n             decimals = 2) |&gt;\n  fmt_percent(columns = \"CV\") |&gt;\n  cols_label(\n    dest = \"Destination\",\n    nos = \"Number of flights\",\n    mean = \"Mean Air Speed (mph)\",\n    sd = \"Std. Dev. Air Speed\",\n    CV = \"Coeff. of Variation\"\n  ) |&gt;\n  gtExtras::gt_theme_538()\n\n\n\n\n\n\nTable 4:  The top 5 destinations with the greatest variation in air speed \n  \n    \n    \n      Destination\n      Number of flights\n      Mean Air Speed (mph)\n      Std. Dev. Air Speed\n      Coeff. of Variation\n    \n  \n  \n    PHL\n1632\n176.45\n30.66\n17.38%\n    DCA\n9705\n280.76\n34.04\n12.12%\n    BOS\n15508\n297.62\n32.57\n10.94%\n    BDL\n443\n277.01\n29.85\n10.77%\n    ACK\n265\n288.95\n30.56\n10.58%\n  \n  \n  \n\n\n\n\n\nFurther, we can go one step ahead, and find whether there is a relation between destinations and air speeds along with their variability, graphically as shown in\n\n\nCode\ndest_vec = flights |&gt;\n  group_by(dest) |&gt;\n  count() |&gt;\n  filter(n &gt; 10000) |&gt;\n  select(dest) |&gt;\n  as_vector() |&gt;\n  unname()\n  \nflights |&gt;\n  filter(dest %in% dest_vec) |&gt;\n  mutate(speed = 60 * distance/air_time) |&gt;\n  group_by(dest) |&gt;\n  mutate(m_s = mean(speed, na.rm = TRUE)) |&gt;\n  ggplot(aes(y = reorder(dest, m_s), \n             x = speed)\n         ) +\n  geom_boxplot(outlier.alpha = 0.2,\n               fill = \"#cbc6cf\") +\n  theme_minimal() +\n  labs(x = \"Air speed (miles per hour)\",\n       y = NULL, \n       title = \"Farther destinations have higher average air speed of aircrafts, and lesser variability\",\n       subtitle = \"Los Angeles bound aircrafts' average speed is much higher than Boston-bound aircrafts, but spread of data is lesser\") +\n  theme(panel.grid.major.y = element_blank(),\n        panel.grid.minor = element_blank(),\n        plot.title.position = \"plot\") +\n  scale_x_continuous(breaks = seq(100, 600, 100)) +\n  coord_cartesian(xlim = c(100, 600))\n\n\n\n\n\nFigure 11: A box-plot of the air speeds for the 10 most frequent destinations, with &gt;10,000 flights in 2013"
  },
  {
    "objectID": "Chapter14.html#question-3-2",
    "href": "Chapter14.html#question-3-2",
    "title": "Chapter 14",
    "section": "Question 3",
    "text": "Question 3\nCreate a plot to further explore the adventures of EGE. Can you find any evidence that the airport moved locations? Can you find another variable that might explain the difference?\nOnce we start exploring the data, firstly, we observe in Figure 12 and Table 5 that the flight distances of the 213 flights to EGE in 2013 cluster into two groups: one around 1725 miles and the other around 1746 miles. Thus, this is one piece of evidence that the EGE airport changed locations.\n\n\nCode\nflights |&gt;\n  filter(dest == \"EGE\") |&gt;\n  group_by(distance) |&gt;\n  count() |&gt;\n  ungroup() |&gt;\n  gt() |&gt;\n  cols_label(\n    distance = \"Distance (in miles)\",\n    n = \"Number of flights in 2013\"\n  ) |&gt;\n  gtExtras::gt_theme_538()\n\n\n\n\n\n\nTable 5:  Flight distances for flights to EGE \n  \n    \n    \n      Distance (in miles)\n      Number of flights in 2013\n    \n  \n  \n    1725\n51\n    1726\n59\n    1746\n44\n    1747\n59\n  \n  \n  \n\n\n\n\n\n\n\nCode\nflights |&gt;\n  filter(dest == \"EGE\") |&gt;\n  ggplot(aes(x = distance, y = 1)) +\n  geom_jitter(width = 0, \n              height = 0.1,\n              alpha = 0.3) +\n  theme_minimal() +\n  coord_cartesian(xlim = c(1500, 1800),\n                  ylim = c(0.8, 1.2)) +\n  scale_x_continuous(breaks = c(1500, 1600, 1700, 1725, 1745, 1800)) +\n  labs(title = \"Flight distances to EGE cluster into two groups around 1725 and 1746 miles\", \n       x = \"Flight Distance (miles)\", y = NULL) +\n  theme(panel.grid.minor = element_blank(),\n        panel.grid.major.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\n\n\n\n\nFigure 12: Scatter Plot of flight distances for the 213 flights to EGE in 2013\n\n\n\n\nNow, upon exploring further, we find another variable, i.e., origin of the flights might explain the difference since EWR and JFK are slightly away from each other, even within New York City metropolitan area. The Table 6 explains this, and we conclude that after all, EGE airport may not have shifted after all.\n\n\nCode\nflights |&gt;\n  filter(dest == \"EGE\") |&gt;\n  group_by(origin) |&gt;\n  summarise(\n    mean_of_distance = mean(distance, na.rm = TRUE),\n    std_dev_of_distance = sd(distance, na.rm = TRUE),\n    proportion_of_cancelled_flights = mean(is.na(dep_time)),\n    number_of_flights = n()\n  ) |&gt;\n  gt() |&gt;\n  fmt_percent(columns = proportion_of_cancelled_flights) |&gt;\n  fmt_number(decimals = 2,\n             columns = c(mean_of_distance, std_dev_of_distance)) |&gt;\n  cols_label_with(fn = ~ janitor::make_clean_names(., case = \"title\")) |&gt;\n  gtExtras::gt_theme_pff()\n\n\n\n\n\n\nTable 6:  Comparison of EGE-bound flights’ distance based on airport of\norigin \n  \n    \n    \n      Origin\n      Mean of Distance\n      Std Dev of Distance\n      Proportion of Cancelled Flights\n      Number of Flights\n    \n  \n  \n    EWR\n1,725.54\n0.50\n2.73%\n110\n    JFK\n1,746.57\n0.50\n1.94%\n103"
  },
  {
    "objectID": "Chapter15.html",
    "href": "Chapter15.html",
    "title": "Chapter 15",
    "section": "",
    "text": "Important uses of quoting in R: –"
  },
  {
    "objectID": "Chapter15.html#question-1",
    "href": "Chapter15.html#question-1",
    "title": "Chapter 15",
    "section": "Question 1",
    "text": "Question 1\nCreate strings that contain the following values:\n\nHe said \"That's amazing!\"\n\nx = \"He said \\\"That's amazing!\\\"\"\nstr_view(x)\n\n[1] │ He said \"That's amazing!\"\n\n\n\\a\\b\\c\\d\n\nx = \"\\\\a\\\\b\\\\c\\\\d\"\nstr_view(x)\n\n[1] │ \\a\\b\\c\\d\n\n\n\\\\\\\\\\\\\n\nx = \"\\\\\\\\\\\\\\\\\\\\\\\\\"\nstr_view(x)\n\n[1] │ \\\\\\\\\\\\"
  },
  {
    "objectID": "Chapter15.html#question-2",
    "href": "Chapter15.html#question-2",
    "title": "Chapter 15",
    "section": "Question 2",
    "text": "Question 2\nCreate the string in your R session and print it. What happens to the special “\\u00a0”? How does str_view() display it? Can you do a little googling to figure out what this special character is?\nx &lt;- \"This\\u00a0is\\u00a0tricky\"\nThe \"\\u00a0\" represents a white space. By google, I find out that this represents No-Break Space (NBSP). But, str_view() displays it in form of a greenish-blue font {\\u00a0}.\n\n\"\\u00a0\" # This represents a white space\n\n[1] \" \"\n\nstr_view(\"\\u00a0\")\n\n[1] │ {\\u00a0}\n\nx &lt;- \"This\\u00a0is\\u00a0tricky\"\nprint(x)\n\n[1] \"This is tricky\"\n\nstr_view(x)\n\n[1] │ This{\\u00a0}is{\\u00a0}tricky\n\n\nThe \"\\u00a0\" represents a non-breaking space character in Unicode encoding. Unicode is a standardized character encoding system that assigns a unique numerical code to almost every character from every writing system in the world, including various symbols, letters, and special characters.\nIn Unicode, “\\u” is used to indicate that the following four characters represent a Unicode code point in hexadecimal notation. In this case, \"\\u00a0\" represents the code point for the non-breaking space character.\nA non-breaking space is a type of space character that is used in typography and word processing to prevent a line break or word wrap from occurring at that particular space.\nIt is similar to a regular space character (ASCII code 32), but it has the special property of keeping adjacent words or characters together on the same line when text is justified or formatted."
  },
  {
    "objectID": "Chapter15.html#question-1-1",
    "href": "Chapter15.html#question-1-1",
    "title": "Chapter 15",
    "section": "Question 1",
    "text": "Question 1\nCompare and contrast the results of paste0() with str_c() for the following inputs:\nstr_c(\"hi \", NA) \nstr_c(letters[1:2], letters[1:3])\nAs we can see below, paste0 converts NA into a string \"NA\" and simply joins it with another string. However, str_c() behaves more sensibly - it generates NA if any of the strings being joined is NA.\n\nstr_c(\"hi \", NA)\n\n[1] NA\n\npaste0(\"hi \", NA)\n\n[1] \"hi NA\"\n\n\nFurther, we see below that we are joining two string vectors of unequal length, i.e., letters[1:2] is \"a\" \"b\" and letters[1:3] is \"a\" \"b\" \"c\" , both str_c() and paste0() behave differently.\n\nstr_c() throws an error and informs us that the string vectors being joined are of unequal length.\npaste0 simple recycles the shorter string vector silently.\n\n\n# str_c(letters[1:2], letters[1:3])\npaste0(letters[1:2], letters[1:3])\n\n[1] \"aa\" \"bb\" \"ac\""
  },
  {
    "objectID": "Chapter15.html#question-2-1",
    "href": "Chapter15.html#question-2-1",
    "title": "Chapter 15",
    "section": "Question 2",
    "text": "Question 2\nWhat’s the difference between paste() and paste0()? How can you recreate the equivalent of paste() with str_c()?\nIn R, both paste() and paste0() functions are used to concatenate strings together. However, they differ in how they handle separating the concatenated elements.\npaste() concatenates its arguments with a space character as the default separator. We can specify a different separator using the sep argument.\npaste0() is similar to paste(), but it does not add any separator between the concatenated elements. It simply combines them as-is.\nHere is an example: –\n\nvec1 &lt;- c(\"Hello\", \"Hi\")\nvec2 &lt;- c(\"Amy\", \"Tom\", \"Neal\")\npaste(vec1, vec2)\n\n[1] \"Hello Amy\"  \"Hi Tom\"     \"Hello Neal\"\n\npaste(vec1, vec2, sep = \", \")\n\n[1] \"Hello, Amy\"  \"Hi, Tom\"     \"Hello, Neal\"\n\npaste0(vec1, vec2)\n\n[1] \"HelloAmy\"  \"HiTom\"     \"HelloNeal\"\n\n\nWe can recreate the equivalent of paste() using the str_c() function from the stringr package in R. To do this, we can specify the separator using the sep argument in str_c() as follows: –\n\nvec1 &lt;- c(vec1, \"Hallo\")\npaste(vec1, vec2)\n\n[1] \"Hello Amy\"  \"Hi Tom\"     \"Hallo Neal\"\n\nstr_c(vec1, vec2, sep = \" \")\n\n[1] \"Hello Amy\"  \"Hi Tom\"     \"Hallo Neal\"\n\n\nNote: We had to add a string to vec1 so that both vec1 and vec2 are of length 3. Else, str_c will throw up an error."
  },
  {
    "objectID": "Chapter15.html#question-3",
    "href": "Chapter15.html#question-3",
    "title": "Chapter 15",
    "section": "Question 3",
    "text": "Question 3\nConvert the following expressions from str_c() to str_glue() or vice versa:\n\nstr_c(\"The price of \", food, \" is \", price)\nstr_glue(\"The price of {food} is {price}\")\nstr_glue(\"I'm {age} years old and live in {country}\")\nstr_c(\"I'm \", age, \" years old and live in \", country)\nstr_c(\"\\\\section{\", title, \"}\")\nstr_glue(\"\\\\\\\\section{{{title}}}\")\n\nasd\nfind the distribution of lengths of US babynames and then with filter() to look at the longest names, which happen to have 15 letters\n\ndata(\"babynames\")\nbabynames |&gt;\n  mutate(name_lgth = str_length(name)) |&gt;\n  count(name_lgth, wt = n)\n\n# A tibble: 14 × 2\n   name_lgth        n\n       &lt;int&gt;    &lt;int&gt;\n 1         2   338150\n 2         3  8589596\n 3         4 48506739\n 4         5 87011607\n 5         6 90749404\n 6         7 72120767\n 7         8 25404066\n 8         9 11926551\n 9        10  1306159\n10        11  2135827\n11        12    16295\n12        13    10845\n13        14     3681\n14        15      830\n\nbabynames |&gt;\n  filter(str_length(name) == 15) |&gt;\n  count(name, wt = n, sort = TRUE) |&gt;\n  slice_head(n = 5) |&gt;\n  select(name) |&gt;\n  as_vector() |&gt;\n  unname() |&gt;\n  str_sub(start = -3, end = -1)\n\n[1] \"ier\" \"ohn\" \"her\" \"ame\" \"ich\""
  },
  {
    "objectID": "Chapter15.html#question-1-2",
    "href": "Chapter15.html#question-1-2",
    "title": "Chapter 15",
    "section": "Question 1",
    "text": "Question 1\nWhen computing the distribution of the length of babynames, why did we use wt = n?\nThe babynames data-set (Table 1) displays the column n to reflect the frequency, i.e., number of observations of that name in that year. Thus, when we are computing the distribution of the length of baby names (Table 2), we need to weigh the observations by n otherwise each row will be treated as 1 (Table 2 column 3), instead of the actual number reflected in n leading to erroneous results.\n\n\nCode\nbabynames |&gt;\n  slice_head(n = 5) |&gt;\n  gt() |&gt;\n  fmt_number(prop, decimals = 4)\n\n\n\n\n\n\nTable 1:  The babynames data-set \n  \n    \n    \n      year\n      sex\n      name\n      n\n      prop\n    \n  \n  \n    1880\nF\nMary\n7065\n0.0724\n    1880\nF\nAnna\n2604\n0.0267\n    1880\nF\nEmma\n2003\n0.0205\n    1880\nF\nElizabeth\n1939\n0.0199\n    1880\nF\nMinnie\n1746\n0.0179\n  \n  \n  \n\n\n\n\n\n\n\nCode\ndf1 = babynames |&gt;\n  mutate(name_length = str_length(name)) |&gt;\n  count(name_length, wt = n) |&gt;\n  rename(correct_frequency = n)\n\ndf2 = babynames |&gt;\n  mutate(name_length = str_length(name)) |&gt;\n  count(name_length) |&gt;\n  rename(wrong_frequency_without_weights = n)\n\ninner_join(df1, df2, by = \"name_length\") |&gt;\n  gt() |&gt;\n  fmt_number(-name_length , decimals = 0) |&gt;\n  cols_label_with(\n    fn = ~ janitor::make_clean_names(., case = \"title\")\n  ) |&gt;\n  gt_theme_538()\n\n\n\n\n\n\nTable 2:  The distribution of the length of babynames \n  \n    \n    \n      Name Length\n      Correct Frequency\n      Wrong Frequency without Weights\n    \n  \n  \n    2\n338,150\n4,660\n    3\n8,589,596\n41,274\n    4\n48,506,739\n177,838\n    5\n87,011,607\n404,291\n    6\n90,749,404\n546,519\n    7\n72,120,767\n424,360\n    8\n25,404,066\n213,803\n    9\n11,926,551\n78,946\n    10\n1,306,159\n23,437\n    11\n2,135,827\n6,461\n    12\n16,295\n1,610\n    13\n10,845\n946\n    14\n3,681\n390\n    15\n830\n130"
  },
  {
    "objectID": "Chapter15.html#question-2-2",
    "href": "Chapter15.html#question-2-2",
    "title": "Chapter 15",
    "section": "Question 2",
    "text": "Question 2\nUse str_length() and str_sub() to extract the middle letter from each baby name. What will you do if the string has an even number of characters?\nThe code displayed below extracts the middle letter from each baby name, and the results for first 10 names are displayed in Table 3 . If the string has an even number of characters, we can pick the middle two characters.\n\ndf3 = babynames |&gt;\n  mutate(\n    name_length = str_length(name),\n    middle_letter_start = if_else(name_length %% 2 == 0,\n                             name_length/2,\n                             (name_length/2) + 0.5),\n    middle_letter_end = if_else(name_length %% 2 == 0,\n                                (name_length/2) + 1,\n                                (name_length/2) + 0.5),\n    middle_letter = str_sub(name, \n                            start = middle_letter_start, \n                            end = middle_letter_end)\n  ) |&gt;\n  select(-c(year, sex, n, prop)) |&gt;\n  slice_head(n = 10)\n\ndf3 |&gt;\n  gt() |&gt;\n  cols_label_with(fn = ~ janitor::make_clean_names(., case = \"title\")) |&gt;\n  cols_align(align = \"center\",\n             columns = -name) |&gt;\n  gt_theme_538()\n\n\n\n\n\nTable 3:  Middle letters of names \n  \n    \n    \n      Name\n      Name Length\n      Middle Letter Start\n      Middle Letter End\n      Middle Letter\n    \n  \n  \n    Mary\n4\n2\n3\nar\n    Anna\n4\n2\n3\nnn\n    Emma\n4\n2\n3\nmm\n    Elizabeth\n9\n5\n5\na\n    Minnie\n6\n3\n4\nnn\n    Margaret\n8\n4\n5\nga\n    Ida\n3\n2\n2\nd\n    Alice\n5\n3\n3\ni\n    Bertha\n6\n3\n4\nrt\n    Sarah\n5\n3\n3\nr"
  },
  {
    "objectID": "Chapter15.html#question-3-1",
    "href": "Chapter15.html#question-3-1",
    "title": "Chapter 15",
    "section": "Question 3",
    "text": "Question 3\nAre there any major trends in the length of baby names over time? What about the popularity of first and last letters?\nThe Figure 1, Figure 2 and Figure 3 show the trends over time.\n\n\nCode\ndf4 = babynames |&gt;\n  mutate(\n    name_length = str_length(name),\n    name_start = str_sub(name, 1, 1),\n    name_end = str_sub(name, -1, -1)\n  )\ny_coord = c(5.4, 6.3)\ndf4 |&gt;\n  group_by(year) |&gt;\n  count(name_length, wt = n) |&gt;\n  summarise(mean_length = weighted.mean(name_length, w = n)) |&gt;\n  ggplot(aes(x = year, y = mean_length)) +\n  theme_classic() +\n  labs(y = \"Average name length (for each year)\",\n       x = \"Year\", \n       title = \"Baby names have become longer over the past 12 decades\",\n       subtitle = \"Between 1890-1920, and 1960-1990 baby names became longer\\nBut, since 1990 the names are becoming shorter again\") +\n  scale_x_continuous(breaks = seq(1880, 2000, 20)) +\n  geom_rect(mapping = aes(xmin = 1890, xmax = 1920,\n                          ymin = y_coord[1], ymax = y_coord[2]),\n            alpha = 0.01, fill = \"grey\") +\n  geom_rect(mapping = aes(xmin = 1960, xmax = 1990,\n                          ymin = y_coord[1], ymax = y_coord[2]),\n            alpha = 0.01, fill = \"grey\") +\n  geom_line(lwd = 1) +\n  coord_cartesian(ylim = y_coord) +\n  theme(plot.title.position = \"plot\")\n\n\n\n\n\nFigure 1: Length of babynames over time\n\n\n\n\n\n\nCode\nns_vec = df4 |&gt;\n  count(name_start, wt = n, sort = TRUE) |&gt;\n  slice_head(n = 5) |&gt;\n  select(name_start) |&gt;\n  as_vector() |&gt;\n  unname()\n\ndf4 |&gt;\n  filter(name_start %in% ns_vec) |&gt;\n  group_by(year) |&gt;\n  count(name_start, wt = n) |&gt;\n  mutate(prop = 100*n/sum(n)) |&gt;\n  mutate(lbl = if_else(year == 2017, \n                       name_start, \n                       NA)) |&gt;\n  ggplot(aes(x = year, y = prop, \n             col = name_start, label = lbl)) +\n  geom_line(lwd = 1) +\n  ggrepel::geom_label_repel(nudge_x = 1) +\n  labs(x = \"Year\",\n       y = \"Percentage of names starting with character\",\n       title = \"People's preferences for baby names' starting letter change over time\",\n       subtitle = \"Names starting with A are most popular now\\nNames starting with J were popular in the 1940s\\nIn 1950s, names starting with D became popular, while those starting with A lost popularity\") +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title.position = \"plot\") +\n  scale_x_continuous(breaks = seq(1880, 2020, 20))\n\n\n\n\n\nFigure 2: Trends on the starting letter of babynames over time\n\n\n\n\n\n\nCode\nns_vec = df4 |&gt;\n  count(name_end, wt = n, sort = TRUE) |&gt;\n  slice_head(n = 5) |&gt;\n  select(name_end) |&gt;\n  as_vector() |&gt;\n  unname()\n\ndf4 |&gt;\n  filter(name_end %in% ns_vec) |&gt;\n  group_by(year) |&gt;\n  count(name_end, wt = n) |&gt;\n  mutate(prop = 100*n/sum(n)) |&gt;\n  mutate(lbl = if_else(year == 2017, \n                       name_end, \n                       NA)) |&gt;\n  ggplot(aes(x = year, y = prop, \n             col = name_end, label = lbl)) +\n  geom_line(lwd = 1) +\n  ggrepel::geom_label_repel(nudge_x = 1) +\n  labs(x = \"Year\",\n       y = \"Percentage of names ending with character\",\n       title = \"People's preferences for baby names' ending letter change over time\",\n       subtitle = \"Names ending in N have risen in popularity over the decades.\\nNames ending with E have become less popular over time\") +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title.position = \"plot\") +\n  scale_x_continuous(breaks = seq(1880, 2020, 20))\n\n\n\n\n\nFigure 3: Trends on the ending letter of babynames over time"
  },
  {
    "objectID": "Chapter16.html",
    "href": "Chapter16.html",
    "title": "Chapter 16",
    "section": "",
    "text": "library(tidyverse)\nlibrary(babynames)\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(janitor)"
  },
  {
    "objectID": "Chapter16.html#question-1",
    "href": "Chapter16.html#question-1",
    "title": "Chapter 16",
    "section": "Question 1",
    "text": "Question 1\nWhat baby name has the most vowels? What name has the highest proportion of vowels? (Hint: what is the denominator?)\nThe code below shows the analysis. The answers are: —\n\nThe baby names with most vowels, i.e., 8 of them are Mariadelrosario and Mariaguadalupe.\nThe baby names with highest proportion of vowels, i.e. 1 (they are entirely composed of vowels) are Ai, Aia, Aoi, Ea, Eua, Ia, Ii and Io.\n\n\nb1 = babynames |&gt;\n  mutate(\n    nos_vowels = str_count(name, pattern = \"[AEIOUaeiou]\"),\n    name_length = str_length(name),\n    prop_vowels = nos_vowels / name_length\n  )\n  \nb1 |&gt; \n  group_by(name) |&gt;\n  summarise(nos_vowels = mean(nos_vowels)) |&gt;\n  arrange(desc(nos_vowels)) |&gt;\n  slice_head(n = 5)\n\n# A tibble: 5 × 2\n  name            nos_vowels\n  &lt;chr&gt;                &lt;dbl&gt;\n1 Mariadelrosario          8\n2 Mariaguadalupe           8\n3 Aaliyahmarie             7\n4 Abigailmarie             7\n5 Aliciamarie              7\n\nb1 |&gt; \n  group_by(name) |&gt;\n  summarise(prop_vowels = mean(prop_vowels)) |&gt;\n  filter(prop_vowels == 1) |&gt;\n  select(name) |&gt;\n  as_vector() |&gt;\n  str_flatten(collapse = \", \", last = \" and \")\n\n[1] \"Ai, Aia, Aoi, Ea, Eua, Ia, Ii and Io\""
  },
  {
    "objectID": "Chapter16.html#question-2",
    "href": "Chapter16.html#question-2",
    "title": "Chapter 16",
    "section": "Question 2",
    "text": "Question 2\nReplace all forward slashes in \"a/b/c/d/e\" with backslashes. What happens if you attempt to undo the transformation by replacing all backslashes with forward slashes? (We’ll discuss the problem very soon.)\nThe following code replaces the “forward slashes” with “backward slashes”: —\n\ntest_string = \"a/b/c/d/e\"\nstr_replace_all(test_string,\n                pattern = \"/\",\n                replacement = \"\\\\\\\\\") |&gt;\n  str_view()\n\n[1] │ a\\b\\c\\d\\e\n\n\nFurther, when we try to do the same in reverse, there is an error because “\\” is an escape character. Thus, we need to add four \\ to include one in the final output: —\n\n# test_string2 = \"a\\b\\c\\d\\e\" throws an error because \\ is an escape character\n# Thus, we need to add four \\ to include one in the final output.\n\ntest_string2 = str_replace_all(test_string,\n                pattern = \"/\",\n                replacement = \"\\\\\\\\\")\ntest_string2 |&gt;\n  str_replace_all(pattern = \"\\\\\\\\\",\n                  replace = \"/\") |&gt;\n  str_view()\n\n[1] │ a/b/c/d/e"
  },
  {
    "objectID": "Chapter16.html#question-3",
    "href": "Chapter16.html#question-3",
    "title": "Chapter 16",
    "section": "Question 3",
    "text": "Question 3\nImplement a simple version of str_to_lower() using str_replace_all().\nThe following code implements str_to_lower() using str_replace_all() function: —\n\ntest_string3 = \"Take The Match And Strike It Against Your Shoe.\"\n\nstr_replace_all(test_string3,\n                pattern = \"[A-Z|a-z]\",\n                replacement = tolower)\n\n[1] \"take the match and strike it against your shoe.\""
  },
  {
    "objectID": "Chapter16.html#question-4",
    "href": "Chapter16.html#question-4",
    "title": "Chapter 16",
    "section": "Question 4",
    "text": "Question 4\nCreate a regular expression that will match telephone numbers as commonly written in your country.\nThe regular expressions used in the code below can match USA format(s) telephone numbers and generate clean data into columns in Table 1: —\n\ntelephone_numbers = c(\n  \"555-123-4567\",\n  \"(555) 555-7890\",\n  \"888-555-4321\",\n  \"(123) 456-7890\",\n  \"555-987-6543\",\n  \"(555) 123-7890\"\n)\n\ntelephone_numbers |&gt;\n  str_replace(\" \", \"-\") |&gt;\n  str_replace(\"\\\\(\", \"\") |&gt;\n  str_replace(\"\\\\)\", \"\") |&gt;\n  as_tibble() |&gt;\n  separate_wider_regex(\n    cols = value,\n    patterns = c(\n      area_code = \"[0-9]+\",\n      \"-| \",\n      exchange_code = \"[0-9]+\",\n      \"-| \",\n      line_number = \"[0-9]+\"\n    )\n  ) |&gt;\n  gt() |&gt;\n  gtExtras::gt_theme_538() |&gt;\n  cols_label_with(fn = ~ janitor::make_clean_names(., case = \"title\"))\n\n\n\n\n\nTable 1:  Use of regex to match USA telephone numbers \n  \n    \n    \n      Area Code\n      Exchange Code\n      Line Number\n    \n  \n  \n    555\n123\n4567\n    555\n555\n7890\n    888\n555\n4321\n    123\n456\n7890\n    555\n987\n6543\n    555\n123\n7890\n  \n  \n  \n\n\n\n\n\nFurther, telephone numbers in the USA are commonly written in several formats, including:\n\n(123) 456-7890\n123-456-7890\n123.456.7890\n1234567890\n\nYou can use the following regular expression to match these commonly written telephone number formats:\n^(\\(\\d{3}\\)\\s*|\\d{3}[-.]?)\\d{3}[-.]?\\d{4}$\nExplanation of the regular expression:\n\n^ and $ match the start and end of the string, ensuring that the entire string is matched.\n(\\(\\d{3}\\)\\s*|\\d{3}[-.]?) matches the area code, which can be enclosed in parentheses or separated by a hyphen or a period. It uses the | (OR) operator to allow either format.\n\n\\(\\d{3}\\) matches a three-digit area code enclosed in parentheses.\n\\s* matches zero or more whitespace characters (in case there’s space between the closing parenthesis and the next part of the number).\n| is the OR operator.\n\\d{3}[-.]? matches a three-digit area code followed by an optional hyphen or period.\n\n\\d{3}[-.]?\\d{4} matches the main part of the phone number, which is three digits followed by an optional hyphen or period, and then four more digits."
  },
  {
    "objectID": "Chapter16.html#question-1-1",
    "href": "Chapter16.html#question-1-1",
    "title": "Chapter 16",
    "section": "Question 1",
    "text": "Question 1\nHow would you match the literal string \"'\\? How about \"$^$\"?\nTo match the literal string \"'\\ in R using the stringr package, we need to escape the special characters in our regular expression pattern. Thus, the matching pattern becomes \\\"\\'\\\\\\\\ . Here’s an example:\n\n# The string you want to match\ninput_string &lt;- \"\\\"'\\\\\"\nstr_view(input_string)\n\n[1] │ \"'\\\n\n# Pattern to match the literal string\nmatch_pattern &lt;- \"\\\"\\'\\\\\\\\\"\nstr_view(match_pattern)\n\n[1] │ \"'\\\\\n\n# Use str_detect to check if the string contains the pattern\nif (str_detect(input_string, match_pattern)) {\n  print(\"Pattern found in the input string.\")\n} else {\n  print(\"Pattern not found in the input string.\")\n}\n\n[1] \"Pattern found in the input string.\"\n\n\nTo match the literal string \"$^$\" in R using the stringr package, we need to escape the special characters in our regular expression pattern. Thus, the matching pattern becomes . Here’s an example:\n\n# The string you want to match\ninput_string &lt;- \"\\\"$^$\\\"\"\nstr_view(input_string)\n\n[1] │ \"$^$\"\n\n# Pattern to match the literal string\nmatch_pattern &lt;- \"\\\"\\\\$\\\\^\\\\$\\\"\"\nstr_view(match_pattern)\n\n[1] │ \"\\$\\^\\$\"\n\n# Use str_detect to check if the string contains the pattern\nif (str_detect(input_string, match_pattern)) {\n  print(\"Pattern found in the input string.\")\n} else {\n  print(\"Pattern not found in the input string.\")\n}\n\n[1] \"Pattern found in the input string.\""
  },
  {
    "objectID": "Chapter16.html#question-2-1",
    "href": "Chapter16.html#question-2-1",
    "title": "Chapter 16",
    "section": "Question 2",
    "text": "Question 2\nExplain why each of these patterns don’t match a \\: \"\\\", \"\\\\\", \"\\\\\\\".\nEach of the patterns you provided does not match a single backslash \\ for the following reasons: —\n\n\"\\\" - This pattern does not match a single backslash because the backslash is an escape character in regular expressions. In most regular expression engines, a single backslash is used to escape special characters. So, when you use \"\\\" alone, it is interpreted as an escape character, and it doesn’t match a literal backslash in the input string.\n\"\\\\\" - This pattern also does not match a single backslash. It may seem like it should work because you’re escaping the backslash with another backslash, but in many regular expression engines, \"\\\\\" represents a literal backslash when you’re defining the regular expression. However, when applied to the input string, it’s still interpreted as a single backslash.\n\"\\\\\\\" - This pattern does not match a single backslash for the same reason as the previous ones. The combination \"\\\\\\\" is treated as a literal backslash in the regular expression definition, but when applied to the input string, it’s still interpreted as a single backslash, and the extra \"\\\" followed by a quotation mark is not part of the pattern.\n\nTo match a single backslash \"\\\", you would typically need to use four backslashes in the regular expression pattern, \"\\\\\\\\\" . This way, the first two backslashes represent a literal backslash, and the next two backslashes escape each other, resulting in a pattern that matches a single backslash in the input string."
  },
  {
    "objectID": "Chapter16.html#question-3-1",
    "href": "Chapter16.html#question-3-1",
    "title": "Chapter 16",
    "section": "Question 3",
    "text": "Question 3\nGiven the corpus of common words in stringr::words, create regular expressions that find all words that:\n\nStart with “y”.\n\nwords |&gt;\n  str_view(pattern = \"^y\")\n\n[975] │ &lt;y&gt;ear\n[976] │ &lt;y&gt;es\n[977] │ &lt;y&gt;esterday\n[978] │ &lt;y&gt;et\n[979] │ &lt;y&gt;ou\n[980] │ &lt;y&gt;oung\n\n\nDon’t start with “y”.\n\n# To view words not sarting with a y\nwords |&gt;\n  str_view(pattern = \"^(?!y)\")\n\n [1] │ &lt;&gt;a\n [2] │ &lt;&gt;able\n [3] │ &lt;&gt;about\n [4] │ &lt;&gt;absolute\n [5] │ &lt;&gt;accept\n [6] │ &lt;&gt;account\n [7] │ &lt;&gt;achieve\n [8] │ &lt;&gt;across\n [9] │ &lt;&gt;act\n[10] │ &lt;&gt;active\n[11] │ &lt;&gt;actual\n[12] │ &lt;&gt;add\n[13] │ &lt;&gt;address\n[14] │ &lt;&gt;admit\n[15] │ &lt;&gt;advertise\n[16] │ &lt;&gt;affect\n[17] │ &lt;&gt;afford\n[18] │ &lt;&gt;after\n[19] │ &lt;&gt;afternoon\n[20] │ &lt;&gt;again\n... and 954 more\n\n# Check the number of such words\nwords |&gt;\n  str_view(pattern = \"^(?!y)\") |&gt;\n  length()\n\n[1] 974\n\n# Check the number of words starting with y and total number of words\n# to confirm the matter\nwords |&gt; length()\n\n[1] 980\n\n\nEnd with “x”.\n\nTo display these, we can use the regular expression \"x$\"\n\n\nwords |&gt;\n  str_view(pattern = \"x$\")\n\n[108] │ bo&lt;x&gt;\n[747] │ se&lt;x&gt;\n[772] │ si&lt;x&gt;\n[841] │ ta&lt;x&gt;\n\n\nAre exactly three letters long. (Don’t cheat by using str_length()!)\nTo display these, we can use the regular expression \"\\\\b\\\\w{3}\\\\b\", where: –\n\n\\\\b is a word boundary anchor, ensuring that the matched word is exactly three letters long and not part of a longer word.\n\\\\w{3} matches exactly three word characters (letters).\nstr_subset is used to find all words in the dataset that match the specified regular expression pattern.\n\n\n# Finding letters exactly three letters long using regex\nwords |&gt;\n  str_subset(pattern = \"\\\\b\\\\w{3}\\\\b\")\n\n  [1] \"act\" \"add\" \"age\" \"ago\" \"air\" \"all\" \"and\" \"any\" \"arm\" \"art\" \"ask\" \"bad\"\n [13] \"bag\" \"bar\" \"bed\" \"bet\" \"big\" \"bit\" \"box\" \"boy\" \"bus\" \"but\" \"buy\" \"can\"\n [25] \"car\" \"cat\" \"cup\" \"cut\" \"dad\" \"day\" \"die\" \"dog\" \"dry\" \"due\" \"eat\" \"egg\"\n [37] \"end\" \"eye\" \"far\" \"few\" \"fit\" \"fly\" \"for\" \"fun\" \"gas\" \"get\" \"god\" \"guy\"\n [49] \"hit\" \"hot\" \"how\" \"job\" \"key\" \"kid\" \"lad\" \"law\" \"lay\" \"leg\" \"let\" \"lie\"\n [61] \"lot\" \"low\" \"man\" \"may\" \"mrs\" \"new\" \"non\" \"not\" \"now\" \"odd\" \"off\" \"old\"\n [73] \"one\" \"out\" \"own\" \"pay\" \"per\" \"put\" \"red\" \"rid\" \"run\" \"say\" \"see\" \"set\"\n [85] \"sex\" \"she\" \"sir\" \"sit\" \"six\" \"son\" \"sun\" \"tax\" \"tea\" \"ten\" \"the\" \"tie\"\n [97] \"too\" \"top\" \"try\" \"two\" \"use\" \"war\" \"way\" \"wee\" \"who\" \"why\" \"win\" \"yes\"\n[109] \"yet\" \"you\"\n\n# Finding letters exactly three letters long using str_length()\nthree_let_words = str_length(words) == 3\nwords[three_let_words]\n\n  [1] \"act\" \"add\" \"age\" \"ago\" \"air\" \"all\" \"and\" \"any\" \"arm\" \"art\" \"ask\" \"bad\"\n [13] \"bag\" \"bar\" \"bed\" \"bet\" \"big\" \"bit\" \"box\" \"boy\" \"bus\" \"but\" \"buy\" \"can\"\n [25] \"car\" \"cat\" \"cup\" \"cut\" \"dad\" \"day\" \"die\" \"dog\" \"dry\" \"due\" \"eat\" \"egg\"\n [37] \"end\" \"eye\" \"far\" \"few\" \"fit\" \"fly\" \"for\" \"fun\" \"gas\" \"get\" \"god\" \"guy\"\n [49] \"hit\" \"hot\" \"how\" \"job\" \"key\" \"kid\" \"lad\" \"law\" \"lay\" \"leg\" \"let\" \"lie\"\n [61] \"lot\" \"low\" \"man\" \"may\" \"mrs\" \"new\" \"non\" \"not\" \"now\" \"odd\" \"off\" \"old\"\n [73] \"one\" \"out\" \"own\" \"pay\" \"per\" \"put\" \"red\" \"rid\" \"run\" \"say\" \"see\" \"set\"\n [85] \"sex\" \"she\" \"sir\" \"sit\" \"six\" \"son\" \"sun\" \"tax\" \"tea\" \"ten\" \"the\" \"tie\"\n [97] \"too\" \"top\" \"try\" \"two\" \"use\" \"war\" \"way\" \"wee\" \"who\" \"why\" \"win\" \"yes\"\n[109] \"yet\" \"you\"\n\n# Checking results\nwords[three_let_words] |&gt;\n  length()\n\n[1] 110\n\nwords |&gt;\n  str_view(pattern = \"\\\\b\\\\w{3}\\\\b\") |&gt;\n  length()\n\n[1] 110\n\n\nHave seven letters or more.\nTo display these, we can use the regular expression \"\\\\b\\\\w{7,}\\\\b\" , where: –\n\n\\\\b is a word boundary anchor to ensure that the matched word has no additional characters before or after it.\n\\\\w{7,} matches words with 7 or more word characters (letters).\nstr_subset is used to filter the words in the dataset based on the regular expression pattern, so it selects words with 7 letters or more.\n\n\nwords |&gt;\n  str_subset(pattern = \"\\\\b\\\\w{7,}\\\\b\")\n\n  [1] \"absolute\"    \"account\"     \"achieve\"     \"address\"     \"advertise\"  \n  [6] \"afternoon\"   \"against\"     \"already\"     \"alright\"     \"although\"   \n [11] \"america\"     \"another\"     \"apparent\"    \"appoint\"     \"approach\"   \n [16] \"appropriate\" \"arrange\"     \"associate\"   \"authority\"   \"available\"  \n [21] \"balance\"     \"because\"     \"believe\"     \"benefit\"     \"between\"    \n [26] \"brilliant\"   \"britain\"     \"brother\"     \"business\"    \"certain\"    \n [31] \"chairman\"    \"character\"   \"Christmas\"   \"colleague\"   \"collect\"    \n [36] \"college\"     \"comment\"     \"committee\"   \"community\"   \"company\"    \n [41] \"compare\"     \"complete\"    \"compute\"     \"concern\"     \"condition\"  \n [46] \"consider\"    \"consult\"     \"contact\"     \"continue\"    \"contract\"   \n [51] \"control\"     \"converse\"    \"correct\"     \"council\"     \"country\"    \n [56] \"current\"     \"decision\"    \"definite\"    \"department\"  \"describe\"   \n [61] \"develop\"     \"difference\"  \"difficult\"   \"discuss\"     \"district\"   \n [66] \"document\"    \"economy\"     \"educate\"     \"electric\"    \"encourage\"  \n [71] \"english\"     \"environment\" \"especial\"    \"evening\"     \"evidence\"   \n [76] \"example\"     \"exercise\"    \"expense\"     \"experience\"  \"explain\"    \n [81] \"express\"     \"finance\"     \"fortune\"     \"forward\"     \"function\"   \n [86] \"further\"     \"general\"     \"germany\"     \"goodbye\"     \"history\"    \n [91] \"holiday\"     \"hospital\"    \"however\"     \"hundred\"     \"husband\"    \n [96] \"identify\"    \"imagine\"     \"important\"   \"improve\"     \"include\"    \n[101] \"increase\"    \"individual\"  \"industry\"    \"instead\"     \"interest\"   \n[106] \"introduce\"   \"involve\"     \"kitchen\"     \"language\"    \"machine\"    \n[111] \"meaning\"     \"measure\"     \"mention\"     \"million\"     \"minister\"   \n[116] \"morning\"     \"necessary\"   \"obvious\"     \"occasion\"    \"operate\"    \n[121] \"opportunity\" \"organize\"    \"original\"    \"otherwise\"   \"paragraph\"  \n[126] \"particular\"  \"pension\"     \"percent\"     \"perfect\"     \"perhaps\"    \n[131] \"photograph\"  \"picture\"     \"politic\"     \"position\"    \"positive\"   \n[136] \"possible\"    \"practise\"    \"prepare\"     \"present\"     \"pressure\"   \n[141] \"presume\"     \"previous\"    \"private\"     \"probable\"    \"problem\"    \n[146] \"proceed\"     \"process\"     \"produce\"     \"product\"     \"programme\"  \n[151] \"project\"     \"propose\"     \"protect\"     \"provide\"     \"purpose\"    \n[156] \"quality\"     \"quarter\"     \"question\"    \"realise\"     \"receive\"    \n[161] \"recognize\"   \"recommend\"   \"relation\"    \"remember\"    \"represent\"  \n[166] \"require\"     \"research\"    \"resource\"    \"respect\"     \"responsible\"\n[171] \"saturday\"    \"science\"     \"scotland\"    \"secretary\"   \"section\"    \n[176] \"separate\"    \"serious\"     \"service\"     \"similar\"     \"situate\"    \n[181] \"society\"     \"special\"     \"specific\"    \"standard\"    \"station\"    \n[186] \"straight\"    \"strategy\"    \"structure\"   \"student\"     \"subject\"    \n[191] \"succeed\"     \"suggest\"     \"support\"     \"suppose\"     \"surprise\"   \n[196] \"telephone\"   \"television\"  \"terrible\"    \"therefore\"   \"thirteen\"   \n[201] \"thousand\"    \"through\"     \"thursday\"    \"together\"    \"tomorrow\"   \n[206] \"tonight\"     \"traffic\"     \"transport\"   \"trouble\"     \"tuesday\"    \n[211] \"understand\"  \"university\"  \"various\"     \"village\"     \"wednesday\"  \n[216] \"welcome\"     \"whether\"     \"without\"     \"yesterday\"  \n\n\nContain a vowel-consonant pair.\nTo display these, we can use the regular expression \"[aeiou][^aeiou]\" , where: –\n\n[aeiou] matches any vowel (a, e, i, o, or u).\n[^aeiou] matches any character that is not a vowel, which ensures that there’s a consonant after the vowel.\n\n\nwords |&gt;\n  str_view(pattern = \"[aeiou][^aeiou]\")\n\n [2] │ &lt;ab&gt;le\n [3] │ &lt;ab&gt;o&lt;ut&gt;\n [4] │ &lt;ab&gt;s&lt;ol&gt;&lt;ut&gt;e\n [5] │ &lt;ac&gt;c&lt;ep&gt;t\n [6] │ &lt;ac&gt;co&lt;un&gt;t\n [7] │ &lt;ac&gt;hi&lt;ev&gt;e\n [8] │ &lt;ac&gt;r&lt;os&gt;s\n [9] │ &lt;ac&gt;t\n[10] │ &lt;ac&gt;t&lt;iv&gt;e\n[11] │ &lt;ac&gt;tu&lt;al&gt;\n[12] │ &lt;ad&gt;d\n[13] │ &lt;ad&gt;dr&lt;es&gt;s\n[14] │ &lt;ad&gt;m&lt;it&gt;\n[15] │ &lt;ad&gt;v&lt;er&gt;t&lt;is&gt;e\n[16] │ &lt;af&gt;f&lt;ec&gt;t\n[17] │ &lt;af&gt;f&lt;or&gt;d\n[18] │ &lt;af&gt;t&lt;er&gt;\n[19] │ &lt;af&gt;t&lt;er&gt;no&lt;on&gt;\n[20] │ &lt;ag&gt;a&lt;in&gt;\n[21] │ &lt;ag&gt;a&lt;in&gt;st\n... and 924 more\n\n\nContain at least two vowel-consonant pairs in a row.\nTo display these, we can use the regular expression \"[aeiou][^aeiou][aeiou][^aeiou]\" , where: —\n\n[aeiou] matches any vowel (a, e, i, o, or u).\n[^aeiou]* matches zero or more characters that are not vowels, allowing for consonants between the vowel-consonant pairs.\n[aeiou][^aeiou][aeiou][^aeiou] specifies the pattern for at least two consecutive vowel-consonant pairs.\n\n\nwords |&gt;\n  str_view(pattern = \"[aeiou][^aeiou][aeiou][^aeiou]\")\n\n  [4] │ abs&lt;olut&gt;e\n [23] │ &lt;agen&gt;t\n [30] │ &lt;alon&gt;g\n [36] │ &lt;amer&gt;ica\n [39] │ &lt;anot&gt;her\n [42] │ &lt;apar&gt;t\n [43] │ app&lt;aren&gt;t\n [61] │ auth&lt;orit&gt;y\n [62] │ ava&lt;ilab&gt;le\n [63] │ &lt;awar&gt;e\n [64] │ &lt;away&gt;\n [70] │ b&lt;alan&gt;ce\n [75] │ b&lt;asis&gt;\n [81] │ b&lt;ecom&gt;e\n [83] │ b&lt;efor&gt;e\n [84] │ b&lt;egin&gt;\n [85] │ b&lt;ehin&gt;d\n [87] │ b&lt;enef&gt;it\n[119] │ b&lt;usin&gt;ess\n[143] │ ch&lt;arac&gt;ter\n... and 149 more\n\n\nOnly consist of repeated vowel-consonant pairs.\nTo display these, we can use the regular expression \"^(?:[aeiou][^aeiou]){2,}$\" , where: —\n\n^: This anchor asserts the start of the string.\n(?: ... ): This is a non-capturing group used to group the pattern for a vowel-consonant pair.\n[aeiou]: This character class matches any vowel (a, e, i, o, or u). It’s the first part of the vowel-consonant pair.\n[^aeiou]: This character class matches any character that is not a vowel. It’s the second part of the vowel-consonant pair and matches a consonant.\n{2,}: This quantifier specifies that the preceding pattern (the vowel-consonant pair) must occur at least two or more times.\n$: This anchor asserts the end of the string.\n\n\nwords |&gt;\n  str_view(pattern = \"^(?:[aeiou][^aeiou]){2,}$\")\n\n [64] │ &lt;away&gt;\n[265] │ &lt;eleven&gt;\n[279] │ &lt;even&gt;\n[281] │ &lt;ever&gt;\n[436] │ &lt;item&gt;\n[573] │ &lt;okay&gt;\n[579] │ &lt;open&gt;\n[586] │ &lt;original&gt;\n[591] │ &lt;over&gt;\n[905] │ &lt;unit&gt;\n[911] │ &lt;upon&gt;"
  },
  {
    "objectID": "Chapter16.html#question-4-1",
    "href": "Chapter16.html#question-4-1",
    "title": "Chapter 16",
    "section": "Question 4",
    "text": "Question 4\nCreate 11 regular expressions that match the British or American spellings for each of the following words: airplane/aeroplane, aluminum/aluminium, analog/analogue, ass/arse, center/centre, defense/defence, donut/doughnut, gray/grey, modeling/modelling, skeptic/sceptic, summarize/summarise. Try and make the shortest possible regex!\n\n# Sample passage with mixed spellings\nsample_text &lt;- \"The airplane is made of aluminum. The analog signal is stronger. Don't be an ass. The center is closed for defense training. I prefer a donut, while she likes a doughnut. His hair is gray, but hers is grey. We're modeling a new project. The skeptic will not believe it. Please summarize the report.\"\n\n# Define the regular expressions\npatterns_to_detect &lt;- c(\n  \"air(?:plane|oplane)\",\n  \"alumin(?:um|ium)\",\n  \"analog(?:ue)?\",\n  \"ass|arse\",\n  \"cent(?:er|re)\",\n  \"defen(?:se|ce)\",\n  \"dou(?:gh)?nut\",\n  \"gr(?:a|e)y\",\n  \"model(?:ing|ling)\",\n  \"skep(?:tic|tic)\",\n  \"summar(?:ize|ise)\"\n)\n\n# Find and highlight the spellings\nfor (pattern in patterns_to_detect) {\n  matches &lt;- str_extract_all(sample_text, pattern)\n  if (length(matches[[1]]) &gt; 0) {\n    sample_text &lt;- str_replace_all(sample_text, \n                                   pattern, \n                                   paste0(\"**\", matches[[1]], \"**\"))\n  }\n}\n\nsample_text\n\n[1] \"The **airplane** is made of **aluminum**. The **analog** signal is stronger. Don't be an **ass**. The **center** is closed for **defense** training. I prefer a donut, while she likes a **doughnut**. His hair is **gray**, but hers is **gray**. We're **modeling** a new project. The **skeptic** will not believe it. Please **summarize** the report.\"\n[2] \"The **airplane** is made of **aluminum**. The **analog** signal is stronger. Don't be an **ass**. The **center** is closed for **defense** training. I prefer a donut, while she likes a **doughnut**. His hair is **grey**, but hers is **grey**. We're **modeling** a new project. The **skeptic** will not believe it. Please **summarize** the report.\""
  },
  {
    "objectID": "Chapter16.html#question-5",
    "href": "Chapter16.html#question-5",
    "title": "Chapter 16",
    "section": "Question 5",
    "text": "Question 5\nSwitch the first and last letters in words. Which of those strings are still words?\nWe can switch the first and last letters in words using the code given below, with str_replace_all(). To display the new strings which are still words, we can use the code below: —\n\n# Code to switch the first and last letters in words\nnew_words = words |&gt;\n  str_replace_all(pattern = \"\\\\b(\\\\w)(\\\\w*)(\\\\w)\\\\b\", \n                  replacement = \"\\\\3\\\\2\\\\1\")\n\n# Finding which of the new strings are part of the original \"words\"\ntibble(\n  original_words = words[new_words %in% words],\n  new_words = new_words[new_words %in% words]\n) |&gt;\n  gt() |&gt;\n  cols_label_with(columns = everything(),\n                  fn = ~ make_clean_names(., case = \"title\")) |&gt;\n  opt_interactive()"
  },
  {
    "objectID": "Chapter16.html#question-6",
    "href": "Chapter16.html#question-6",
    "title": "Chapter 16",
    "section": "Question 6",
    "text": "Question 6\nDescribe in words what these regular expressions match: (read carefully to see if each entry is a regular expression or a string that defines a regular expression.)\nA Regular Expression (Regex) is a formalized way to represent a pattern that can match various strings. It is language-independent and can be used in various programming languages, such as Python, R etc. On the other hand, a string that defines a Regular Expression is a regular character string that contains the textual representation of a regular expression. It is not the actual regular expression itself but rather a sequence of characters that programmers use to create and define regular expressions in their code. It is passed as an argument to a function or method provided by the programming language’s regular expression library to create a regular expression object.\nHere’s a description of what each of these regular expressions matches:\n\n^.*$: This regular expression matches an entire string. It starts with ^ (caret), which anchors the match to the beginning of the string, followed by .* which matches any number of characters (including none), and ends with $ (dollar sign), which anchors the match to the end of the string. So, it essentially matches any string, including an empty one.\n\"\\\\{.+\\\\}\": This is a string defining a regular expression matches strings that contain curly braces {} with content inside them. The double backslashes \\\\ are used to escape the curly braces, and .+ matches one or more of any characters within the braces. So, it would match strings like “{abc}” or “{123}”.\n\\d{4}-\\d{2}-\\d{2}: This regular expression matches a date-like pattern in the format “YYYY-MM-DD.” Here, \\d matches a digit, and {4}, {2}, and {2} specify the exact number of digits for the year, month, and day, respectively. So, it matches strings like “2023-09-14.”\n\"\\\\\\\\{4}\": This is a string that defines a regular expression which matches strings that contains exactly four backslashes. Each backslash is escaped with another backslash, so \\\\ matches a single backslash, and {4} specifies that exactly four backslashes must appear consecutively in the string. It matches strings like “\\\\\\\\abcd” but not “\\\\efg” (which contains only two backslashes).\n\\..\\..\\..: This regular expression matches strings that have three dots separated by any character. The dot . is a special character in regular expressions, so it’s escaped with a backslash \\. to match a literal dot . . Thereafter, the . matches any character, and this pattern is repeated three times. So, it matches strings like “.a.b.c” or “.1.2.3”\n\ntest_string = c(\"a.bc..de\", \"1.2.3\", \"x...y\", \".1.2.3\",\n                \"a.b.c.\", \".a.b.c\")\n\ntest_regex = \"\\\\..\\\\..\\\\..\"\nstr_view(test_regex)\n\n[1] │ \\..\\..\\..\n\ntibble(\n  test_string = test_string,\n  match_result = str_detect(test_string, test_regex)\n)\n\n# A tibble: 6 × 2\n  test_string match_result\n  &lt;chr&gt;       &lt;lgl&gt;       \n1 a.bc..de    FALSE       \n2 1.2.3       FALSE       \n3 x...y       FALSE       \n4 .1.2.3      TRUE        \n5 a.b.c.      FALSE       \n6 .a.b.c      TRUE        \n\n\n(.)\\1\\1: This regular expression matches strings that contain three consecutive identical characters. The parentheses (.) capture any single character, and \\1 refers to the first captured character. So, it matches strings like “aaa” or “111.”\n\"(..)\\\\1\": This is a string that represents a regular expression which matches strings consisting of two identical characters repeated twice. The (..) captures any two characters, and \\\\1 refers to the first captured two characters. So, it matches strings like aa or 11 within double quotes."
  },
  {
    "objectID": "Chapter16.html#question-1-2",
    "href": "Chapter16.html#question-1-2",
    "title": "Chapter 16",
    "section": "Question 1",
    "text": "Question 1\nFor each of the following challenges, try solving it by using both a single regular expression, and a combination of multiple str_detect() calls.\n\nFind all words that start or end with x.\nThe words are displayed below: —\n\n# Using a singular regular expression\nstr_view(words, \"(^xX)|(x$)\")\n\n[108] │ bo&lt;x&gt;\n[747] │ se&lt;x&gt;\n[772] │ si&lt;x&gt;\n[841] │ ta&lt;x&gt;\n\n# Using a combination of multiple str_detect() calls\nwords[str_detect(words, \"^xX\") | str_detect(words, \"x$\")]\n\n[1] \"box\" \"sex\" \"six\" \"tax\"\n\n\nFind all words that start with a vowel and end with a consonant.\nThe words are displayed below: —\n\n# Using a singular regular expression\npattern_b = \"^(?i)[aeiou].*[^aeiou]$\"\nstr_subset(words, pattern_b)\n\n  [1] \"about\"       \"accept\"      \"account\"     \"across\"      \"act\"        \n  [6] \"actual\"      \"add\"         \"address\"     \"admit\"       \"affect\"     \n [11] \"afford\"      \"after\"       \"afternoon\"   \"again\"       \"against\"    \n [16] \"agent\"       \"air\"         \"all\"         \"allow\"       \"almost\"     \n [21] \"along\"       \"already\"     \"alright\"     \"although\"    \"always\"     \n [26] \"amount\"      \"and\"         \"another\"     \"answer\"      \"any\"        \n [31] \"apart\"       \"apparent\"    \"appear\"      \"apply\"       \"appoint\"    \n [36] \"approach\"    \"arm\"         \"around\"      \"art\"         \"as\"         \n [41] \"ask\"         \"at\"          \"attend\"      \"authority\"   \"away\"       \n [46] \"awful\"       \"each\"        \"early\"       \"east\"        \"easy\"       \n [51] \"eat\"         \"economy\"     \"effect\"      \"egg\"         \"eight\"      \n [56] \"either\"      \"elect\"       \"electric\"    \"eleven\"      \"employ\"     \n [61] \"end\"         \"english\"     \"enjoy\"       \"enough\"      \"enter\"      \n [66] \"environment\" \"equal\"       \"especial\"    \"even\"        \"evening\"    \n [71] \"ever\"        \"every\"       \"exact\"       \"except\"      \"exist\"      \n [76] \"expect\"      \"explain\"     \"express\"     \"identify\"    \"if\"         \n [81] \"important\"   \"in\"          \"indeed\"      \"individual\"  \"industry\"   \n [86] \"inform\"      \"instead\"     \"interest\"    \"invest\"      \"it\"         \n [91] \"item\"        \"obvious\"     \"occasion\"    \"odd\"         \"of\"         \n [96] \"off\"         \"offer\"       \"often\"       \"okay\"        \"old\"        \n[101] \"on\"          \"only\"        \"open\"        \"opportunity\" \"or\"         \n[106] \"order\"       \"original\"    \"other\"       \"ought\"       \"out\"        \n[111] \"over\"        \"own\"         \"under\"       \"understand\"  \"union\"      \n[116] \"unit\"        \"university\"  \"unless\"      \"until\"       \"up\"         \n[121] \"upon\"        \"usual\"      \n\n# Using a combination of multiple str_detect() calls\nwords[\n  str_detect(words, \"^(?i)[aeiou]\") &\n  str_detect(words, \"[^aeiou]$\")  \n]\n\n  [1] \"about\"       \"accept\"      \"account\"     \"across\"      \"act\"        \n  [6] \"actual\"      \"add\"         \"address\"     \"admit\"       \"affect\"     \n [11] \"afford\"      \"after\"       \"afternoon\"   \"again\"       \"against\"    \n [16] \"agent\"       \"air\"         \"all\"         \"allow\"       \"almost\"     \n [21] \"along\"       \"already\"     \"alright\"     \"although\"    \"always\"     \n [26] \"amount\"      \"and\"         \"another\"     \"answer\"      \"any\"        \n [31] \"apart\"       \"apparent\"    \"appear\"      \"apply\"       \"appoint\"    \n [36] \"approach\"    \"arm\"         \"around\"      \"art\"         \"as\"         \n [41] \"ask\"         \"at\"          \"attend\"      \"authority\"   \"away\"       \n [46] \"awful\"       \"each\"        \"early\"       \"east\"        \"easy\"       \n [51] \"eat\"         \"economy\"     \"effect\"      \"egg\"         \"eight\"      \n [56] \"either\"      \"elect\"       \"electric\"    \"eleven\"      \"employ\"     \n [61] \"end\"         \"english\"     \"enjoy\"       \"enough\"      \"enter\"      \n [66] \"environment\" \"equal\"       \"especial\"    \"even\"        \"evening\"    \n [71] \"ever\"        \"every\"       \"exact\"       \"except\"      \"exist\"      \n [76] \"expect\"      \"explain\"     \"express\"     \"identify\"    \"if\"         \n [81] \"important\"   \"in\"          \"indeed\"      \"individual\"  \"industry\"   \n [86] \"inform\"      \"instead\"     \"interest\"    \"invest\"      \"it\"         \n [91] \"item\"        \"obvious\"     \"occasion\"    \"odd\"         \"of\"         \n [96] \"off\"         \"offer\"       \"often\"       \"okay\"        \"old\"        \n[101] \"on\"          \"only\"        \"open\"        \"opportunity\" \"or\"         \n[106] \"order\"       \"original\"    \"other\"       \"ought\"       \"out\"        \n[111] \"over\"        \"own\"         \"under\"       \"understand\"  \"union\"      \n[116] \"unit\"        \"university\"  \"unless\"      \"until\"       \"up\"         \n[121] \"upon\"        \"usual\"      \n\n\nAre there any words that contain at least one of each different vowel?\nNo, there are no such words in words.\n\npattern_c = \"^(?=.*a)(?=.*e)(?=.*i)(?=.*o)(?=.*u).+\"\nstr_subset(words, pattern_c)\n\ncharacter(0)"
  },
  {
    "objectID": "Chapter16.html#question-2-2",
    "href": "Chapter16.html#question-2-2",
    "title": "Chapter 16",
    "section": "Question 2",
    "text": "Question 2\nConstruct patterns to find evidence for and against the rule “i before e except after c”?\nThe code given below provides annotated evidence for and against the rule “i before e except after c”.\n\n# Creating the regexp's first to use in stringr functions\npattern_1a = \"\\\\b\\\\w*ie\\\\w*\\\\b\"\npattern_1b = \"\\\\b\\\\w+ei\\\\w*\\\\b\"\n\npattern_2a = \"\\\\b\\\\w*cei\\\\w*\\\\b\"\npattern_2b = \"\\\\b\\\\w*cie\\\\w*\\\\b\"\n\n# Words which contain \"i\" before \"e\"\nwords[str_detect(words, pattern_1a)]\n\n [1] \"achieve\"    \"believe\"    \"brief\"      \"client\"     \"die\"       \n [6] \"experience\" \"field\"      \"friend\"     \"lie\"        \"piece\"     \n[11] \"quiet\"      \"science\"    \"society\"    \"tie\"        \"view\"      \n\n# Words which contain \"e\" before an \"i\", thus giving evidence against\n# the rule, unless there is a preceeding \"c\"\nwords[str_detect(words, pattern_1b)]\n\n[1] \"receive\" \"weigh\"  \n\n# Words which contain \"e\" before an \"i\" after \"c\", thus following the rule.\n# That is, evidence in favour of the rule\nwords[str_detect(words, pattern_2a)]\n\n[1] \"receive\"\n\n# Words which contain an \"i\" before \"e\" after \"c\", thus violating the rule.\n# That is, evidence against the rule\nwords[str_detect(words, pattern_2b)]\n\n[1] \"science\" \"society\""
  },
  {
    "objectID": "Chapter16.html#question-3-2",
    "href": "Chapter16.html#question-3-2",
    "title": "Chapter 16",
    "section": "Question 3",
    "text": "Question 3\ncolors() contains a number of modifiers like “lightgray” and “darkblue”. How could you automatically identify these modifiers? (Think about how you might detect and then removed the colors that are modified).\n\nThe R code col_vec = colours(distinct = TRUE) creates a vector col_vec containing a set of distinct color names available in R’s default color palette.\nThe code col_vec = col_vec[!str_detect(col_vec, \"\\\\b\\\\w*\\\\d\\\\w*\\\\b\")] filters the vector col_vec to exclude color names that contain any digits within them.\nFinally, the code col_vec[str_detect(col_vec, \"\\\\b(?:light|dark)\\\\w*\\\\b\")] will return a subset of the col_vec vector containing color names that have modifiers like “light” or “dark” in them, effectively identifying color names with modifiers.\n\n\ncol_vec = colours(distinct = TRUE)\ncol_vec = col_vec[!str_detect(col_vec, \"\\\\b\\\\w*\\\\d\\\\w*\\\\b\")]\n\ncol_vec[str_detect(col_vec, \"\\\\b(?:light|dark)\\\\w*\\\\b\")]\n\n [1] \"darkgoldenrod\"        \"darkgray\"             \"darkgreen\"           \n [4] \"darkkhaki\"            \"darkmagenta\"          \"darkolivegreen\"      \n [7] \"darkorange\"           \"darkorchid\"           \"darkred\"             \n[10] \"darksalmon\"           \"darkseagreen\"         \"darkslateblue\"       \n[13] \"darkslategray\"        \"darkturquoise\"        \"darkviolet\"          \n[16] \"lightblue\"            \"lightcoral\"           \"lightcyan\"           \n[19] \"lightgoldenrod\"       \"lightgoldenrodyellow\" \"lightgray\"           \n[22] \"lightgreen\"           \"lightpink\"            \"lightsalmon\"         \n[25] \"lightseagreen\"        \"lightskyblue\"         \"lightslateblue\"      \n[28] \"lightslategray\"       \"lightsteelblue\"       \"lightyellow\""
  },
  {
    "objectID": "Chapter16.html#question-4-2",
    "href": "Chapter16.html#question-4-2",
    "title": "Chapter 16",
    "section": "Question 4",
    "text": "Question 4\nCreate a regular expression that finds any base R dataset. You can get a list of these data-sets via a special use of the data() function: data(package = \"datasets\")$results[, \"Item\"]. Note that a number of old data-sets are individual vectors; these contain the name of the grouping “data frame” in parentheses, so you’ll need to strip those off.\nThe following code does the job, and purpose of each line is explained in the annotations.\n\n# Extract all base R datasets into a character vector\nbase_r_packs = data(package = \"datasets\")$results[, \"Item\"]\n\n# Remove all the names of grouping data.frames in parenthesis \nbase_r_packs = str_replace_all(base_r_packs, \n                pattern = \"\\\\([^()]+\\\\)\", \n                replacement = \"\")\n# Remove the whitespace, i.e., \" \" let after removing the parenthesis words\nbase_r_packs = str_replace_all(base_r_packs, \n                pattern = \"\\\\s+$\", \n                replacement = \"\")\n\n# Create the regular expression\nhuge_regex = str_c(\"\\\\b(\", str_flatten(base_r_packs, \"|\"), \")\\\\b\")"
  },
  {
    "objectID": "Chapter17.html",
    "href": "Chapter17.html",
    "title": "Chapter 17",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggthemes)\nlibrary(gt)\nlibrary(gtExtras)\ndata(\"gss_cat\")\nImportant Points"
  },
  {
    "objectID": "Chapter17.html#question-1",
    "href": "Chapter17.html#question-1",
    "title": "Chapter 17",
    "section": "Question 1",
    "text": "Question 1\nExplore the distribution of rincome (reported income). What makes the default bar chart hard to understand? How could you improve the plot?\nThe default bar chart is hard to understand because: –\n\nIt is vertical, and the names of categories overlap on x-axis.\nThe “Not applicable” category is before the lowest income group. Thus, the pattern is disturbed.\n\nWe could improve the plot, as shown in Figure 1, by: —\n\nMaking it into a horizontal bar chart to allow space and easy reading of categories of income levels.\nMove the “Not Applicable” level after the highest income level, along-side “Refused”, “Dont’ know” and “No answer”.\nFurther, we could remove non-data ink, as per principles of Mr. Tufte to make our pattern stand out. Also, we could create a separate colouring scheme for data outside the income levels.\n\n\nno_levels = levels(gss_cat$rincome)[c(1:3, 16)]\n\ngss_cat |&gt;\n  mutate(col_level = rincome %in% no_levels) |&gt;\n  ggplot(aes(y = fct_relevel(rincome, \n                             \"Not applicable\",\n                             after = 3),\n             fill = col_level)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Number of respondents\", y = NULL,\n       title = \"Income Levels of respondents in General Social Survey\") +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = \"none\") +\n  scale_fill_manual(values = c(\"#3d3b3b\", \"#999494\"))\n\n\n\n\nFigure 1: Improved bar chart"
  },
  {
    "objectID": "Chapter17.html#question-2",
    "href": "Chapter17.html#question-2",
    "title": "Chapter 17",
    "section": "Question 2",
    "text": "Question 2\nWhat is the most common relig in this survey? What’s the most common partyid?\nThe most common relig is “Protestant”. And, the most common partyid is “Independent”.\n\ngss_cat |&gt;\n  count(relig, sort = TRUE)\n\n# A tibble: 15 × 2\n   relig                       n\n   &lt;fct&gt;                   &lt;int&gt;\n 1 Protestant              10846\n 2 Catholic                 5124\n 3 None                     3523\n 4 Christian                 689\n 5 Jewish                    388\n 6 Other                     224\n 7 Buddhism                  147\n 8 Inter-nondenominational   109\n 9 Moslem/islam              104\n10 Orthodox-christian         95\n11 No answer                  93\n12 Hinduism                   71\n13 Other eastern              32\n14 Native american            23\n15 Don't know                 15\n\ngss_cat |&gt;\n  count(partyid, sort = TRUE)\n\n# A tibble: 10 × 2\n   partyid                n\n   &lt;fct&gt;              &lt;int&gt;\n 1 Independent         4119\n 2 Not str democrat    3690\n 3 Strong democrat     3490\n 4 Not str republican  3032\n 5 Ind,near dem        2499\n 6 Strong republican   2314\n 7 Ind,near rep        1791\n 8 Other party          393\n 9 No answer            154\n10 Don't know             1"
  },
  {
    "objectID": "Chapter17.html#question-3",
    "href": "Chapter17.html#question-3",
    "title": "Chapter 17",
    "section": "Question 3",
    "text": "Question 3\nWhich relig does denom (denomination) apply to? How can you find out with a table? How can you find out with a visualization?\nWe can see from the code below that more than one factor values in denom (denomination) occur only in “Protestant”, “Christian” and “Other” religions. To explore further, we can cross-tabulate religion and denomination, as shown in Table 1, and realize that the only religion to which denomination really applies to is “Protestant”.\nWe could also do a visualization as in Figure 2 .\n\ngss_cat |&gt;\n  group_by(relig) |&gt;\n  summarise(n = n_distinct(denom)) |&gt;\n  arrange(desc(n)) |&gt;\n  filter(n &gt; 1)\n\n# A tibble: 3 × 2\n  relig          n\n  &lt;fct&gt;      &lt;int&gt;\n1 Protestant    29\n2 Christian      4\n3 Other          2\n\n\n\ngss_cat |&gt;\n  filter(relig %in% c(\"Protestant\", \"Christian\", \"Other\")) |&gt;\n  group_by(relig, denom) |&gt;\n  tally() |&gt;\n  spread(relig, n) |&gt;\n  arrange(desc(Christian)) |&gt;\n  gt() |&gt;\n  sub_missing(missing_text = \"\") |&gt;\n  gt_theme_538()\n\n\n\n\n\nTable 1:  Cross-Table of the deominations within the three religions \n  \n    \n    \n      denom\n      Christian\n      Other\n      Protestant\n    \n  \n  \n    No denomination\n452\n7\n1224\n    Not applicable\n224\n217\n\n    Don't know\n11\n\n41\n    No answer\n2\n\n22\n    Other\n\n\n2534\n    Episcopal\n\n\n397\n    Presbyterian-dk wh\n\n\n244\n    Presbyterian, merged\n\n\n67\n    Other presbyterian\n\n\n47\n    United pres ch in us\n\n\n110\n    Presbyterian c in us\n\n\n104\n    Lutheran-dk which\n\n\n267\n    Evangelical luth\n\n\n122\n    Other lutheran\n\n\n30\n    Wi evan luth synod\n\n\n71\n    Lutheran-mo synod\n\n\n212\n    Luth ch in america\n\n\n71\n    Am lutheran\n\n\n146\n    Methodist-dk which\n\n\n239\n    Other methodist\n\n\n33\n    United methodist\n\n\n1067\n    Afr meth ep zion\n\n\n32\n    Afr meth episcopal\n\n\n77\n    Baptist-dk which\n\n\n1457\n    Other baptists\n\n\n213\n    Southern baptist\n\n\n1536\n    Nat bapt conv usa\n\n\n40\n    Nat bapt conv of am\n\n\n76\n    Am bapt ch in usa\n\n\n130\n    Am baptist asso\n\n\n237\n  \n  \n  \n\n\n\n\n\n\ngss_cat |&gt;\n  group_by(relig) |&gt;\n  summarise(n = n_distinct(denom)) |&gt;\n  arrange(desc(n)) |&gt;\n  ggplot(aes(y = reorder(relig, n), x = n)) +\n  geom_bar(stat = \"identity\") +\n  theme_minimal() +\n  labs(x = \"Number of denominations\", y = NULL,\n       title = \"Only Protestant religion has demoninations within it\")\n\n\n\n\nFigure 2: ?(caption)"
  },
  {
    "objectID": "Chapter17.html#question-1-1",
    "href": "Chapter17.html#question-1-1",
    "title": "Chapter 17",
    "section": "Question 1",
    "text": "Question 1\nThere are some suspiciously high numbers in tvhours. Is the mean a good summary?\nNo, mean is not a good summary as the distribution of tvhours is right skewed. Instead, we should use median as a summary measure.\n\ngss_cat |&gt;\n  drop_na() |&gt;\n  mutate(tvhours = as_factor(tvhours)) |&gt;\n  ggplot(aes(x = tvhours)) +\n  geom_bar(col = \"black\", fill = \"white\") +\n  theme_clean() +\n  labs(x = \"Hours per day spent watching TV\",\n       y = \"Numbers\", title = \"Distribution of TV Hours is right skewed\")"
  },
  {
    "objectID": "Chapter17.html#question-2-1",
    "href": "Chapter17.html#question-2-1",
    "title": "Chapter 17",
    "section": "Question 2",
    "text": "Question 2\nFor each factor in gss_cat identify whether the order of the levels is arbitrary or principled.\nThe variables in the gss_cat data-set which are factors are: —\n\n\n\n\n\n\n\n\nFactor Variable\nLevels\nOrder is arbitrary or principled\n\n\n\n\nmarital\nNo answer, Never married, Separated, Divorced, Widowed and, Married\nArbitrary, since they are not in a specific order\n\n\nrace\nOther, Black, White and, Not applicable\nArbitrary, since they are not in a specific order\n\n\nrincome\nNo answer, Don’t know, Refused, $25000 or more, $20000 - 24999, $15000 - 19999, $10000 - 14999, $8000 to 9999, $7000 to 7999, $6000 to 6999, $5000 to 5999, $4000 to 4999, $3000 to 3999, $1000 to 2999, Lt $1000 and, Not applicable\nPrincipled, since the income levels are in a specified increasing or decreasing order, with few levels arbitrary\n\n\npartyid\nNo answer, Don’t know, Other party, Strong republican, Not str republican, Ind,near rep, Independent, Ind,near dem, Not str democrat and, Strong democrat\nPartly Principled, as there are two extremes, and then levels in the middle.\n\n\nrelig\nNo answer, Don’t know, Inter-nondenominational, Native american, Christian, Orthodox-christian, Moslem/islam, Other eastern, Hinduism, Buddhism, Other, None, Jewish, Catholic, Protestant and, Not applicable\nArbitrary, as the religions are not in a specific order.\n\n\ndenom\nNo answer, Don’t know, No denomination, Other, Episcopal, Presbyterian-dk wh, Presbyterian, merged, Other presbyterian, United pres ch in us, Presbyterian c in us, Lutheran-dk which, Evangelical luth, Other lutheran, Wi evan luth synod, Lutheran-mo synod, Luth ch in america, Am lutheran, Methodist-dk which, Other methodist, United methodist, Afr meth ep zion, Afr meth episcopal, Baptist-dk which, Other baptists, Southern baptist, Nat bapt conv usa, Nat bapt conv of am, Am bapt ch in usa, Am baptist asso and, Not applicable\nArbitrary, as the denominations are not in a specific order."
  },
  {
    "objectID": "Chapter17.html#question-3-1",
    "href": "Chapter17.html#question-3-1",
    "title": "Chapter 17",
    "section": "Question 3",
    "text": "Question 3\nWhy did moving “Not applicable” to the front of the levels move it to the bottom of the plot?\nMoving “Not applicable” to the front of the levels move it to the bottom of the plot, because ggplot2 plots the levels in increasing order, starting bottom’s upwards."
  },
  {
    "objectID": "Chapter17.html#question-1-2",
    "href": "Chapter17.html#question-1-2",
    "title": "Chapter 17",
    "section": "Question 1",
    "text": "Question 1\nHow have the proportions of people identifying as Democrat, Republican, and Independent changed over time?\nAs reflected in the Figure 3, the proportions of people identifying as Democrat has slightly increased, Republican has slightly decreased, and Independent has increased, over the period of 15 years reflected in the data-set.\n\ngss_cat |&gt;\n  mutate(\n    partyid = fct_collapse(partyid,\n      \"Republican\"  = c(\"Strong republican\",  \"Not str republican\"),\n      \"Democrat\"    = c(\"Strong democrat\", \"Not str democrat\"),\n      \"Independent\" = c(\"Independent\", \"Ind,near dem\", \"Ind,near rep\"),\n      \"Others\"      = c(\"No answer\", \"Don't know\", \"Other party\")\n    )\n  ) |&gt;\n  group_by(year, partyid) |&gt;\n  count() |&gt;\n  ggplot(aes(x = year, y = n, fill = partyid)) +\n  geom_col(position = \"fill\") +\n  scale_fill_manual(values = c(\"lightgrey\", \"red\", \"grey\", \"blue\")) +\n  theme_classic() +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"Year\", y = \"Proportion of respondents\", fill = \"Party\",\n       subtitle = \"Proportion of republicans has decreased, while that of independents has increased over the years\",\n       title = \"In 15 years, share of parties' supporters has changed\")\n\n\n\n\nFigure 3: Stacked bar chart of the partyid in the data-set"
  },
  {
    "objectID": "Chapter17.html#question-2-2",
    "href": "Chapter17.html#question-2-2",
    "title": "Chapter 17",
    "section": "Question 2",
    "text": "Question 2\nHow could you collapse rincome into a small set of categories?\nWe could collapse the rincome into a small set of categories using the following functions: –\n\nfct_lump_n()\nfct_lump_lowfreq()\nfct_lump_min()\nfct_lump_prop()\nfct_lump()\nfct_collapse()\n\n\ngss_cat|&gt;\n  mutate(\n    rincome = fct_lump_n(rincome, n = 6)\n  ) |&gt;\n  group_by(rincome) |&gt;\n  count() |&gt;\n  arrange(desc(n)) |&gt;\n  ungroup() |&gt;\n  gt() |&gt;\n  cols_label(rincome = \"Annual Income\",\n             n = \"Numbers\")\n\n\n\n\n\n  \n    \n    \n      Annual Income\n      Numbers\n    \n  \n  \n    $25000 or more\n7363\n    Not applicable\n7043\n    Other\n2603\n    $20000 - 24999\n1283\n    $10000 - 14999\n1168\n    $15000 - 19999\n1048\n    Refused\n975"
  },
  {
    "objectID": "Chapter17.html#question-3-2",
    "href": "Chapter17.html#question-3-2",
    "title": "Chapter 17",
    "section": "Question 3",
    "text": "Question 3\nNotice there are 9 groups (excluding other) in the fct_lump example above. Why not 10? (Hint: type ?fct_lump, and find the default for the argument other_level is “Other”.)\nYes, there are 9 groups (excluding other) in this example, as shown below also in Figure 4. This is because n = 10 argument limits the total groups to 10, and the function needs one group for “Other”, i.e. all other groups whose count is lesser than top 9 groups. Thus, the groups shown are 9, with 1 as “Other” (at the end).\n\ngss_cat |&gt;\n  mutate(relig = fct_lump_n(relig, n = 10)) |&gt;\n  count(relig) |&gt;\n  gt()\n\n\n\n\n\n\n  \n    \n    \n      relig\n      n\n    \n  \n  \n    Inter-nondenominational\n109\n    Christian\n689\n    Orthodox-christian\n95\n    Moslem/islam\n104\n    Buddhism\n147\n    None\n3523\n    Jewish\n388\n    Catholic\n5124\n    Protestant\n10846\n    Other\n458\n  \n  \n  \n\n\nFigure 4: Table of number of respondents from each of top 10 religions, including Other"
  },
  {
    "objectID": "Chapter18.html",
    "href": "Chapter18.html",
    "title": "Chapter 18",
    "section": "",
    "text": "library(tidyverse)\nlibrary(nycflights13)\nlibrary(lubridate)\ndata(\"flights\")\nlibrary(gt)\nlibrary(gtExtras)"
  },
  {
    "objectID": "Chapter18.html#all-date-formats-understood-by-readr",
    "href": "Chapter18.html#all-date-formats-understood-by-readr",
    "title": "Chapter 18",
    "section": "All date formats understood by readr",
    "text": "All date formats understood by readr\n(As taken from the book R for Data Science (2e) and (Grolemund and Wickham 2011a))\n\n\n\nType\nCode\nMeaning\nExample\n\n\n\n\nYear\n%Y\n4 digit year\n2021\n\n\n\n%y\n2 digit year\n21\n\n\nMonth\n%m\nNumber\n2\n\n\n\n%b\nAbbreviated name\nFeb\n\n\n\n%B\nFull name\nFebruary\n\n\nDay\n%d\nTwo digits\n02\n\n\n\n%e\nOne or two digits\n2\n\n\nTime\n%H\n24-hour hour\n13\n\n\n\n%I\n12-hour hour\n1\n\n\n\n%p\nAM/PM\npm\n\n\n\n%M\nMinutes\n35\n\n\n\n%S\nSeconds\n45\n\n\n\n%OS\nSeconds with decimal component\n45.35\n\n\n\n%Z\nTime zone name\nAmerica/Chicago\n\n\n\n%z\nOffset from UTC\n+0800\n\n\nOther\n%.\nSkip one non-digit\n:\n\n\n\n%*\nSkip any number of non-digits"
  },
  {
    "objectID": "Chapter18.html#question-1",
    "href": "Chapter18.html#question-1",
    "title": "Chapter 18",
    "section": "Question 1",
    "text": "Question 1\nWhat happens if you parse a string that contains invalid dates?\nymd(c(\"2010-10-10\", \"bananas\"))\nWhenever a string is parsed, that contains invalid dates, a missing value, i.e., NA will be generated.\n\nymd(c(\"2010-10-10\", \"bananas\"))\n\n[1] \"2010-10-10\" NA"
  },
  {
    "objectID": "Chapter18.html#question-2",
    "href": "Chapter18.html#question-2",
    "title": "Chapter 18",
    "section": "Question 2",
    "text": "Question 2\nWhat does the tzone argument to today() do? Why is it important?\nThe tzone argument aloows us to write “a character vector specifying which time zone you would like the current time in. tzone defaults to your computer’s system timezone.”(Grolemund and Wickham 2011b)\nThus, we can find the date at the current moment in local time zone of the computer system with today() and at any other timezone, say UTC, with today(tzone = \"UTC\") . It is important when analyzing real-time data, or data from multiples locations across the globe, as the date may not be the same at all places at all times.\nA more important role of tzone is with now() as time is different at different zones.\n\ntoday()\n\n[1] \"2023-09-17\"\n\ntoday(tzone = \"UTC\")\n\n[1] \"2023-09-17\"\n\nnow()\n\n[1] \"2023-09-17 10:57:56 IST\"\n\nnow(tzone = \"UTC\")\n\n[1] \"2023-09-17 05:27:56 UTC\""
  },
  {
    "objectID": "Chapter18.html#question-3",
    "href": "Chapter18.html#question-3",
    "title": "Chapter 18",
    "section": "Question 3",
    "text": "Question 3\nFor each of the following date-times, show how you’d parse it using a readr column specification and a lubridate function.\nd1 &lt;- \"January 1, 2010\" \nd2 &lt;- \"2015-Mar-07\" \nd3 &lt;- \"06-Jun-2017\" \nd4 &lt;- c(\"August 19 (2015)\", \"July 1 (2015)\") \nd5 &lt;- \"12/30/14\" # Dec 30, 2014 \nt1 &lt;- \"1705\" \nt2 &lt;- \"11:15:10.12 PM\"\nThe parsing of date and time from these values is shown below: —\n\nd1 &lt;- \"January 1, 2010\" \nd2 &lt;- \"2015-Mar-07\" \nd3 &lt;- \"06-Jun-2017\" \nd4 &lt;- c(\"August 19 (2015)\", \"July 1 (2015)\") \nd5 &lt;- \"12/30/14\" # Dec 30, 2014 \nt1 &lt;- \"1705\" \nt2 &lt;- \"11:15:10.12 PM\"\n\ndf = tibble(d1, d2, d3, d4, d5, t1, t2) |&gt;\n  slice(1)\n\ndf |&gt;\n  mutate(\n    d1 = mdy(d1),\n    d2 = ymd(d2),\n    d3 = dmy(d3),\n    d4 = mdy(d4),\n    d5 = mdy(d5),\n    t1 = hm(paste0(as.numeric(t1) %/% 100, \":\", as.numeric(t1) %% 100)),\n    t2 = hms(t2)\n  )\n\n# A tibble: 1 × 7\n  d1         d2         d3         d4         d5         t1       \n  &lt;date&gt;     &lt;date&gt;     &lt;date&gt;     &lt;date&gt;     &lt;date&gt;     &lt;Period&gt; \n1 2010-01-01 2015-03-07 2017-06-06 2015-08-19 2014-12-30 17H 5M 0S\n# ℹ 1 more variable: t2 &lt;Period&gt;"
  },
  {
    "objectID": "Chapter18.html#question-1-1",
    "href": "Chapter18.html#question-1-1",
    "title": "Chapter 18",
    "section": "Question 1",
    "text": "Question 1\nHow does the distribution of flight times within a day change over the course of the year?\nThe distribution of flight times within a day change over the course of the year is displayed in Figure 1 . To check whether this is due to only some extraordinarily delayed flights, or due to longer flight times in general, we can plot the median, instead of mean, as shown in Figure 2 . Both plots provide the same conclusion, as explained there-in.\n\n\nCode\nflights |&gt;\n  mutate(\n    dep_time = make_datetime(year, month, day, dep_time %/% 100, dep_time %% 100),\n    dep_day = round_date(dep_time, unit = \"day\")\n  ) |&gt;\n  group_by(dep_day) |&gt;\n  summarise(mean_air_time = mean(air_time, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = dep_day,\n             y = mean_air_time)) +\n  geom_point() +\n  geom_smooth(span = 0.5) +\n  theme_minimal() +\n  labs(y = \"Average flight time for flights departing each day (in min)\",\n       x = NULL,\n       title = \"The average flight time increases towards the winter months\",\n       subtitle = \"The lowest average flight times occur in late summer and early fall; while highest occur in December\",\n       caption = \"Each dot represents the mean air time for flights departing in that day\") +\n  theme(panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank())\n\n\n\n\n\nFigure 1: Scatterplot of mean air time for each day of the year, with a overlaid loess smooother\n\n\n\n\n\n\nCode\nflights |&gt;\n  mutate(\n    dep_time = make_datetime(year, month, day, dep_time %/% 100, dep_time %% 100),\n    dep_day = round_date(dep_time, unit = \"day\")\n  ) |&gt;\n  group_by(dep_day) |&gt;\n  summarise(median_air_time = median(air_time, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = dep_day,\n             y = median_air_time)) +\n  geom_point() +\n  geom_smooth(span = 0.5) +\n  theme_minimal() +\n  labs(y = \"Median flight time for flights departing each day (in min)\",\n       x = NULL,\n       title = \"The median flight time increases towards the winter months\",\n       subtitle = \"The lowest median flight times occur in late summer and early fall; while highest occur in December\",\n       caption = \"Each dot represents the median air time for flights departing in that day\") +\n  theme(panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank())\n\n\n\n\n\nFigure 2: Scatterplot of MEDIAN air time for each day of the year, with a overlaid loess smooother"
  },
  {
    "objectID": "Chapter18.html#question-2-1",
    "href": "Chapter18.html#question-2-1",
    "title": "Chapter 18",
    "section": "Question 2",
    "text": "Question 2\nCompare dep_time, sched_dep_time and dep_delay. Are they consistent? Explain your findings.\n\nf1 = flights |&gt;\n  select(-hour, -minute, -time_hour) |&gt;\n  filter(!is.na(dep_time) & !is.na(arr_time) & !is.na(sched_dep_time)) |&gt;\n  mutate(\n   dep_time = make_datetime(year, month, day, dep_time %/% 100, dep_time %% 100),\n   sched_dep_time = make_datetime(year, month, day, sched_dep_time %/% 100, sched_dep_time %% 100),\n   dep_delay_calc = dep_time - sched_dep_time,\n   comparison = dep_delay_calc/60 == dep_delay,\n   .keep = \"used\"\n  )\n\nf1 |&gt;\n  slice_head(n = 5) |&gt;\n  gt() |&gt;\n  gt_theme_538()\n\n\n\n\n\nTable 1:  Table displaying top 5 observations comparing Departure Delay\ncalculated vs. dep_delay \n  \n    \n    \n      year\n      month\n      day\n      dep_time\n      sched_dep_time\n      dep_delay\n      dep_delay_calc\n      comparison\n    \n  \n  \n    2013\n1\n1\n2013-01-01 05:17:00\n2013-01-01 05:15:00\n2\n120\nTRUE\n    2013\n1\n1\n2013-01-01 05:33:00\n2013-01-01 05:29:00\n4\n240\nTRUE\n    2013\n1\n1\n2013-01-01 05:42:00\n2013-01-01 05:40:00\n2\n120\nTRUE\n    2013\n1\n1\n2013-01-01 05:44:00\n2013-01-01 05:45:00\n-1\n-60\nTRUE\n    2013\n1\n1\n2013-01-01 05:54:00\n2013-01-01 06:00:00\n-6\n-360\nTRUE\n  \n  \n  \n\n\n\n\n\nNow, when we compute the percentage of observations where dep_delay == dep_delay_calc , we get only 99.63%. This means that for 0.36% of flights, the calculation doesn’t match. We need to explore these further.\n\nmean(f1$comparison) * 100\n\n[1] 99.63269\n\n## Code Part 1\nf1 |&gt;\n  filter(!comparison)\n\n# A tibble: 1,205 × 8\n    year month   day dep_time            sched_dep_time      dep_delay\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dttm&gt;              &lt;dttm&gt;                  &lt;dbl&gt;\n 1  2013     1     1 2013-01-01 08:48:00 2013-01-01 18:35:00       853\n 2  2013     1     2 2013-01-02 00:42:00 2013-01-02 23:59:00        43\n 3  2013     1     2 2013-01-02 01:26:00 2013-01-02 22:50:00       156\n 4  2013     1     3 2013-01-03 00:32:00 2013-01-03 23:59:00        33\n 5  2013     1     3 2013-01-03 00:50:00 2013-01-03 21:45:00       185\n 6  2013     1     3 2013-01-03 02:35:00 2013-01-03 23:59:00       156\n 7  2013     1     4 2013-01-04 00:25:00 2013-01-04 23:59:00        26\n 8  2013     1     4 2013-01-04 01:06:00 2013-01-04 22:45:00       141\n 9  2013     1     5 2013-01-05 00:14:00 2013-01-05 23:59:00        15\n10  2013     1     5 2013-01-05 00:37:00 2013-01-05 22:30:00       127\n# ℹ 1,195 more rows\n# ℹ 2 more variables: dep_delay_calc &lt;drtn&gt;, comparison &lt;lgl&gt;\n\n## Code Part 2\nf1 |&gt;\n  filter(!comparison) |&gt;\n  filter(dep_delay_calc &gt; 0)\n\n# A tibble: 0 × 8\n# ℹ 8 variables: year &lt;int&gt;, month &lt;int&gt;, day &lt;int&gt;, dep_time &lt;dttm&gt;,\n#   sched_dep_time &lt;dttm&gt;, dep_delay &lt;dbl&gt;, dep_delay_calc &lt;drtn&gt;,\n#   comparison &lt;lgl&gt;\n\n\nNow, after seeing results of ## Code Part 1, we realize that some flights are so delayed that their scheduled departure time goes over to the next day, and this, unfortunately we did not figure in our calculations. To further confirm, using ## Code Part II , we confirm that the only mismatches are the ones where calculated departure delay is negative.\nHence, we need to re-work our calculations, as follows: —\n\n## Create new tibble from flights with datetime columns\nf2 = flights |&gt;\n  select(-hour, -minute, -time_hour) |&gt;\n  filter(!is.na(dep_time) & !is.na(arr_time) & !is.na(sched_dep_time)) |&gt;\n  mutate(\n   dep_time = make_datetime(year, month, day, dep_time %/% 100, dep_time %% 100),\n   sched_dep_time = make_datetime(year, month, day, sched_dep_time %/% 100, sched_dep_time %% 100)\n   )\n\n## Check range of dep_delay: to understand how early can a flight depart from scheduled time\n## The earliest is -43 min, i.e., 43 min early\nrange(f2$dep_delay)\n\n[1]  -43 1301\n\n## Now, change the departure time into +1 day, if the dep_time is earlier than scheduled time \n## by more than 45 min\nf2 = f2 |&gt;\n  mutate(\n   dep_time = if_else(dep_time &lt; sched_dep_time - minutes(45),\n                      true = dep_time + days(1),\n                      false = dep_time),\n   dep_delay_calc = dep_time - sched_dep_time,\n   dep_delay_calc = dep_delay_calc/60,\n   comparison = dep_delay_calc == dep_delay\n  )\n\n## Percentage of cases where our calculated departure delay is exactly same as dep_delay column\nmean(f2$comparison) * 100\n\n[1] 100\n\n\nThus, we realize that our 100% of our calculations match, once we factor in the flights departing the next day from their scheduled date as reflected in Table 2.\n\n## Show some flights delayed so much that they depart the next day, to see comparison\nf2 |&gt;\n  filter(day(dep_time) != day(sched_dep_time)) |&gt;\n  select(year, month, day, dep_time, sched_dep_time, \n         carrier, tailnum, dep_delay, dep_delay_calc, \n         comparison) |&gt;\n  slice_head(n = 5) |&gt;\n  gt() |&gt;\n  gt_theme_538()\n\n\n\n\n\nTable 2:  Comparison table for 5 flights departing next day from scheduled\ndate \n  \n    \n    \n      year\n      month\n      day\n      dep_time\n      sched_dep_time\n      carrier\n      tailnum\n      dep_delay\n      dep_delay_calc\n      comparison\n    \n  \n  \n    2013\n1\n1\n2013-01-02 08:48:00\n2013-01-01 18:35:00\nMQ\nN942MQ\n853\n853\nTRUE\n    2013\n1\n2\n2013-01-03 00:42:00\n2013-01-02 23:59:00\nB6\nN580JB\n43\n43\nTRUE\n    2013\n1\n2\n2013-01-03 01:26:00\n2013-01-02 22:50:00\nB6\nN636JB\n156\n156\nTRUE\n    2013\n1\n3\n2013-01-04 00:32:00\n2013-01-03 23:59:00\nB6\nN763JB\n33\n33\nTRUE\n    2013\n1\n3\n2013-01-04 00:50:00\n2013-01-03 21:45:00\nB6\nN329JB\n185\n185\nTRUE"
  },
  {
    "objectID": "Chapter18.html#question-3-1",
    "href": "Chapter18.html#question-3-1",
    "title": "Chapter 18",
    "section": "Question 3",
    "text": "Question 3\nCompare air_time with the duration between the departure and arrival. Explain your findings. (Hint: consider the location of the airport.)\nIf we compare the air_time with duration between departure time and arrival time, we get hardly 0.06% matches, even after adjusting for dep_times that occur on the next day, or arrival times occurring on the next day. This means that something else is going on, perhaps the origin and dest airports are in different time zones.\n\n\nCode\nf3 = flights |&gt;\n  select(-hour, -minute, -time_hour) |&gt;\n  filter(!is.na(dep_time) & !is.na(arr_time) & !is.na(air_time)) |&gt;\n  mutate(\n   dep_time = make_datetime(year, month, day, dep_time %/% 100, dep_time %% 100),\n   sched_dep_time = make_datetime(year, month, day, sched_dep_time %/% 100, sched_dep_time %% 100),\n   dep_time = if_else(dep_time &lt; sched_dep_time - minutes(45),\n                      true = dep_time + days(1),\n                      false = dep_time),\n   arr_time = make_datetime(year, month, day, arr_time %/% 100, arr_time %% 100),\n   sched_arr_time = make_datetime(year, month, day, sched_arr_time %/% 100, sched_arr_time %% 100),\n   air_time_calc = (arr_time - dep_time),\n   comparison = air_time_calc == air_time\n  )\n\nf3 |&gt;\n  select(c(dep_time,\n           sched_dep_time,\n           arr_time,\n           sched_arr_time,\n           origin,\n           dest,\n           air_time,\n           air_time_calc,\n           comparison)) |&gt;\n  slice(200:205) |&gt;\n  gt() |&gt;\n  gt_theme_nytimes() |&gt;\n  tab_style(\n    style = list(cell_text(weight = \"bold\")),\n    locations = cells_body(columns = comparison)\n  )\n\n\n\n\n\n\nTable 3:  The air_time variable and caculated difference between departure and\narrival times don’t match if we dont account for time zones \n  \n    \n    \n      dep_time\n      sched_dep_time\n      arr_time\n      sched_arr_time\n      origin\n      dest\n      air_time\n      air_time_calc\n      comparison\n    \n  \n  \n    2013-01-01 09:31:00\n2013-01-01 09:30:00\n2013-01-01 11:21:00\n2013-01-01 11:08:00\nLGA\nORD\n154\n110\nFALSE\n    2013-01-01 09:32:00\n2013-01-01 09:30:00\n2013-01-01 12:19:00\n2013-01-01 12:25:00\nJFK\nLAS\n324\n167\nFALSE\n    2013-01-01 09:33:00\n2013-01-01 09:37:00\n2013-01-01 10:57:00\n2013-01-01 11:02:00\nJFK\nBUF\n66\n84\nFALSE\n    2013-01-01 09:33:00\n2013-01-01 09:04:00\n2013-01-01 12:52:00\n2013-01-01 12:10:00\nJFK\nFLL\n170\n199\nFALSE\n    2013-01-01 09:33:00\n2013-01-01 09:35:00\n2013-01-01 11:20:00\n2013-01-01 11:05:00\nLGA\nMDW\n145\n107\nFALSE\n    2013-01-01 09:36:00\n2013-01-01 09:40:00\n2013-01-01 12:35:00\n2013-01-01 12:51:00\nLGA\nTPA\n159\n179\nFALSE\n  \n  \n  \n\n\n\n\n\nCode\nmean(f3$comparison) * 100\n\n\n[1] 0.05987548\n\n\nLet’s try to add the time-zones of destination airports, and then calculate the flight times. As we see below, the results still don’t match. Perhaps air_time excludes the time between dep_time and arr_time spent on the tarmac, runway etc. I hope to return to this analysis later sometime.\n\n\nCode\n#########################################################################################\n# Load data from airports which contains their time zones\ndata(\"airports\")\n\n# Left join the data set\nflights_airports = left_join(flights, airports, by = join_by(dest == faa)) |&gt;\n  select(-hour, -minute, -time_hour, -carrier, -tailnum, -lat, -lon, -alt) |&gt;\n  filter(!is.na(dep_time) & !is.na(arr_time) & !is.na(air_time))\n\nf4 = flights_airports |&gt;\n  select(year, month, day, dep_time, sched_dep_time, dep_delay, \n         arr_time, sched_arr_time, arr_delay,\n         flight, origin, dest,air_time, distance, name, tzone) |&gt;\n  mutate(\n    dep_time = make_datetime(year, month, day, \n                             dep_time %/% 100, dep_time %% 100, \n                             tz = \"America/New_York\"),\n    arr_time = make_datetime(year, month, day, \n                             arr_time %/% 100, arr_time %% 100, \n                             tz = tzone)\n    )\n\nf4 |&gt;\n  slice(202) |&gt;\n  mutate(\n    air_time = air_time,\n    tzone = tzone,\n    air_time_calc = interval(dep_time, arr_time)/minutes(1),\n    .keep = \"used\"\n  )\n\n\n\n?(caption)"
  },
  {
    "objectID": "Chapter18.html#question-4",
    "href": "Chapter18.html#question-4",
    "title": "Chapter 18",
    "section": "Question 4",
    "text": "Question 4\nHow does the average delay time change over the course of a day? Should you use dep_time or sched_dep_time? Why?\nThe average delay time increases over the course of a given day. The plot in Figure 3 shows us that peak delays occur in the late evening.\nIn my view, we should use scheduled departure time because people arrive at the airport, and plan their travel as per scheduled departure time, and thus, information about scheduled departure time will be more useful to the consumers of the data visualization.\n\nflights |&gt;\n  group_by(sched_dep_time) |&gt;\n  summarise(\n    mean_dep_delay = mean(dep_delay, na.rm = TRUE),\n    median_dep_delay = median(dep_delay, na.rm = TRUE)\n  ) |&gt;\n  mutate(\n    sched_dep_hour = sched_dep_time %/% 100,\n    sched_dep_min = sched_dep_time %% 100,\n    sched_dep_time = sched_dep_hour + (sched_dep_min/60)\n  ) |&gt;\n  \n  ggplot(aes(x = sched_dep_time,\n             y = mean_dep_delay)) +\n  geom_point(size = 0.2) +\n  geom_line() +\n  geom_smooth(col = \"red\") +\n  coord_cartesian(xlim = c(4, 24),\n                  ylim = c(0,80)) +\n  scale_x_continuous(breaks = seq(5,24,1)) +\n  theme_minimal() +\n  labs(x = \"Scheduled Depature Time during the day\",\n       y = \"Average Departure Delay (minutes)\",\n       title = \"Average departure delay for flights rises as the day progresses\",\n       subtitle = \"Flights in the evening, 6 pm to 10 pm, have the highest departure delays\\nConversely, early morning flights have minimal delays\") +\n   theme(panel.grid.minor.x = element_blank(),\n         plot.title.position = \"plot\")\n\n\n\n\nFigure 3: Line Chart (with loess smoother) of average departure delays over the course of a day"
  },
  {
    "objectID": "Chapter18.html#question-5",
    "href": "Chapter18.html#question-5",
    "title": "Chapter 18",
    "section": "Question 5",
    "text": "Question 5\nOn what day of the week should you leave if you want to minimise the chance of a delay?\nAs we can see in the Figure 4, if we want to minimize the chance of delay, we should leave on Saturday. Since the questions uses “should you leave”, we use dep_time in place of sched_dep_time .\n\nlabels_grid = c(\n  mean_delay = \"Mean delay (in mins)\",\n  median_delay = \"Median delay (in mins)\",\n  numbers = \"Number of flights\")\n\nflights |&gt;\n  mutate(\n   dep_time = make_datetime(year, month, day,\n                            dep_time %/% 100,\n                            dep_time %% 100),\n   weekday = wday(dep_time, label = TRUE, abbr = FALSE)\n  ) |&gt;\n  group_by(weekday) |&gt;\n  summarise(\n    mean_delay = mean(dep_delay, na.rm = TRUE),\n    median_delay = median(dep_delay, na.rm = TRUE),\n    numbers = n()\n  ) |&gt;\n  drop_na() |&gt;\n  pivot_longer(cols = -weekday,\n               names_to = \"indicator\",\n               values_to = \"val\") |&gt;\n  ggplot(aes(y = weekday, x = val, fill = indicator)) +\n  geom_bar(stat = \"identity\") +\n  geom_vline(xintercept = 0) +\n  facet_grid(~ indicator, scales = \"free_x\",\n             labeller = labeller(indicator = labels_grid)) +\n  labs(x = NULL, y = NULL) + \n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_fill_brewer(palette = \"Dark2\")\n\n\n\n\nFigure 4: Plot of mean & median delays, and number of flights on each day of the week"
  },
  {
    "objectID": "Chapter18.html#question-6",
    "href": "Chapter18.html#question-6",
    "title": "Chapter 18",
    "section": "Question 6",
    "text": "Question 6\nWhat makes the distribution of diamonds$carat and flights$sched_dep_time similar?\nThe distribution of diamonds$carat and flights$sched_dep_time are similar in having tendency to round off the values to nearest 0.5 Carat (in case of carat) or 5/30/60 minutes (in case of sched_dep_time). This means when humans record a continuous variable, there is a tendency to round it off to nearest unit, thus leading to a distribution which shows spikes at fixed intervals.\n\ndata(\"diamonds\")\n\ndiamonds |&gt;\n  ggplot(aes(x = carat)) +\n  geom_histogram(bins = 500) +\n  coord_cartesian(xlim = c(0, 3)) +\n  theme_minimal() +\n  labs(x = \"Carat of Diamond\", y = \"Number of Diamonds\",\n       title = \"Carats of diamonds are mostly in multiples of 0.5\",\n       subtitle = \"This suggests observations recorders' bias in propensity to round off carat to nearest 0.5\") +\n  scale_x_continuous(breaks = seq(0, 3, 0.5))\n\n\n\nf5 = flights |&gt;\n  mutate(sched_dep_time = hour + minute/60)\n\nf5 |&gt;\n  ggplot(aes(sched_dep_time)) +\n  geom_histogram(bins = 1000) +\n  coord_cartesian(xlim = c(5, 24)) +\n  theme_minimal() +\n  labs(x = \"Scheduled departure time (in hr)\", \n       y = \"Number of Flights\",\n       title = \"Scheduled Departure times are geenrally in multiples of 5, 30 or 60 minutes\",\n       subtitle = \"This suggests propensity to round off scheduled departure time to nearest hour, or atleast 5 minutes\") +\n  scale_x_continuous(breaks = seq(5, 24, 1))"
  },
  {
    "objectID": "Chapter18.html#question-7",
    "href": "Chapter18.html#question-7",
    "title": "Chapter 18",
    "section": "Question 7",
    "text": "Question 7\nConfirm our hypothesis that the early departures of flights in minutes 20-30 and 50-60 are caused by scheduled flights that leave early. Hint: create a binary variable that tells you whether or not a flight was delayed.\nIn the code below, we compare the proportion of flights that were delayed: —\n\nwith scheduled departure times between 20-30 min and 50-60 min (0.538)\nwith scheduled departure times other than these minutes (0.573)\n\nA two-sided t-test of the comparison of these two proportions has a p-value of &lt; 0.001, thus, there is a significant difference between the two proportions. Thus, we conclude that flights with scheduled departure times between 20-30 min and 50-60 min have a significantly lesser delay than other flights.\n\nf7 = flights |&gt;\n  mutate(\n    sdt_20_30 = (minute &gt;= 20 & minute &lt;= 30),\n    sdt_50_60 = (minute &gt;= 50 & minute &lt;= 60),\n    sdt_round_times = (sdt_20_30 | sdt_50_60),\n    was_delayed = (dep_delay &lt; 0),\n    .keep = \"used\"\n  )\n\nf7 |&gt;\n  group_by(sdt_round_times) |&gt;\n  summarise(prop_delayed = mean(was_delayed, na.rm = TRUE),\n            n = n())\n\n# A tibble: 2 × 3\n  sdt_round_times prop_delayed      n\n  &lt;lgl&gt;                  &lt;dbl&gt;  &lt;int&gt;\n1 FALSE                  0.573 197298\n2 TRUE                   0.538 139478\n\nf8test = prop.test(\n  x = c(113092, 75098),\n  n = c(197298, 139478)\n)\n\nf8test\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(113092, 75098) out of c(197298, 139478)\nX-squared = 400.83, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.03136898 0.03819533\nsample estimates:\n   prop 1    prop 2 \n0.5732040 0.5384218 \n\nf8test$p.value\n\n[1] 3.641236e-89"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Title\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Chapter18.html#question-1-2",
    "href": "Chapter18.html#question-1-2",
    "title": "Chapter 18",
    "section": "Question 1",
    "text": "Question 1\nExplain days(!overnight) and days(overnight) to someone who has just started learning R. What is the key fact you need to know?\nIn R, TRUE is represented numerically as 1, and FALSE is represented numerically as 0. Thus, days(!overnight) is equal to days(0), i.e., a time period of zero days, for an overnight flight, and 1 day for a regular flight.\nOn the other hand, days(overnight) is equal to days(1), i.e., a time period of 1 day, for an overnight flight, and 0 day for a regular flight.\nThe key fact that we need to know is whether the value of overnight is TRUE or FALSE , i.e., whether the flight is an overnight one or not. And, the fact that R treats TRUE as 1, and FALSE as 0."
  },
  {
    "objectID": "Chapter18.html#question-2-2",
    "href": "Chapter18.html#question-2-2",
    "title": "Chapter 18",
    "section": "Question 2",
    "text": "Question 2\nCreate a vector of dates giving the first day of every month in 2015. Create a vector of dates giving the first day of every month in the current year.\n\nseq(from = ymd(\"2015-01-01\"),\n    to   = ymd(\"2015-12-31\"),\n    by   = \"1 month\")\n\n [1] \"2015-01-01\" \"2015-02-01\" \"2015-03-01\" \"2015-04-01\" \"2015-05-01\"\n [6] \"2015-06-01\" \"2015-07-01\" \"2015-08-01\" \"2015-09-01\" \"2015-10-01\"\n[11] \"2015-11-01\" \"2015-12-01\"\n\nseq(from = ymd(paste0(year(today()), \"-01-01\")),\n    to   = ymd(paste0(year(today()), \"-12-31\")),\n    by   = \"1 month\")\n\n [1] \"2023-01-01\" \"2023-02-01\" \"2023-03-01\" \"2023-04-01\" \"2023-05-01\"\n [6] \"2023-06-01\" \"2023-07-01\" \"2023-08-01\" \"2023-09-01\" \"2023-10-01\"\n[11] \"2023-11-01\" \"2023-12-01\""
  },
  {
    "objectID": "Chapter18.html#question-3-2",
    "href": "Chapter18.html#question-3-2",
    "title": "Chapter 18",
    "section": "Question 3",
    "text": "Question 3\nWrite a function that given your birthday (as a date), returns how old you are in years.\nThe function find_age() is created below. An example is also shown: —\n\nfind_age = function(birthday){\n  age = floor((interval(parse_date(birthday), today()))/years(1))\n  \n  cat(\"You are\", age, \"years old.\")\n}\n\nfind_age(\"1991-09-24\")\n\nYou are 31 years old."
  },
  {
    "objectID": "Chapter18.html#question-4-1",
    "href": "Chapter18.html#question-4-1",
    "title": "Chapter 18",
    "section": "Question 4",
    "text": "Question 4\nWhy can’t (today() %--% (today() + years(1))) / months(1) work?\nThis expression should not work because the interval although the interval (today() %--% (today() + years(1))) can be computed; but, the duration of months(1) is not defined, since, each month is different in length - ranging from 28 days to 31 days. Hence, there is no clear accurate answer to the expression.\n\n(today() %--% (today() + years(1))) / months(1)\n\n[1] 12"
  },
  {
    "objectID": "Chapter19.html",
    "href": "Chapter19.html",
    "title": "Chapter 19",
    "section": "",
    "text": "library(tidyverse)\nlibrary(janitor)\nlibrary(gt)\nlibrary(gtExtras)"
  },
  {
    "objectID": "Chapter19.html#exercises",
    "href": "Chapter19.html#exercises",
    "title": "Chapter 19",
    "section": "19.3.4 Exercises",
    "text": "19.3.4 Exercises"
  },
  {
    "objectID": "Chapter19.html#question-1",
    "href": "Chapter19.html#question-1",
    "title": "Chapter 19",
    "section": "Question 1",
    "text": "Question 1\nCan you find any relationship between the carrier and the rows that appear to be missing from planes?\nYes, as we can see in the Table 1 , the airline carriers MQ and AA have most of their aircrafts’ tail numbers missing from the planes data-set, apart from few other carriers that have a small percentage of their data missing.\n\nlibrary(nycflights13)\ndata(\"flights\")\ndata(\"planes\")\n\n# Create a vector of carriers that have tailsnums missing in planes\ncar_vec = flights |&gt;\n  distinct(tailnum, carrier) |&gt;\n  anti_join(planes) |&gt;\n  distinct(carrier) |&gt;\n  as_vector() |&gt;\n  unname()\n\n# Find total tailnums for these carriers\ntotal_tails = flights |&gt;\n  filter(carrier %in% car_vec) |&gt;\n  group_by(carrier) |&gt;\n  summarize(\n    total_aircrafts = n_distinct(tailnum)\n    )\n\nflights |&gt;\n  distinct(tailnum, carrier) |&gt;\n  anti_join(planes) |&gt;\n  count(carrier, name = \"missing_tailnums\") |&gt;\n  full_join(total_tails) |&gt;\n  mutate(percentage_missing = missing_tailnums/total_aircrafts) |&gt;\n  arrange(desc(percentage_missing)) |&gt;\n  gt() |&gt;\n  gt_theme_538() |&gt;\n  cols_label_with(fn = ~ janitor::make_clean_names(., case = \"title\")) |&gt;\n  fmt_percent(columns = percentage_missing) |&gt;\n   tab_style(\n    style = list(cell_text(weight = \"bold\") ),\n    locations = cells_body(columns = percentage_missing)) \n\n\n\n\n\nTable 1:  Percentage of tail numbers of each carrier that is missing from\nplanes data set \n  \n    \n    \n      Carrier\n      Missing Tailnums\n      Total Aircrafts\n      Percentage Missing\n    \n  \n  \n    MQ\n234\n238\n98.32%\n    AA\n430\n601\n71.55%\n    F9\n3\n26\n11.54%\n    FL\n12\n129\n9.30%\n    UA\n23\n621\n3.70%\n    US\n9\n290\n3.10%\n    DL\n10\n629\n1.59%\n    B6\n3\n193\n1.55%\n    WN\n3\n583\n0.51%\n    9E\n1\n204\n0.49%"
  },
  {
    "objectID": "Chapter20.html",
    "href": "Chapter20.html",
    "title": "Chapter 20",
    "section": "",
    "text": "# Loading required datasets and libraries\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(nycflights13)\nlibrary(janitor)\ndata(\"flights\")\ndata(\"weather\")\ndata(\"airports\")"
  },
  {
    "objectID": "Chapter20.html#question-1",
    "href": "Chapter20.html#question-1",
    "title": "Chapter 20",
    "section": "Question 1",
    "text": "Question 1\nWe forgot to draw the relationship between weather and airports in Figure 20.1. What is the relationship and how should it appear in the diagram?\nThe relation between weather and airports is depicted below in the image adapted and copied from R for Data Science 2(e), Fig 20.1.\n\nThe primary key will be airports$faa .\nIt corresponds to a compound secondary key, weather$origin and weather$time_hour.\n\n\n\n\nThe relationship between datasets: weather and airports."
  },
  {
    "objectID": "Chapter20.html#question-2",
    "href": "Chapter20.html#question-2",
    "title": "Chapter 20",
    "section": "Question 2",
    "text": "Question 2\nweather only contains information for the three origin airports in NYC. If it contained weather records for all airports in the USA, what additional connection would it make to flights?\nIf weather contained the weather records for all airports in the USA, it would have made an additional connection to the variable dest in the flights dataset."
  },
  {
    "objectID": "Chapter20.html#question-3",
    "href": "Chapter20.html#question-3",
    "title": "Chapter 20",
    "section": "Question 3",
    "text": "Question 3\nThe year, month, day, hour, and origin variables almost form a compound key for weather, but there’s one hour that has duplicate observations. Can you figure out what’s special about that hour?\nAs we can see in the Table 1 , on November 3, 2013 at 1 am, we have a duplicate weather record. This means that the combination of year, month, day, hour, and origin variables does not form a compound key for weather , since some observations are not unique.\nThis happens because the daylight savings time clock changed on November 3, 2013 in New York City as follows: –\n\nStart of DST in 2013: Sunday, March 10, 2013 – 1 hour forward - 1 hour is skipped.\nEnd of DST in 2013: Sunday, November 3, 2013 – 1 hour backward at 1 am.\n\n\nweather |&gt;\n  group_by(year, month, day, hour, origin) |&gt;\n  count() |&gt;\n  filter(n &gt; 1) |&gt;\n  ungroup() |&gt;\n  gt() |&gt;\n  gt_theme_538()\n\n\n\n\n\nTable 1:  Day and hour that has two weather reports \n  \n    \n    \n      year\n      month\n      day\n      hour\n      origin\n      n\n    \n  \n  \n    2013\n11\n3\n1\nEWR\n2\n    2013\n11\n3\n1\nJFK\n2\n    2013\n11\n3\n1\nLGA\n2"
  },
  {
    "objectID": "Chapter20.html#question-4",
    "href": "Chapter20.html#question-4",
    "title": "Chapter 20",
    "section": "Question 4",
    "text": "Question 4\nWe know that some days of the year are special and fewer people than usual fly on them (e.g., Christmas eve and Christmas day). How might you represent that data as a data frame? What would be the primary key? How would it connect to the existing data frames?\nWe can create a data frame or a tibble, as shown in the code below, named holidays to represent holidays and the pre-holiday days.\nThe primary key would be a compound key of year , month and day. It would connect to the existing data frames using a secondary compound key of of year , month and day.\n[Note: to make things easier, without using a compound key, I have used the make_date() function to create a single key flight_date() ]\n\n\nCode\n# Create a tibble for the major holidays in the USA in 2013\nholidays &lt;- tibble(\n  year = 2013,\n  month = c(1, 2, 5, 7, 9, 10, 11, 12),\n  day = c(1, 14, 27, 4, 2, 31, 28, 25),\n  holiday_name = c(\n    \"New Year's Day\",\n    \"Valentine's Day\",\n    \"Memorial Day\",\n    \"Independence Day\",\n    \"Labor Day\",\n    \"Halloween\",\n    \"Thanksgiving\",\n    \"Christmas Day\"\n  ),\n  holiday_type = \"Holiday\"\n)\n\n# Computing the pre-holiday date and adding it to holidays\nholidays &lt;- bind_rows(\n  # Exisitng tibble of holidays\n  holidays,\n  # A new tibble of holiday eves\n  holidays |&gt;\n  mutate(\n    day = day-1,\n    holiday_name = str_c(holiday_name, \" Eve\"),\n    holiday_type = \"Pre-Holiday\"\n  ) |&gt;\n  slice(2:8)\n) |&gt;\n  mutate(flight_date = make_date(year, month, day))\n\n# Display\nholidays |&gt; \n  gt() |&gt; \n  # cols_label_with(fn = ~ make_clean_names(., case = \"title\")) |&gt;\n  gt_theme_nytimes()\n\n\n\n\n\n\n  \n    \n    \n      year\n      month\n      day\n      holiday_name\n      holiday_type\n      flight_date\n    \n  \n  \n    2013\n1\n1\nNew Year's Day\nHoliday\n2013-01-01\n    2013\n2\n14\nValentine's Day\nHoliday\n2013-02-14\n    2013\n5\n27\nMemorial Day\nHoliday\n2013-05-27\n    2013\n7\n4\nIndependence Day\nHoliday\n2013-07-04\n    2013\n9\n2\nLabor Day\nHoliday\n2013-09-02\n    2013\n10\n31\nHalloween\nHoliday\n2013-10-31\n    2013\n11\n28\nThanksgiving\nHoliday\n2013-11-28\n    2013\n12\n25\nChristmas Day\nHoliday\n2013-12-25\n    2013\n2\n13\nValentine's Day Eve\nPre-Holiday\n2013-02-13\n    2013\n5\n26\nMemorial Day Eve\nPre-Holiday\n2013-05-26\n    2013\n7\n3\nIndependence Day Eve\nPre-Holiday\n2013-07-03\n    2013\n9\n1\nLabor Day Eve\nPre-Holiday\n2013-09-01\n    2013\n10\n30\nHalloween Eve\nPre-Holiday\n2013-10-30\n    2013\n11\n27\nThanksgiving Eve\nPre-Holiday\n2013-11-27\n    2013\n12\n24\nChristmas Day Eve\nPre-Holiday\n2013-12-24\n  \n  \n  \n\n\n\n\nNow, we can use this new tibble, join it with our existing data sets and try to figure out whether there is any difference in number of flights on holidays, and pre-holidays, vs. the rest of the days. The results are in Figure 1 .\n\n\nCode\n# A tibble on the number of flights each day, along with whether each day \n# is holiday or not; and if yes, which holiday\nnos_flights &lt;- flights |&gt;\n  mutate(flight_date = make_date(year, month, day)) |&gt;\n  left_join(holidays) |&gt;\n  group_by(flight_date, holiday_type, holiday_name) |&gt;\n  count()\n\nnos_flights |&gt;\n  group_by(holiday_type) |&gt;\n  summarize(avg_flights = mean(n)) |&gt;\n  mutate(holiday_type = if_else(is.na(holiday_type),\n                                \"Other Days\",\n                                holiday_type)) |&gt;\n  ggplot(aes(x = avg_flights,\n             y = reorder(holiday_type, avg_flights))) +\n  geom_bar(stat = \"identity\", fill = \"grey\") +\n  theme_minimal() +\n  theme(panel.grid = element_blank()) +\n  labs(y = NULL, x = \"Average Number of flights (per day)\",\n       title = \"Holidays / pre-holiday have lower number of flights, on average\") +\n  theme(plot.title.position = \"plot\")\n\n\n\n\n\nFigure 1: Average number of flights on holidays vs pre-holidays vs rest of the days\n\n\n\n\nThe number of flights on various holidays and pre-holiday days is shown in Figure 2 .\n\n\nCode\nnos_flights |&gt;\n  group_by(holiday_name) |&gt;\n  summarize(avg_flights = mean(n)) |&gt;\n  mutate(holiday_name = if_else(is.na(holiday_name),\n                                \"Other Days\",\n                                holiday_name)) |&gt;\n  mutate(col_var = holiday_name == \"Other Days\") |&gt;\n  ggplot(aes(x = avg_flights,\n             y = reorder(holiday_name, avg_flights),\n             fill = col_var,\n             label = round(avg_flights, 0))) +\n  geom_bar(stat = \"identity\") +\n  geom_text(nudge_x = 20, size = 3) +\n  theme_minimal() +\n  theme(panel.grid = element_blank(),\n        plot.title.position = \"plot\",\n        legend.position = \"none\") +\n  labs(y = NULL, x = \"Number of flights (per day)\") +\n  scale_fill_brewer(palette = \"Paired\") +\n  coord_cartesian(xlim = c(500, 1050))\n\n\n\n\n\nFigure 2: Average number of flights on some important days vs others"
  },
  {
    "objectID": "Chapter20.html#question-5",
    "href": "Chapter20.html#question-5",
    "title": "Chapter 20",
    "section": "Question 5",
    "text": "Question 5\nDraw a diagram illustrating the connections between the Batting, People, and Salaries data frames in the Lahman package. Draw another diagram that shows the relationship between People, Managers, AwardsManagers. How would you characterize the relationship between the Batting, Pitching, and Fielding data frames?\nThe data-frames are shown below, alongwith the check that playerID is a key: –\nIn Batting , the variables playerID , yearID and stint form a compound key.\n\nlibrary(Lahman)\nBatting |&gt; as_tibble() |&gt;\n  group_by(playerID, yearID, stint) |&gt;\n  count() |&gt;\n  filter(n &gt; 1)\n\n# A tibble: 0 × 4\n# Groups:   playerID, yearID, stint [0]\n# ℹ 4 variables: playerID &lt;chr&gt;, yearID &lt;int&gt;, stint &lt;int&gt;, n &lt;int&gt;\n\nhead(Batting) |&gt; tibble() |&gt; \n  gt() |&gt; gt_theme_538() |&gt;\n   tab_style(\n    style = list(cell_fill(color = \"yellow\"),\n                 cell_text(weight = \"bold\")),\n    locations = cells_body(columns = c(playerID, yearID, stint))\n  ) |&gt;\n  tab_header(title = md(\"**`Batting`**\"))\n\n\n\n\n\n  \n    \n      Batting\n    \n    \n    \n      playerID\n      yearID\n      stint\n      teamID\n      lgID\n      G\n      AB\n      R\n      H\n      X2B\n      X3B\n      HR\n      RBI\n      SB\n      CS\n      BB\n      SO\n      IBB\n      HBP\n      SH\n      SF\n      GIDP\n    \n  \n  \n    abercda01\n1871\n1\nTRO\nNA\n1\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nNA\nNA\nNA\nNA\n0\n    addybo01\n1871\n1\nRC1\nNA\n25\n118\n30\n32\n6\n0\n0\n13\n8\n1\n4\n0\nNA\nNA\nNA\nNA\n0\n    allisar01\n1871\n1\nCL1\nNA\n29\n137\n28\n40\n4\n5\n0\n19\n3\n1\n2\n5\nNA\nNA\nNA\nNA\n1\n    allisdo01\n1871\n1\nWS3\nNA\n27\n133\n28\n44\n10\n2\n2\n27\n1\n1\n0\n2\nNA\nNA\nNA\nNA\n0\n    ansonca01\n1871\n1\nRC1\nNA\n25\n120\n29\n39\n11\n3\n0\n16\n6\n2\n2\n1\nNA\nNA\nNA\nNA\n0\n    armstbo01\n1871\n1\nFW1\nNA\n12\n49\n9\n11\n2\n1\n0\n5\n0\n1\n0\n1\nNA\nNA\nNA\nNA\n0\n  \n  \n  \n\n\n\n\nIn People, the variable playerID is unique for each observation, and hence a primary key.\n\nPeople |&gt; \n  as_tibble() |&gt;\n  group_by(playerID) |&gt;\n  count() |&gt;\n  filter(n &gt; 1)\n\n# A tibble: 0 × 2\n# Groups:   playerID [0]\n# ℹ 2 variables: playerID &lt;chr&gt;, n &lt;int&gt;\n\nhead(People) |&gt; tibble() |&gt; \n  gt() |&gt; gt_theme_538() |&gt;\n   tab_style(\n    style = list(cell_fill(color = \"yellow\"),\n                 cell_text(weight = \"bold\")),\n    locations = cells_body(columns = c(playerID))\n  ) |&gt;\n  tab_header(title = md(\"**`People`**\"))\n\n\n\n\n\n  \n    \n      People\n    \n    \n    \n      playerID\n      birthYear\n      birthMonth\n      birthDay\n      birthCountry\n      birthState\n      birthCity\n      deathYear\n      deathMonth\n      deathDay\n      deathCountry\n      deathState\n      deathCity\n      nameFirst\n      nameLast\n      nameGiven\n      weight\n      height\n      bats\n      throws\n      debut\n      finalGame\n      retroID\n      bbrefID\n      deathDate\n      birthDate\n    \n  \n  \n    aardsda01\n1981\n12\n27\nUSA\nCO\nDenver\nNA\nNA\nNA\nNA\nNA\nNA\nDavid\nAardsma\nDavid Allan\n215\n75\nR\nR\n2004-04-06\n2015-08-23\naardd001\naardsda01\nNA\n1981-12-27\n    aaronha01\n1934\n2\n5\nUSA\nAL\nMobile\n2021\n1\n22\nUSA\nGA\nAtlanta\nHank\nAaron\nHenry Louis\n180\n72\nR\nR\n1954-04-13\n1976-10-03\naaroh101\naaronha01\n2021-01-22\n1934-02-05\n    aaronto01\n1939\n8\n5\nUSA\nAL\nMobile\n1984\n8\n16\nUSA\nGA\nAtlanta\nTommie\nAaron\nTommie Lee\n190\n75\nR\nR\n1962-04-10\n1971-09-26\naarot101\naaronto01\n1984-08-16\n1939-08-05\n    aasedo01\n1954\n9\n8\nUSA\nCA\nOrange\nNA\nNA\nNA\nNA\nNA\nNA\nDon\nAase\nDonald William\n190\n75\nR\nR\n1977-07-26\n1990-10-03\naased001\naasedo01\nNA\n1954-09-08\n    abadan01\n1972\n8\n25\nUSA\nFL\nPalm Beach\nNA\nNA\nNA\nNA\nNA\nNA\nAndy\nAbad\nFausto Andres\n184\n73\nL\nL\n2001-09-10\n2006-04-13\nabada001\nabadan01\nNA\n1972-08-25\n    abadfe01\n1985\n12\n17\nD.R.\nLa Romana\nLa Romana\nNA\nNA\nNA\nNA\nNA\nNA\nFernando\nAbad\nFernando Antonio\n235\n74\nL\nL\n2010-07-28\n2021-10-01\nabadf001\nabadfe01\nNA\n1985-12-17\n  \n  \n  \n\n\n\n\nIn Salaries the variables playerID , yearID and stint form a compound key.\n\nSalaries |&gt; \n  as_tibble() |&gt;\n  group_by(playerID, yearID, teamID) |&gt;\n  count() |&gt;\n  filter(n &gt; 1)\n\n# A tibble: 0 × 4\n# Groups:   playerID, yearID, teamID [0]\n# ℹ 4 variables: playerID &lt;chr&gt;, yearID &lt;int&gt;, teamID &lt;fct&gt;, n &lt;int&gt;\n\nhead(Salaries) |&gt; tibble() |&gt; \n  gt() |&gt; gt_theme_538() |&gt;\n   tab_style(\n    style = list(cell_fill(color = \"yellow\"),\n                 cell_text(weight = \"bold\")),\n    locations = cells_body(columns = c(playerID, yearID, teamID))\n  )|&gt;\n  tab_header(title = md(\"**`Salaries`**\"))\n\n\n\n\n\n  \n    \n      Salaries\n    \n    \n    \n      yearID\n      teamID\n      lgID\n      playerID\n      salary\n    \n  \n  \n    1985\nATL\nNL\nbarkele01\n870000\n    1985\nATL\nNL\nbedrost01\n550000\n    1985\nATL\nNL\nbenedbr01\n545000\n    1985\nATL\nNL\ncampri01\n633333\n    1985\nATL\nNL\nceronri01\n625000\n    1985\nATL\nNL\nchambch01\n800000\n  \n  \n  \n\n\n\n\nThe diagram illustrating the connections is shown below: –\n\nNow, we show another diagram that shows the relationship between People, Managers, AwardsManagers.\nFor Managers, the key is a compound key of playerID, yearID and inseason\n\nhead(Managers)\n\n   playerID yearID teamID lgID inseason  G  W  L rank plyrMgr\n1 wrighha01   1871    BS1   NA        1 31 20 10    3       Y\n2  woodji01   1871    CH1   NA        1 28 19  9    2       Y\n3 paborch01   1871    CL1   NA        1 29 10 19    8       Y\n4 lennobi01   1871    FW1   NA        1 14  5  9    8       Y\n5 deaneha01   1871    FW1   NA        2  5  2  3    8       Y\n6 fergubo01   1871    NY2   NA        1 33 16 17    5       Y\n\nManagers |&gt;\n  as_tibble() |&gt;\n  group_by(playerID, yearID, inseason) |&gt;\n  count() |&gt;\n  filter(n &gt; 1)\n\n# A tibble: 0 × 4\n# Groups:   playerID, yearID, inseason [0]\n# ℹ 4 variables: playerID &lt;chr&gt;, yearID &lt;int&gt;, inseason &lt;int&gt;, n &lt;int&gt;\n\nhead(Managers) |&gt; as_tibble() |&gt;\n  gt() |&gt; \n  gt_theme_538() |&gt;\n  tab_style(\n    style = list(cell_fill(color = \"yellow\"),\n                 cell_text(weight = \"bold\")),\n    locations = cells_body(columns = c(playerID, yearID, inseason))\n  ) |&gt;\n  tab_header(title = md(\"**`Managers`**\"))\n\n\n\n\n\n  \n    \n      Managers\n    \n    \n    \n      playerID\n      yearID\n      teamID\n      lgID\n      inseason\n      G\n      W\n      L\n      rank\n      plyrMgr\n    \n  \n  \n    wrighha01\n1871\nBS1\nNA\n1\n31\n20\n10\n3\nY\n    woodji01\n1871\nCH1\nNA\n1\n28\n19\n9\n2\nY\n    paborch01\n1871\nCL1\nNA\n1\n29\n10\n19\n8\nY\n    lennobi01\n1871\nFW1\nNA\n1\n14\n5\n9\n8\nY\n    deaneha01\n1871\nFW1\nNA\n2\n5\n2\n3\n8\nY\n    fergubo01\n1871\nNY2\nNA\n1\n33\n16\n17\n5\nY\n  \n  \n  \n\n\n\n\nFor AwardsManagers , the primary key is a compound key of playerID , awardID and yearID .\n\nhead(AwardsManagers)\n\n   playerID                   awardID yearID lgID  tie notes\n1 larusto01 BBWAA Manager of the Year   1983   AL &lt;NA&gt;    NA\n2 lasorto01 BBWAA Manager of the Year   1983   NL &lt;NA&gt;    NA\n3 andersp01 BBWAA Manager of the Year   1984   AL &lt;NA&gt;    NA\n4  freyji99 BBWAA Manager of the Year   1984   NL &lt;NA&gt;    NA\n5   coxbo01 BBWAA Manager of the Year   1985   AL &lt;NA&gt;    NA\n6 herzowh01 BBWAA Manager of the Year   1985   NL &lt;NA&gt;    NA\n\nAwardsManagers |&gt;\n  as_tibble() |&gt;\n  group_by(playerID, awardID, yearID) |&gt;\n  count() |&gt;\n  filter(n &gt; 1)\n\n# A tibble: 0 × 4\n# Groups:   playerID, awardID, yearID [0]\n# ℹ 4 variables: playerID &lt;chr&gt;, awardID &lt;chr&gt;, yearID &lt;int&gt;, n &lt;int&gt;\n\nhead(AwardsManagers) |&gt; as_tibble() |&gt;\n  gt() |&gt; \n  gt_theme_538() |&gt;\n  tab_style(\n    style = list(cell_fill(color = \"yellow\"),\n                 cell_text(weight = \"bold\")),\n    locations = cells_body(columns = c(playerID, yearID, awardID))\n  ) |&gt;\n  tab_header(title = md(\"**`AwardsManagers`**\"))\n\n\n\n\n\n  \n    \n      AwardsManagers\n    \n    \n    \n      playerID\n      awardID\n      yearID\n      lgID\n      tie\n      notes\n    \n  \n  \n    larusto01\nBBWAA Manager of the Year\n1983\nAL\nNA\nNA\n    lasorto01\nBBWAA Manager of the Year\n1983\nNL\nNA\nNA\n    andersp01\nBBWAA Manager of the Year\n1984\nAL\nNA\nNA\n    freyji99\nBBWAA Manager of the Year\n1984\nNL\nNA\nNA\n    coxbo01\nBBWAA Manager of the Year\n1985\nAL\nNA\nNA\n    herzowh01\nBBWAA Manager of the Year\n1985\nNL\nNA\nNA\n  \n  \n  \n\n\n\n\nHence, the relationship between People, Managers, AwardsManagers is as follows: –\n\nNow, let’s try to characterize the relationship between Batting , Pitching and Fielding.\n\nPitching |&gt; as_tibble() |&gt;\n  group_by(playerID, yearID, stint) |&gt;\n  count() |&gt;\n  filter(n &gt; 1)\n\n# A tibble: 0 × 4\n# Groups:   playerID, yearID, stint [0]\n# ℹ 4 variables: playerID &lt;chr&gt;, yearID &lt;int&gt;, stint &lt;int&gt;, n &lt;int&gt;\n\nhead(Pitching) |&gt; as_tibble() |&gt;\n  gt() |&gt; \n  gt_theme_538() |&gt;\n  tab_style(\n    style = list(cell_fill(color = \"yellow\"),\n                 cell_text(weight = \"bold\")),\n    locations = cells_body(columns = c(playerID, yearID, stint))\n  ) |&gt;\n  tab_header(title = md(\"**`Pitching`**\"))\n\n\n\n\n\n  \n    \n      Pitching\n    \n    \n    \n      playerID\n      yearID\n      stint\n      teamID\n      lgID\n      W\n      L\n      G\n      GS\n      CG\n      SHO\n      SV\n      IPouts\n      H\n      ER\n      HR\n      BB\n      SO\n      BAOpp\n      ERA\n      IBB\n      WP\n      HBP\n      BK\n      BFP\n      GF\n      R\n      SH\n      SF\n      GIDP\n    \n  \n  \n    bechtge01\n1871\n1\nPH1\nNA\n1\n2\n3\n3\n2\n0\n0\n78\n43\n23\n0\n11\n1\nNA\n7.96\nNA\n7\nNA\n0\n146\n0\n42\nNA\nNA\nNA\n    brainas01\n1871\n1\nWS3\nNA\n12\n15\n30\n30\n30\n0\n0\n792\n361\n132\n4\n37\n13\nNA\n4.50\nNA\n7\nNA\n0\n1291\n0\n292\nNA\nNA\nNA\n    fergubo01\n1871\n1\nNY2\nNA\n0\n0\n1\n0\n0\n0\n0\n3\n8\n3\n0\n0\n0\nNA\n27.00\nNA\n2\nNA\n0\n14\n0\n9\nNA\nNA\nNA\n    fishech01\n1871\n1\nRC1\nNA\n4\n16\n24\n24\n22\n1\n0\n639\n295\n103\n3\n31\n15\nNA\n4.35\nNA\n20\nNA\n0\n1080\n1\n257\nNA\nNA\nNA\n    fleetfr01\n1871\n1\nNY2\nNA\n0\n1\n1\n1\n1\n0\n0\n27\n20\n10\n0\n3\n0\nNA\n10.00\nNA\n0\nNA\n0\n57\n0\n21\nNA\nNA\nNA\n    flowedi01\n1871\n1\nTRO\nNA\n0\n0\n1\n0\n0\n0\n0\n3\n1\n0\n0\n0\n0\nNA\n0.00\nNA\n0\nNA\n0\n3\n1\n0\nNA\nNA\nNA\n  \n  \n  \n\n\n\n\nIn the Fielding dataset, the primary key is a compound key comprised of playerID , yearID , stint and POS.\n\nFielding |&gt; as_tibble() |&gt;\n  group_by(playerID, yearID, stint, POS) |&gt;\n  count() |&gt;\n  filter(n &gt; 1)\n\n# A tibble: 0 × 5\n# Groups:   playerID, yearID, stint, POS [0]\n# ℹ 5 variables: playerID &lt;chr&gt;, yearID &lt;int&gt;, stint &lt;int&gt;, POS &lt;chr&gt;, n &lt;int&gt;\n\nhead(Fielding) |&gt; as_tibble() |&gt;\n  gt() |&gt; \n  gt_theme_538() |&gt;\n  tab_style(\n    style = list(cell_fill(color = \"yellow\"),\n                 cell_text(weight = \"bold\")),\n    locations = cells_body(columns = c(playerID, yearID, stint, POS))\n  ) |&gt;\n  tab_header(title = md(\"**`Fielding`**\"))\n\n\n\n\n\n  \n    \n      Fielding\n    \n    \n    \n      playerID\n      yearID\n      stint\n      teamID\n      lgID\n      POS\n      G\n      GS\n      InnOuts\n      PO\n      A\n      E\n      DP\n      PB\n      WP\n      SB\n      CS\n      ZR\n    \n  \n  \n    abercda01\n1871\n1\nTRO\nNA\nSS\n1\n1\n24\n1\n3\n2\n0\nNA\nNA\nNA\nNA\nNA\n    addybo01\n1871\n1\nRC1\nNA\n2B\n22\n22\n606\n67\n72\n42\n5\nNA\nNA\nNA\nNA\nNA\n    addybo01\n1871\n1\nRC1\nNA\nSS\n3\n3\n96\n8\n14\n7\n0\nNA\nNA\nNA\nNA\nNA\n    allisar01\n1871\n1\nCL1\nNA\n2B\n2\n0\n18\n1\n4\n0\n0\nNA\nNA\nNA\nNA\nNA\n    allisar01\n1871\n1\nCL1\nNA\nOF\n29\n29\n729\n51\n3\n7\n1\nNA\nNA\nNA\nNA\nNA\n    allisdo01\n1871\n1\nWS3\nNA\nC\n27\n27\n681\n68\n15\n20\n4\n18\nNA\n0\n0\nNA\n  \n  \n  \n\n\n\n\nThus, the relationship between the Batting, Pitching, and Fielding data frames is as follows: –"
  },
  {
    "objectID": "Chapter20.html#question-1-1",
    "href": "Chapter20.html#question-1-1",
    "title": "Chapter 20",
    "section": "Question 1",
    "text": "Question 1\nFind the 48 hours (over the course of the whole year) that have the worst delays. Cross-reference it with the weather data. Can you see any patterns?\nFirst, we find out the 48 hours (over the course of the whole year) that have the worst delays. As we can see in Figure 3 , these are quite similar across the 3 origin airports, for which we have the weather data.\n\n\nCode\n# Create a dataframe of 48 hours with highestaverage delays \n# (for each of the 3 origin airports)\ndelayhours = flights |&gt;\n  group_by(origin, time_hour) |&gt;\n  summarize(avg_delay = mean(dep_delay, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_delay), .by_group = TRUE) |&gt;\n  slice_head(n = 48) |&gt;\n  arrange(time_hour)\n\ndelayhours |&gt;\n  ggplot(aes(y = time_hour, x = avg_delay)) +\n  geom_point(size = 2, alpha = 0.5) +\n  facet_wrap(~origin, dir = \"h\") +\n  theme_minimal() +\n  labs(x = \"Average delay during the hour (in mins.)\", y = NULL,\n       title = \"The worst 48 hours for departure delays are similar across 3 airports\")\n\n\n\n\n\nFigure 3: Distribution of the 48 worst delay hours over the course of the year in three airports of New York City\n\n\n\n\nThe Figure 4 depicts that across the three airports, the 48 hours with worst delays consistently have much higher rainfall (precipitation in inches) and poorer visibility (lower visibility in miles and higher dew-point in degrees F).\n\n\nCode\nvar_labels = c(\"Temperature (F)\", \"Dewpoint (F)\", \n               \"Relative Humidity %\", \"Precipitation (inches)\", \n               \"Visibility (miles)\")\nnames(var_labels) = c(\"temp\", \"dewp\", \"humid\", \"precip\", \"visib\")\n\ng1 = weather |&gt;\n  filter(origin == \"EWR\") |&gt;\n  left_join(delayhours) |&gt;\n  mutate(\n    del_hrs = if_else(is.na(avg_delay),\n                      \"Other hours\",\n                      \"Hours with max delays\"),\n    precip = precip * 25.4\n  ) |&gt;\n  pivot_longer(\n    cols = c(temp, dewp, humid, precip, visib),\n    names_to = \"variable\",\n    values_to = \"values\"\n  ) |&gt;\n  group_by(origin, del_hrs, variable) |&gt;\n  summarise(means = mean(values, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = del_hrs, y = means, fill = del_hrs)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap( ~ variable, scales = \"free\", ncol = 5,\n              labeller = labeller(variable = var_labels)) +\n  scale_fill_brewer(palette = \"Dark2\") + \n  theme_minimal() +\n  theme(panel.grid.major.x = element_blank(),\n        axis.title = element_blank(),\n        axis.text.x = element_blank(),\n        legend.position = \"bottom\") +\n  labs(subtitle = \"Weather Patterns for Newark Airport (EWR)\",\n       fill = \"\")\n  \ng2 = weather |&gt;\n  filter(origin == \"JFK\") |&gt;\n  left_join(delayhours) |&gt;\n  mutate(\n    del_hrs = if_else(is.na(avg_delay),\n                      \"Other hours\",\n                      \"Hours with max delays\"),\n    precip = precip * 25.4\n  ) |&gt;\n  pivot_longer(\n    cols = c(temp, dewp, humid, precip, visib),\n    names_to = \"variable\",\n    values_to = \"values\"\n  ) |&gt;\n  group_by(origin, del_hrs, variable) |&gt;\n  summarise(means = mean(values, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = del_hrs, y = means, fill = del_hrs)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap( ~ variable, scales = \"free\", ncol = 5,\n              labeller = labeller(variable = var_labels)) +\n  scale_fill_brewer(palette = \"Dark2\") + \n  theme_minimal() +\n  theme(panel.grid.major.x = element_blank(),\n        axis.title = element_blank(),\n        axis.text.x = element_blank(),\n        legend.position = \"bottom\")  +\n  labs(subtitle = \"Weather Patterns for John F Kennedy Airport (JFK)\",\n       fill = \"\")\n\ng3 = weather |&gt;\n  filter(origin == \"LGA\") |&gt;\n  left_join(delayhours) |&gt;\n  mutate(\n    del_hrs = if_else(is.na(avg_delay),\n                      \"Other hours\",\n                      \"Hours with max delays\"),\n    precip = precip * 25.4\n  ) |&gt;\n  pivot_longer(\n    cols = c(temp, dewp, humid, precip, visib),\n    names_to = \"variable\",\n    values_to = \"values\"\n  ) |&gt;\n  group_by(origin, del_hrs, variable) |&gt;\n  summarise(means = mean(values, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = del_hrs, y = means, fill = del_hrs)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap( ~ variable, scales = \"free\", ncol = 5,\n              labeller = labeller(variable = var_labels)) +\n  scale_fill_brewer(palette = \"Dark2\") + \n  theme_minimal() +\n  theme(panel.grid.major.x = element_blank(),\n        axis.title = element_blank(),\n        axis.text.x = element_blank(),\n        legend.position = \"bottom\")  +\n  labs(subtitle = \"Weather Patterns for La Guardia Airport (LGA)\",\n       fill = \"\") \n\nlibrary(patchwork)\n\ng1 / g2 / g3 + plot_layout(guides = \"collect\") & theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 4: Comparison of weather patterns for hours with worst delays vs the rest"
  },
  {
    "objectID": "Chapter20.html#question-2-1",
    "href": "Chapter20.html#question-2-1",
    "title": "Chapter 20",
    "section": "Question 2",
    "text": "Question 2\nImagine you’ve found the top 10 most popular destinations using this code:\ntop_dest &lt;- flights2 |&gt;   \n  count(dest, sort = TRUE) |&gt;   \n  head(10)\nHow can you find all flights to those destinations?\nWe can first create a vector of the names of the top 10 destinations, using select(dest) and as_vector() . Thereafter, we can filter(dest %in% top_dest_vec) as shown below: –\n\nflights2 &lt;- flights |&gt; \n  mutate(id = row_number(), .before = 1)\ntop_dest &lt;- flights2 |&gt;   \n  count(dest, sort = TRUE) |&gt;   \n  head(10)\ntop_dest_vec &lt;- top_dest |&gt; select(dest) |&gt; as_vector()\nflights |&gt;\n  filter(dest %in% top_dest_vec) \n\n# A tibble: 141,145 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      542            540         2      923            850\n 2  2013     1     1      554            600        -6      812            837\n 3  2013     1     1      554            558        -4      740            728\n 4  2013     1     1      555            600        -5      913            854\n 5  2013     1     1      557            600        -3      838            846\n 6  2013     1     1      558            600        -2      753            745\n 7  2013     1     1      558            600        -2      924            917\n 8  2013     1     1      558            600        -2      923            937\n 9  2013     1     1      559            559         0      702            706\n10  2013     1     1      600            600         0      851            858\n# ℹ 141,135 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;"
  },
  {
    "objectID": "Chapter20.html#question-3-1",
    "href": "Chapter20.html#question-3-1",
    "title": "Chapter 20",
    "section": "Question 3",
    "text": "Question 3\nDoes every departing flight have corresponding weather data for that hour?\nNo, as we can see from the code below, every departing flight DOES NOT have corresponding weather data for that hour. 1556 flights do not have associated weather data; and these correspond to 38 different hours during the year.\n\n# Number of flights that do not have associated weather data\nflights |&gt;\n  anti_join(weather) |&gt;\n  nrow()\n\n[1] 1556\n\n# Number of distinct time_hours that do not have such data\nflights |&gt;\n  anti_join(weather) |&gt;\n  distinct(time_hour)\n\n# A tibble: 48 × 1\n   time_hour          \n   &lt;dttm&gt;             \n 1 2013-01-01 12:00:00\n 2 2013-01-06 06:00:00\n 3 2013-10-23 06:00:00\n 4 2013-10-23 07:00:00\n 5 2013-10-25 23:00:00\n 6 2013-10-25 20:00:00\n 7 2013-10-25 21:00:00\n 8 2013-10-25 22:00:00\n 9 2013-10-26 21:00:00\n10 2013-11-02 20:00:00\n# ℹ 38 more rows\n\n# A check to confirm our results\nflights |&gt;\n  select(year, month, day, origin, dest, time_hour) |&gt;\n  left_join(weather) |&gt;\n  summarise(\n    missing_temp_or_windspeed = mean(is.na(temp) & is.na(wind_speed)),\n    missing_dewp = mean(is.na(dewp))\n  )\n\n# A tibble: 1 × 2\n  missing_temp_or_windspeed missing_dewp\n                      &lt;dbl&gt;        &lt;dbl&gt;\n1                   0.00462      0.00467\n\n(as.numeric(flights |&gt; anti_join(weather) |&gt; nrow())) / nrow(flights)\n\n[1] 0.004620282"
  },
  {
    "objectID": "Chapter20.html#question-4-1",
    "href": "Chapter20.html#question-4-1",
    "title": "Chapter 20",
    "section": "Question 4",
    "text": "Question 4\nWhat do the tail numbers that don’t have a matching record in planes have in common? (Hint: one variable explains ~90% of the problems.)\nThe tail numbers that don’t have a matching record in planes mostly belong the a select few airline carriers, i.e., AA and MQ . The variable carrier explains most of the problems in missing data, as shown in Figure 5.\n\n\nCode\n# Create a unique flight ID for each flight\nflights2 &lt;- flights |&gt;\n  mutate(id = row_number(), .before = 1)\n  \nids_no_record = flights2 |&gt;\n  anti_join(planes, by = join_by(tailnum)) |&gt;\n  select(id) |&gt;\n  as_vector() |&gt; unname()\n\nflights2 = flights2 |&gt;\n  mutate(\n    missing_record = id %in% ids_no_record\n  )\n\nlabel_vec = c(\"Flights with missing tailnum in planes\", \"Other flights\")\nnames(label_vec) = c(FALSE, TRUE)\n\nflights2 |&gt;\n  group_by(missing_record) |&gt;\n  count(carrier) |&gt;\n  mutate(col_var = carrier %in% c(\"MQ\", \"AA\")) |&gt;\n  ggplot(aes(x = n, y = carrier, fill = col_var)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ missing_record, \n             scales = \"free_x\", \n             labeller = labeller(missing_record = label_vec)) +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Number of flights\",  y = \"Carrier\",\n       title = \"Flights with missing tailnum in planes belong to a select few carriers\") + \n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n\nFigure 5: Bar Chart of number of flights per carrier"
  },
  {
    "objectID": "Chapter20.html#question-5-1",
    "href": "Chapter20.html#question-5-1",
    "title": "Chapter 20",
    "section": "Question 5",
    "text": "Question 5\nAdd a column to planes that lists every carrier that has flown that plane. You might expect that there’s an implicit relationship between plane and airline, because each plane is flown by a single airline. Confirm or reject this hypothesis using the tools you’ve learned in previous chapters.\nUsing the code below, we confirm that there are 17 such different airplanes (identified by tailnum) that have been flown by two carriers. These are shown in Figure 6 .\n\n\nCode\n# Displaying tail numbers which have been used by more than one carriers\nflights |&gt;\n  group_by(tailnum) |&gt;\n  summarise(number_of_carriers = n_distinct(carrier)) |&gt;\n  filter(number_of_carriers &gt; 1) |&gt;\n  drop_na() |&gt;\n  gt() |&gt;\n  opt_interactive(page_size_default = 5,\n                  use_highlight = TRUE, \n                  pagination_type = \"simple\") |&gt;\n  cols_label_with(fn = ~ janitor::make_clean_names(., case = \"title\"))\n\n\n\n\n\n\n\n\n\nFigure 6: ?(caption)\n\n\n\nThe following code adds a column to planes that lists every carrier that has flown that plane.\n\n# A tibble that lists all carriers a tailnum has flown\nall_carrs = flights |&gt;\n  group_by(tailnum) |&gt;\n  distinct(carrier) |&gt;\n  summarise(carriers = paste0(carrier, collapse = \", \")) |&gt;\n  arrange(desc(str_length(carriers)))\n# Display the tibble\nslice_head(all_carrs, n= 30) |&gt;\n   gt() |&gt; opt_interactive(page_size_default = 5)\n\n\n\n\n\n\n\n\n# Merge with planes\nplanes |&gt;\n  left_join(all_carrs)\n\n# A tibble: 3,322 × 10\n   tailnum  year type     manufacturer model engines seats speed engine carriers\n   &lt;chr&gt;   &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;   \n 1 N10156   2004 Fixed w… EMBRAER      EMB-…       2    55    NA Turbo… EV      \n 2 N102UW   1998 Fixed w… AIRBUS INDU… A320…       2   182    NA Turbo… US      \n 3 N103US   1999 Fixed w… AIRBUS INDU… A320…       2   182    NA Turbo… US      \n 4 N104UW   1999 Fixed w… AIRBUS INDU… A320…       2   182    NA Turbo… US      \n 5 N10575   2002 Fixed w… EMBRAER      EMB-…       2    55    NA Turbo… EV      \n 6 N105UW   1999 Fixed w… AIRBUS INDU… A320…       2   182    NA Turbo… US      \n 7 N107US   1999 Fixed w… AIRBUS INDU… A320…       2   182    NA Turbo… US      \n 8 N108UW   1999 Fixed w… AIRBUS INDU… A320…       2   182    NA Turbo… US      \n 9 N109UW   1999 Fixed w… AIRBUS INDU… A320…       2   182    NA Turbo… US      \n10 N110UW   1999 Fixed w… AIRBUS INDU… A320…       2   182    NA Turbo… US      \n# ℹ 3,312 more rows"
  },
  {
    "objectID": "Chapter20.html#question-6",
    "href": "Chapter20.html#question-6",
    "title": "Chapter 20",
    "section": "Question 6",
    "text": "Question 6\nAdd the latitude and the longitude of the origin and destination airport to flights. Is it easier to rename the columns before or after the join?\nThe code shown below adds the latitude and the longitude of the origin and destination airport to flights. As we can see, it easier to rename the columns after the join, so that we the same airport might (though not in this case) may be used as origin and/or dest. Further, the use of rename() after the join allows us to write the code in flow.\n\nflights |&gt;\n  left_join(airports, by = join_by(dest == faa)) |&gt;\n  rename(\n    \"dest_lat\" = lat,\n    \"dest_lon\" = lon\n  ) |&gt;\n  left_join(airports, by = join_by(origin == faa)) |&gt;\n  rename(\n    \"origin_lat\" = lat,\n    \"origin_lon\" = lon\n  ) |&gt;\n  relocate(origin, origin_lat, origin_lon,\n           dest, dest_lat, dest_lon,\n           .before = 1)\n\n# A tibble: 336,776 × 33\n   origin origin_lat origin_lon dest  dest_lat dest_lon  year month   day\n   &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 EWR          40.7      -74.2 IAH       30.0    -95.3  2013     1     1\n 2 LGA          40.8      -73.9 IAH       30.0    -95.3  2013     1     1\n 3 JFK          40.6      -73.8 MIA       25.8    -80.3  2013     1     1\n 4 JFK          40.6      -73.8 BQN       NA       NA    2013     1     1\n 5 LGA          40.8      -73.9 ATL       33.6    -84.4  2013     1     1\n 6 EWR          40.7      -74.2 ORD       42.0    -87.9  2013     1     1\n 7 EWR          40.7      -74.2 FLL       26.1    -80.2  2013     1     1\n 8 LGA          40.8      -73.9 IAD       38.9    -77.5  2013     1     1\n 9 JFK          40.6      -73.8 MCO       28.4    -81.3  2013     1     1\n10 LGA          40.8      -73.9 ORD       42.0    -87.9  2013     1     1\n# ℹ 336,766 more rows\n# ℹ 24 more variables: dep_time &lt;int&gt;, sched_dep_time &lt;int&gt;, dep_delay &lt;dbl&gt;,\n#   arr_time &lt;int&gt;, sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;,\n#   minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, name.x &lt;chr&gt;, alt.x &lt;dbl&gt;, tz.x &lt;dbl&gt;,\n#   dst.x &lt;chr&gt;, tzone.x &lt;chr&gt;, name.y &lt;chr&gt;, alt.y &lt;dbl&gt;, tz.y &lt;dbl&gt;,\n#   dst.y &lt;chr&gt;, tzone.y &lt;chr&gt;"
  },
  {
    "objectID": "Chapter20.html#question-7",
    "href": "Chapter20.html#question-7",
    "title": "Chapter 20",
    "section": "Question 7",
    "text": "Question 7\nCompute the average delay by destination, then join on the airports data frame so you can show the spatial distribution of delays. Here’s an easy way to draw a map of the United States:\nairports |&gt;   \n  semi_join(flights, join_by(faa == dest)) |&gt;   \n  ggplot(aes(x = lon, y = lat)) +     \n  borders(\"state\") +     \n  geom_point() +     \n  coord_quickmap()\nYou might want to use the size or color of the points to display the average delay for each airport.\nThe following code and the resulting Figure 7 displays the result. I would like to avoid using size as an aesthetic, as it is not easy to compare on a continuous scale, and leads to visually tough comparison. Instead, I prefer to use an interactive visualization shown further below.\n\n# Create a dataframe of 1 row for origin airports\nor_apts = airports |&gt;\n  filter(faa %in% c(\"EWR\", \"JFK\", \"LGA\")) |&gt;\n  select(-c(alt, tz, dst, tzone)) |&gt;\n  rename(dest = faa) |&gt;\n  mutate(type = \"New York City\",\n         avg_delay = 0)\n    \n# Start with the flights data-set\nflights |&gt;\n\n  # Compute average delay for each location\n  group_by(dest) |&gt;\n  summarise(avg_delay = mean(arr_delay, na.rm = TRUE)) |&gt;\n  \n  # Add the latitude and longitude data\n  left_join(airports, join_by(dest == faa)) |&gt;\n  select(-c(alt, tz, dst, tzone)) |&gt;\n  mutate(type = \"Destinations\") |&gt;\n  \n  # Add a row for origin airports data\n  bind_rows(or_apts) |&gt;\n \n  # Plot the map and points\n  ggplot(aes(x = lon, y = lat, \n             col = avg_delay, \n             shape = type,\n             label = name)) +     \n  borders(\"state\", colour = \"white\", fill = \"lightgrey\") +     \n  geom_point(size = 2) +     \n  coord_quickmap(xlim = c(-130, -65),\n                 ylim = c(23, 50)) +\n  scale_color_viridis_c(option = \"C\") +\n  labs(col = \"Average Delay at Arrival (mins.)\", shape = \"\") +\n  \n  # Themes and Customization\n  theme_void() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 7: Airport destinations from New York City, with average arrival delays\n\n\n\n\n\nAn interactive map to see average arrival delays: –"
  },
  {
    "objectID": "Chapter20.html#question-8",
    "href": "Chapter20.html#question-8",
    "title": "Chapter 20",
    "section": "Question 8",
    "text": "Question 8\nWhat happened on June 13 2013? Draw a map of the delays, and then use Google to cross-reference with the weather.\nIn the map shown in figure Figure 8 , we see abnormally large delays for all destinations than normal.\n\n\nCode\nflights |&gt;\n  mutate(Date = if_else((month == 6 & day == 13),\n                       \"June 13, 2013\",\n                       \"Rest of the year\")) |&gt;\n  group_by(Date) |&gt;\n  summarise(average_departure_delay = mean(dep_delay, na.rm = TRUE)) |&gt;\n  gt() |&gt;\n  cols_label_with(fn = ~ janitor::make_clean_names(., case = \"title\")) |&gt;\n  fmt_number(columns = average_departure_delay) |&gt;\n  gt_theme_538()\n\n\n\n\n\n\n  \n    \n    \n      Date\n      Average Departure Delay\n    \n  \n  \n    June 13, 2013\n45.79\n    Rest of the year\n12.55\n  \n  \n  \n\n\n\n\nFurther, when we search the weather on internet using google, we find that a major storm system had hit New York City on June 13, 2013. Thus, the departure delays are expected. The links to the weather reports are here, and in an article on severe flight cancellations and delays.\n\n# Start with the flights data-set for June 13, 2013\nflights |&gt;\n  filter(month == 6 & day == 13) |&gt;\n  # Compute average delay for each location\n  group_by(dest) |&gt;\n  summarise(avg_delay = mean(arr_delay, na.rm = TRUE)) |&gt;\n  \n  # Add the latitude and longitude data\n  left_join(airports, join_by(dest == faa)) |&gt;\n  select(-c(alt, tz, dst, tzone)) |&gt;\n \n  # Plot the map and points\n  ggplot(aes(x = lon, y = lat, \n             col = avg_delay, \n             label = name)) +     \n  borders(\"state\", colour = \"white\", fill = \"lightgrey\") +     \n  geom_point(size = 3) +     \n  coord_quickmap(xlim = c(-130, -65),\n                 ylim = c(23, 50)) +\n  scale_color_viridis_c(option = \"C\") +\n  labs(col = \"Average Delay at Arrival (mins.)\", shape = \"\",\n       title = \"Flight delays on June 13, 2013 re much higher than normal\") +\n  \n  # Themes and Customization\n  theme_void() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 8: Flight delays on June 13, 2013 for flights originating in New York City"
  },
  {
    "objectID": "Chapter20.html#question-1-2",
    "href": "Chapter20.html#question-1-2",
    "title": "Chapter 20",
    "section": "Question 1",
    "text": "Question 1\nCan you explain what’s happening with the keys in this equi join? Why are they different?\nx |&gt; \n  full_join(y, by = \"key\")\n#&gt; # A tibble: 4 × 3\n#&gt;     key val_x val_y\n#&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n#&gt; 1     1 x1    y1   \n#&gt; 2     2 x2    y2   \n#&gt; 3     3 x3    &lt;NA&gt; \n#&gt; 4     4 &lt;NA&gt;  y3\n\nx |&gt; \n  full_join(y, by = \"key\", keep = TRUE)\n#&gt; # A tibble: 4 × 4\n#&gt;   key.x val_x key.y val_y\n#&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;\n#&gt; 1     1 x1        1 y1   \n#&gt; 2     2 x2        2 y2   \n#&gt; 3     3 x3       NA &lt;NA&gt; \n#&gt; 4    NA &lt;NA&gt;      4 y3\nYes, the key column names in the output are different because when we use the option keep = TRUE in the full_join() function, the execution by dplyr retains both the keys and names them as key.x and key.y for ease of recognition."
  },
  {
    "objectID": "Chapter20.html#question-2-2",
    "href": "Chapter20.html#question-2-2",
    "title": "Chapter 20",
    "section": "Question 2",
    "text": "Question 2\nWhen finding if any party period overlapped with another party period we used q &lt; q in the join_by()? Why? What happens if you remove this inequality?\nThe default syntax for function inner_join is inner_join(x, y, by = NULL, ...) . The default for by = argument is NULL, where the default *_join()⁠ will perform a natural join, using all variables in common across x and y.\nThus, when we skip q &lt; q , the inner_join finds that the variables q , start and end are common. The start and end variables are taken care of by the helper function overlaps() . But q remains. Since q is common in parties and parties all observations get matched. To prevent observations from matching on q we can keep a condition q &lt; q , and thus each observation and match is repeated only once, leading to correct results.\n\nparties &lt;- tibble(\n  q = 1:4,\n  party = ymd(c(\"2022-01-10\", \"2022-04-04\", \"2022-07-11\", \"2022-10-03\")),\n  start = ymd(c(\"2022-01-01\", \"2022-04-04\", \"2022-07-11\", \"2022-10-03\")),\n  end = ymd(c(\"2022-04-03\", \"2022-07-11\", \"2022-10-02\", \"2022-12-31\"))\n)\n\n# Using the correct code in textbook\nparties |&gt; \n  inner_join(parties, join_by(overlaps(start, end, start, end), q &lt; q)) |&gt;\n  select(start.x, end.x, start.y, end.y)\n\n# A tibble: 1 × 4\n  start.x    end.x      start.y    end.y     \n  &lt;date&gt;     &lt;date&gt;     &lt;date&gt;     &lt;date&gt;    \n1 2022-04-04 2022-07-11 2022-07-11 2022-10-02\n\n# Removing the \"q &lt; q\" in the join_by()\nparties |&gt; \n  inner_join(parties, join_by(overlaps(start, end, start, end))) |&gt;\n  select(start.x, end.x, start.y, end.y)\n\n# A tibble: 6 × 4\n  start.x    end.x      start.y    end.y     \n  &lt;date&gt;     &lt;date&gt;     &lt;date&gt;     &lt;date&gt;    \n1 2022-01-01 2022-04-03 2022-01-01 2022-04-03\n2 2022-04-04 2022-07-11 2022-04-04 2022-07-11\n3 2022-04-04 2022-07-11 2022-07-11 2022-10-02\n4 2022-07-11 2022-10-02 2022-04-04 2022-07-11\n5 2022-07-11 2022-10-02 2022-07-11 2022-10-02\n6 2022-10-03 2022-12-31 2022-10-03 2022-12-31"
  },
  {
    "objectID": "Chapter22.html",
    "href": "Chapter22.html",
    "title": "Chapter 22",
    "section": "",
    "text": "library(DBI)\nlibrary(dbplyr)\nlibrary(tidyverse)\n# Reading in the data as a database\n\nconnection &lt;- DBI::dbConnect(duckdb::duckdb())\ndbWriteTable(connection, name = \"db_diamonds\", value = ggplot2::diamonds)\n\ndb_df &lt;- tbl(connection, \"db_diamonds\")"
  },
  {
    "objectID": "Chapter22.html#question-1",
    "href": "Chapter22.html#question-1",
    "title": "Chapter 22",
    "section": "Question 1",
    "text": "Question 1\nWhat is distinct() translated to? How about head()?\nAs we can see below, the dplyr function distinct() translates into SELECT DISTINCT &lt;variable name&gt; .\nSimilarly, the function head() translates to SELECT * FROM &lt;table name&gt; LIMIT 6.\n\ndb_df |&gt;\n  distinct(cut) |&gt;\n  show_query()\n\n&lt;SQL&gt;\nSELECT DISTINCT cut\nFROM db_diamonds\n\ndb_df |&gt;\n  head() |&gt;\n  show_query()\n\n&lt;SQL&gt;\nSELECT *\nFROM db_diamonds\nLIMIT 6"
  },
  {
    "objectID": "Chapter22.html#question-2",
    "href": "Chapter22.html#question-2",
    "title": "Chapter 22",
    "section": "Question 2",
    "text": "Question 2\nExplain what each of the following SQL queries do and try recreate them using dbplyr.\nSELECT *  \nFROM flights \nWHERE dep_delay &lt; arr_delay  \nThe code above is equivalent to dplyr’s flights |&gt; filter(dep_delay &lt; arr_delay) .\nSELECT *, distance / (airtime / 60) AS speed \nFROM flights\nThe code above is equivalent to dplyr’s flights |&gt; mutate(speed = distance / (airtime / 60)) ."
  },
  {
    "objectID": "Chapter23.html",
    "href": "Chapter23.html",
    "title": "Chapter 23",
    "section": "",
    "text": "# Loading Required Packages\nlibrary(tidyverse)\nlibrary(arrow)\nlibrary(dbplyr, warn.conflicts = FALSE)\nlibrary(duckdb)"
  },
  {
    "objectID": "Chapter23.html#section-23.3",
    "href": "Chapter23.html#section-23.3",
    "title": "Chapter 23",
    "section": "Section 23.3",
    "text": "Section 23.3\n\nWorkign with the Seattle library 9 GB data!\n\ndir.create(\"data\", showWarnings = FALSE)\n\ncurl::multi_download(\n  url = \"https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv\",\n  destfiles = \"data/seattle-library-checkouts.csv\",\n  resume = TRUE\n)\n\n# Opening the dataset file\n\nseattle_csv &lt;- open_dataset(\n  sources = \"data/seattle-library-checkouts.csv\", \n  format = \"csv\"\n)\n# Viewing a portion of the data-set file\nseattle_csv |&gt;\n  glimpse()"
  },
  {
    "objectID": "Chapter23.html#the-parquet-format",
    "href": "Chapter23.html#the-parquet-format",
    "title": "Chapter 23",
    "section": "23.4 The Parquet Format",
    "text": "23.4 The Parquet Format\n\n# Convert the 9GB csv file into smaller parquet files, \n# each for 1 checkout year.\n\npq_path &lt;- \"data/seattle-library-checkouts\"\n\nseattle_csv |&gt;\n  group_by(CheckoutYear) |&gt;\n  write_dataset(path = pq_path, format = \"parquet\")\n\n# Viweing the results\ntibble(\n  files = list.files(pq_path, recursive = TRUE),\n  size_MB = file.size(file.path(pq_path, files)) / 1024^2\n)"
  },
  {
    "objectID": "Chapter23.html#using-dplyr-with-arrow",
    "href": "Chapter23.html#using-dplyr-with-arrow",
    "title": "Chapter 23",
    "section": "23.5 Using dplyr with arrow",
    "text": "23.5 Using dplyr with arrow\n\n# Read in the parquet files\nseattle_pq &lt;- open_dataset(pq_path)\n\n# Count the total number of books checked out in \n# each month for the last five years\nquery &lt;- seattle_pq |&gt; \n  filter(CheckoutYear &gt;= 2018, MaterialType == \"BOOK\") |&gt;\n  group_by(CheckoutYear, CheckoutMonth) |&gt;\n  summarize(TotalCheckouts = sum(Checkouts)) |&gt;\n  arrange(CheckoutYear, CheckoutMonth)\n\nquery\n\n# Get the results by calling collect()\nquery |&gt; collect()\n\n# Comparing performance\nseattle_csv |&gt; \n  filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |&gt;\n  group_by(CheckoutMonth) |&gt;\n  summarize(TotalCheckouts = sum(Checkouts)) |&gt;\n  arrange(desc(CheckoutMonth)) |&gt;\n  collect() |&gt; \n  system.time()\n\nseattle_pq |&gt; \n  filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |&gt;\n  group_by(CheckoutMonth) |&gt;\n  summarize(TotalCheckouts = sum(Checkouts)) |&gt;\n  arrange(desc(CheckoutMonth)) |&gt;\n  collect() |&gt; \n  system.time()\n\nImportant: Whenever we want to output the results of our dplyr code, we will use collect() ."
  },
  {
    "objectID": "Chapter23.html#opening-a-dataset",
    "href": "Chapter23.html#opening-a-dataset",
    "title": "Chapter 23",
    "section": "23.3 Opening a dataset",
    "text": "23.3 Opening a dataset\n\nWorkign with the Seattle library 9 GB data!\n\ndir.create(\"data\", showWarnings = FALSE)\n\ncurl::multi_download(\n  url = \"https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv\",\n  destfiles = \"data/seattle-library-checkouts.csv\",\n  resume = TRUE\n)\n\n# Opening the dataset file\n\nseattle_csv &lt;- open_dataset(\n  sources = \"data/seattle-library-checkouts.csv\", \n  format = \"csv\"\n)\n# Viewing a portion of the data-set file\nseattle_csv |&gt;\n  glimpse()"
  },
  {
    "objectID": "Chapter24.html",
    "href": "Chapter24.html",
    "title": "Chapter 24",
    "section": "",
    "text": "library(tidyverse)\nlibrary(jsonlite)\nlibrary(repurrrsive)\nlibrary(gt)\nlibrary(gtExtras)"
  },
  {
    "objectID": "Chapter24.html#question-1",
    "href": "Chapter24.html#question-1",
    "title": "Chapter 24",
    "section": "Question 1",
    "text": "Question 1\nWhat happens when you use unnest_wider() with unnamed list-columns like df2? What argument is now necessary? What happens to missing values?\nWhen we use unnest_wider() with unnamed list columns, it results in an error because unnest_wider() cannot un-nest columns with missing names of the children of the list-column. The error message tells us that the argument names_sep = \"_\" is now necessary, where \"_\" is any separator that we may wish to use. The result names the new columns as y_1 , y_2 etc. The missing values are represented with an NA .\n\n# Create df2 with an unnamed list-column\ndf2 &lt;- tribble(\n  ~x, ~y,\n  1, list(11, 12, 13),\n  2, list(21),\n  3, list(31, 32),\n)\n\n# Running unnest_wider()\n# df2 |&gt;\n#   unnest_wider(y)\n# Error in `unnest_wider()`:\n# ℹ In column: `y`.\n# ℹ In row: 1.\n# Caused by error:\n# ! Can't unnest elements with missing names.\n# ℹ Supply `names_sep` to generate automatic names.\n# Run `rlang::last_trace()` to see where the error occurred.\n\ndf2 |&gt;\n  unnest_wider(y, names_sep = \"_\")\n\n# A tibble: 3 × 4\n      x   y_1   y_2   y_3\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1    11    12    13\n2     2    21    NA    NA\n3     3    31    32    NA"
  },
  {
    "objectID": "Chapter24.html#question-2",
    "href": "Chapter24.html#question-2",
    "title": "Chapter 24",
    "section": "Question 2",
    "text": "Question 2\nWhat happens when you use unnest_longer() with named list-columns like df1? What additional information do you get in the output? How can you suppress that extra detail?\nWhen we use unnest_longer() with named list-columns like df1 , we get a new row for each value of the list, and in addition, we get a new column named \"&lt;name of list column&gt;_id\" which tells us the name of the list child from which the value has been picked up.\nWe can suppress this extra detail by adding the argument indices_include = FALSE .\n\n# Create a named list-column like df1\ndf1 &lt;- tribble(\n  ~x, ~y,\n  1, list(a = 11, b = 12),\n  2, list(a = 21, b = 22),\n  3, list(a = 31, b = 32),\n)\n\n# Unnest Longer function on a named list-column\ndf1 |&gt;\n  unnest_longer(y)\n\n# A tibble: 6 × 3\n      x     y y_id \n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1     1    11 a    \n2     1    12 b    \n3     2    21 a    \n4     2    22 b    \n5     3    31 a    \n6     3    32 b    \n\n# Additional argument to suppress the extra detail of _id column\ndf1 |&gt;\n  unnest_longer(y, indices_include = FALSE)\n\n# A tibble: 6 × 2\n      x     y\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1    11\n2     1    12\n3     2    21\n4     2    22\n5     3    31\n6     3    32"
  },
  {
    "objectID": "Chapter24.html#question-3",
    "href": "Chapter24.html#question-3",
    "title": "Chapter 24",
    "section": "Question 3",
    "text": "Question 3\nFrom time-to-time you encounter data frames with multiple list-columns with aligned values. For example, in the following data frame, the values of y and z are aligned (i.e. y and z will always have the same length within a row, and the first value of y corresponds to the first value of z). What happens if you apply two unnest_longer() calls to this data frame? How can you preserve the relationship between x and y? (Hint: carefully read the docs).\ndf4 &lt;- tribble(\n  ~x, ~y, ~z,\n  \"a\", list(\"y-a-1\", \"y-a-2\"), list(\"z-a-1\", \"z-a-2\"),\n  \"b\", list(\"y-b-1\", \"y-b-2\", \"y-b-3\"), list(\"z-b-1\", \"z-b-2\", \"z-b-3\")\n)\nIf we apply two consequtive unnest_longer() calls to this data-frame it results in a permutation-combination like situation, where each newly created row of y is treated a un-linked to each element of lists in z and thus the resulting data-frame produces a combination of all possible values of y and z .\n\ndf4 &lt;- tribble(\n  ~x, ~y, ~z,\n  \"a\", list(\"y-a-1\", \"y-a-2\"), list(\"z-a-1\", \"z-a-2\"),\n  \"b\", list(\"y-b-1\", \"y-b-2\", \"y-b-3\"), list(\"z-b-1\", \"z-b-2\", \"z-b-3\")\n)\n\n# Result if we apply two unnest_longer() calls to this data frame\ndf4 |&gt;\n  unnest_longer(y) |&gt;\n  unnest_longer(z)\n\n# A tibble: 13 × 3\n   x     y     z    \n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 a     y-a-1 z-a-1\n 2 a     y-a-1 z-a-2\n 3 a     y-a-2 z-a-1\n 4 a     y-a-2 z-a-2\n 5 b     y-b-1 z-b-1\n 6 b     y-b-1 z-b-2\n 7 b     y-b-1 z-b-3\n 8 b     y-b-2 z-b-1\n 9 b     y-b-2 z-b-2\n10 b     y-b-2 z-b-3\n11 b     y-b-3 z-b-1\n12 b     y-b-3 z-b-2\n13 b     y-b-3 z-b-3\n\n\nWe can preserve the relationship between x and y (and z ) by including multiple columns in a single call of unnest_longer() . The help documentation with unnest_longer() provides that\n\ncol : List-column(s) to unnest. When selecting multiple columns, values from the same row will be recycled to their common size.\n\n\ndf4 |&gt;\n  unnest_longer(c(y, z))\n\n# A tibble: 5 × 3\n  x     y     z    \n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 a     y-a-1 z-a-1\n2 a     y-a-2 z-a-2\n3 b     y-b-1 z-b-1\n4 b     y-b-2 z-b-2\n5 b     y-b-3 z-b-3"
  },
  {
    "objectID": "Chapter24.html#question-1-1",
    "href": "Chapter24.html#question-1-1",
    "title": "Chapter 24",
    "section": "Question 1",
    "text": "Question 1\nRoughly estimate when gh_repos was created. Why can you only roughly estimate the date?\nUsing the code shown below, and viewing the results of the maximum (i.e. the latest time stamps) we find that the latest data observation in the data-set is of\n2016-10-25 03:09:53\nThus, the data-set gh_repos was created on or after 25 October, 2016 3:09 AM UTC. We can only find the estimate, and not the exact time of its creation since the data-set does not explicitly contain the information of date and time of its creation. We can only infer it from the contents of the data.\n\n# Convert the gh_repos into a tibble for easy viewing\nghrepos = tibble(json = gh_repos)\nghrepos |&gt;\n  \n  # Rectangling the data\n  unnest_longer(json) |&gt;\n  unnest_wider(json) |&gt;\n  \n  # Selecting the time variables to find the latest date in the data\n  select(created_at, updated_at, pushed_at) |&gt;\n  \n  # Covert to date-time objects\n  mutate(across(.cols = 1:3, .fns = ymd_hms)) |&gt;\n  \n  # Find the maximum (i.e. the latest) time in the three columns\n  summarize(\n    max_created_at = max(created_at),\n    max_updated_at = max(updated_at),\n    max_pushed_at = max(pushed_at)\n  ) |&gt;\n  \n  # Nice display\n  gt() |&gt;\n  cols_label_with(fn = ~ janitor::make_clean_names(., case = \"title\")) |&gt;\n  gt_theme_538()\n\n\n\n\n\n  \n    \n    \n      Max Created at\n      Max Updated at\n      Max Pushed at\n    \n  \n  \n    2016-10-24 08:52:01\n2016-10-25 03:09:53\n2016-10-24 19:29:48"
  },
  {
    "objectID": "Chapter24.html#question-2-1",
    "href": "Chapter24.html#question-2-1",
    "title": "Chapter 24",
    "section": "Question 2",
    "text": "Question 2\nThe owner column of gh_repo contains a lot of duplicated information because each owner can have many repos. Can you construct an owners data frame that contains one row for each owner? (Hint: does distinct() work with list-cols?)\nThe code below shows the two ways of creating an an owners data frame that contains one row for each owner. The Method 1 uses the unnest_wider() first, and then uses distinct() on the resulting data frame. The Method 2 first uses distinct() and then unnest_wider() . The results are the same.\nYes, we observe that surprisingly, the function distinct() does work with list-cols .\n\n########### Method 1: The long way, after unnesting\nghrepos |&gt;\n  \n  # Rectangling the data\n  unnest_longer(json) |&gt;\n  unnest_wider(json) |&gt;\n  \n  # Selecting the owner variable\n  select(owner) |&gt;\n  \n  # Unnesting the named list i.e., owner column, \n  # using names_sep to ensure id is not duplicated original id column\n  # when joining the data frames later\n  unnest_wider(owner, names_sep = \"_\") |&gt;\n  \n  # Checking the nature of data, what are the contents of each column like\n  # visdat::vis_dat()\n  \n  # Keeping only distinct columns, dropping duplicated rows\n  distinct()\n\n# A tibble: 6 × 17\n  owner_login owner_id owner_avatar_url              owner_gravatar_id owner_url\n  &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;                         &lt;chr&gt;             &lt;chr&gt;    \n1 gaborcsardi   660288 https://avatars.githubuserco… \"\"                https://…\n2 jennybc       599454 https://avatars.githubuserco… \"\"                https://…\n3 jtleek       1571674 https://avatars.githubuserco… \"\"                https://…\n4 juliasilge  12505835 https://avatars.githubuserco… \"\"                https://…\n5 leeper       3505428 https://avatars.githubuserco… \"\"                https://…\n6 masalmon     8360597 https://avatars.githubuserco… \"\"                https://…\n# ℹ 12 more variables: owner_html_url &lt;chr&gt;, owner_followers_url &lt;chr&gt;,\n#   owner_following_url &lt;chr&gt;, owner_gists_url &lt;chr&gt;, owner_starred_url &lt;chr&gt;,\n#   owner_subscriptions_url &lt;chr&gt;, owner_organizations_url &lt;chr&gt;,\n#   owner_repos_url &lt;chr&gt;, owner_events_url &lt;chr&gt;,\n#   owner_received_events_url &lt;chr&gt;, owner_type &lt;chr&gt;, owner_site_admin &lt;lgl&gt;\n\n######### Method 2: The short way - using distinct() on the owner list-column\nowners &lt;- ghrepos |&gt;\n   # Rectangling the data\n  unnest_longer(json) |&gt;\n  unnest_wider(json) |&gt;\n  distinct(owner) |&gt;\n  unnest_wider(owner, names_sep = \"_\")\nowners\n\n# A tibble: 6 × 17\n  owner_login owner_id owner_avatar_url              owner_gravatar_id owner_url\n  &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;                         &lt;chr&gt;             &lt;chr&gt;    \n1 gaborcsardi   660288 https://avatars.githubuserco… \"\"                https://…\n2 jennybc       599454 https://avatars.githubuserco… \"\"                https://…\n3 jtleek       1571674 https://avatars.githubuserco… \"\"                https://…\n4 juliasilge  12505835 https://avatars.githubuserco… \"\"                https://…\n5 leeper       3505428 https://avatars.githubuserco… \"\"                https://…\n6 masalmon     8360597 https://avatars.githubuserco… \"\"                https://…\n# ℹ 12 more variables: owner_html_url &lt;chr&gt;, owner_followers_url &lt;chr&gt;,\n#   owner_following_url &lt;chr&gt;, owner_gists_url &lt;chr&gt;, owner_starred_url &lt;chr&gt;,\n#   owner_subscriptions_url &lt;chr&gt;, owner_organizations_url &lt;chr&gt;,\n#   owner_repos_url &lt;chr&gt;, owner_events_url &lt;chr&gt;,\n#   owner_received_events_url &lt;chr&gt;, owner_type &lt;chr&gt;, owner_site_admin &lt;lgl&gt;"
  },
  {
    "objectID": "Chapter24.html#question-3-1",
    "href": "Chapter24.html#question-3-1",
    "title": "Chapter 24",
    "section": "Question 3",
    "text": "Question 3\nFollow the steps used for titles to create similar tables for the aliases, allegiances, books, and TV series for the Game of Thrones characters.\n\n# A tibble for aliases\naliases = tibble(json = got_chars) |&gt;\n  unnest_wider(json) |&gt;\n  select(id, aliases) |&gt;\n  unnest_longer(aliases) |&gt;\n  filter(aliases != \"\")\naliases\n\n# A tibble: 107 × 2\n      id aliases           \n   &lt;int&gt; &lt;chr&gt;             \n 1  1022 Prince of Fools   \n 2  1022 Theon Turncloak   \n 3  1022 Reek              \n 4  1022 Theon Kinslayer   \n 5  1052 The Imp           \n 6  1052 Halfman           \n 7  1052 The boyman        \n 8  1052 Giant of Lannister\n 9  1052 Lord Tywin's Doom \n10  1052 Lord Tywin's Bane \n# ℹ 97 more rows\n\n# A tibble for allegiances\nallegiances = tibble(json = got_chars) |&gt;\n  unnest_wider(json) |&gt;\n  select(id, allegiances) |&gt;\n  unnest_longer(allegiances) |&gt;\n  filter(allegiances != \"\")\nallegiances\n\n# A tibble: 33 × 2\n      id allegiances                      \n   &lt;int&gt; &lt;chr&gt;                            \n 1  1022 House Greyjoy of Pyke            \n 2  1052 House Lannister of Casterly Rock \n 3  1074 House Greyjoy of Pyke            \n 4  1166 House Nymeros Martell of Sunspear\n 5   130 House Nymeros Martell of Sunspear\n 6  1303 House Targaryen of King's Landing\n 7  1319 House Baratheon of Dragonstone   \n 8  1319 House Seaworth of Cape Wrath     \n 9   148 House Stark of Winterfell        \n10   149 House Oakheart of Old Oak        \n# ℹ 23 more rows\n\n# A tibble for aliases\nbooks = tibble(json = got_chars) |&gt;\n  unnest_wider(json) |&gt;\n  select(id, books) |&gt;\n  unnest_longer(books) |&gt;\n  filter(books != \"\")\nbooks\n\n# A tibble: 77 × 2\n      id books                    \n   &lt;int&gt; &lt;chr&gt;                    \n 1  1022 A Game of Thrones        \n 2  1022 A Storm of Swords        \n 3  1022 A Feast for Crows        \n 4  1052 A Feast for Crows        \n 5  1052 The World of Ice and Fire\n 6  1074 A Game of Thrones        \n 7  1074 A Clash of Kings         \n 8  1074 A Storm of Swords        \n 9  1109 A Clash of Kings         \n10  1166 A Game of Thrones        \n# ℹ 67 more rows\n\n# A tibble for aliases\ntvSeries = tibble(json = got_chars) |&gt;\n  unnest_wider(json) |&gt;\n  select(id, tvSeries) |&gt;\n  unnest_longer(tvSeries) |&gt;\n  filter(tvSeries != \"\")\ntvSeries\n\n# A tibble: 93 × 2\n      id tvSeries\n   &lt;int&gt; &lt;chr&gt;   \n 1  1022 Season 1\n 2  1022 Season 2\n 3  1022 Season 3\n 4  1022 Season 4\n 5  1022 Season 5\n 6  1022 Season 6\n 7  1052 Season 1\n 8  1052 Season 2\n 9  1052 Season 3\n10  1052 Season 4\n# ℹ 83 more rows\n\n\nWe could also try to create a function for this: –\n\n# Create a function to do the same job for each variable in the nested data\ngot_unnest &lt;- function(variable, nested_tibble){\n  \n  tibble(json = nested_tibble) |&gt;\n  unnest_wider(json) |&gt;\n  select(id, variable) |&gt;\n  unnest_longer(variable) |&gt;\n  filter(variable != \"\")\n}"
  },
  {
    "objectID": "Chapter24.html#question-4",
    "href": "Chapter24.html#question-4",
    "title": "Chapter 24",
    "section": "Question 4",
    "text": "Question 4\nExplain the following code line-by-line. Why is it interesting? Why does it work for got_chars but might not work in general?\nThe following annotations to the code explain it line by line. This is interesting because it provides us a way to directly create a tidy data, in long form, for plotting by ggplot2 etc. from the list.\n\n# Convert lsit into tibble for easy viewing and rectangling operations\ntibble(json = got_chars) |&gt; \n  \n  # Unnest the named column json into 18 different columns\n  unnest_wider(json) |&gt; \n  \n  # Select to view only the id column, and nested columns which are list-columns\n  select(id, where(is.list)) |&gt; \n  \n  # Convert into long format by making different row for each column\n  pivot_longer(\n    where(is.list), \n    names_to = \"name\", \n    values_to = \"value\"\n  ) |&gt;  \n  \n  # Now unnest the value column to display one row for each list item\n  unnest_longer(value)\n\n# A tibble: 479 × 3\n      id name        value                                               \n   &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;                                               \n 1  1022 titles      Prince of Winterfell                                \n 2  1022 titles      Lord of the Iron Islands (by law of the green lands)\n 3  1022 aliases     Prince of Fools                                     \n 4  1022 aliases     Theon Turncloak                                     \n 5  1022 aliases     Reek                                                \n 6  1022 aliases     Theon Kinslayer                                     \n 7  1022 allegiances House Greyjoy of Pyke                               \n 8  1022 books       A Game of Thrones                                   \n 9  1022 books       A Storm of Swords                                   \n10  1022 books       A Feast for Crows                                   \n# ℹ 469 more rows\n\n\nThis worked for got_chars because luckily, each list that we pivoted in pivot_longer() had exactly the same level of nesting. That is, none of the contents of the created column value has more than one level of nesting. This may not work in general because different data sets and lists may have different levels of nesting for each column. For example, one column may have 3 levels of nesting, while other may have 10 levels of nesting."
  },
  {
    "objectID": "Chapter24.html#question-5",
    "href": "Chapter24.html#question-5",
    "title": "Chapter 24",
    "section": "Question 5",
    "text": "Question 5\nIn gmaps_cities, what does address_components contain? Why does the length vary between rows? Un-nest it appropriately to figure it out. (Hint: types always appears to contain two elements. Does unnest_wider() make it easier to work with than unnest_longer()?) .\nThe address_components contain the complete words and parts (city, county, state and country) that come for the complete address of the city. The contents are the following: –\n\nlong_name : The actual complete name of the component (city, or county or state or country)\nshort_name : The abbreviation for the name (for states and country)\ntypes: The address_components further sub-component types contains two elements, one is the type of address, (i..e one of the Locality, Administrative Area Level 2, Administrative Area Level 1, Country) and second is same for all, i.e. type of address political .\n\nThe length varies between rows because some address have only two levels, for example:--\n\nWashington, United States\n\nBut, others have three levels in the address such as: –\n\nNew York City, New York State, United States\n\nAnd, lastly, others have four levels in the address such as: –\n\nHouston, Harris County, Texas, United States\n\nYes, unnest_wider() make it easier to work with than unnest_longer() because we can create columns of the components for easy display. Tidy data in long form is good for ggplot2 visualization, but not easy to read. The output is shown below in Figure 1.\n\n\nCode\ngmaps_cities |&gt;\n  unnest_wider(json) |&gt;\n  \n  # Remove status as it adds no info\n  select(-status) |&gt;\n  \n  # Unnamed lists, so unnest longer - make rows\n  unnest_longer(results) |&gt;\n  \n  # Named lists to unnest wider into columns\n  unnest_wider(results) |&gt;\n  \n  # Select an id variable and address_components\n  select(formatted_address, address_components) |&gt;\n  \n  # Since we know the address components have City, County, State and Country\n  # Names, lets try to create a column for each by unnest_wider\n  unnest_wider(address_components, names_sep = \"_\") |&gt;\n  \n  # To create tidy data of address levels\n  pivot_longer(cols = -formatted_address,\n               names_to = \"level\",\n               values_to = \"address_components\") |&gt;\n  mutate(level = parse_number(level)) |&gt;\n  \n  # Further, making new columns from remaining list-columns\n  unnest_wider(address_components) |&gt;\n  unnest_wider(types, names_sep = \"_\") |&gt;\n  \n  # Remove types_2 (\"political) as it does not add any information\n  select(-types_2) |&gt;\n  \n  # Tidying up final display\n  rename(level_type = types_1) |&gt;\n  relocate(level_type, .before = long_name) |&gt;\n  drop_na() |&gt;\n  mutate(level_type = snakecase::to_any_case(level_type, \"title\")) |&gt;\n  \n  # gt() to display the output nicely\n  gt(rowname_col = NULL,\n     groupname_col = \"formatted_address\") |&gt;\n  tab_options(row_group.as_column = TRUE) |&gt;\n  cols_label_with(fn = ~ janitor::make_clean_names(., case = \"title\")) |&gt;\n  gt_theme_538()\n\n\n\n\n\n\n\n  \n    \n    \n      \n      Level\n      Level Type\n      Long Name\n      Short Name\n    \n  \n  \n    Houston, TX, USA\n1\nLocality\nHouston\nHouston\n    2\nAdministrative Area Level 2\nHarris County\nHarris County\n    3\nAdministrative Area Level 1\nTexas\nTX\n    4\nCountry\nUnited States\nUS\n    Washington, USA\n1\nAdministrative Area Level 1\nWashington\nWA\n    2\nCountry\nUnited States\nUS\n    Washington, DC, USA\n1\nLocality\nWashington\nWashington\n    2\nAdministrative Area Level 2\nDistrict of Columbia\nDistrict of Columbia\n    3\nAdministrative Area Level 1\nDistrict of Columbia\nDC\n    4\nCountry\nUnited States\nUS\n    New York, NY, USA\n1\nLocality\nNew York\nNew York\n    2\nAdministrative Area Level 1\nNew York\nNY\n    3\nCountry\nUnited States\nUS\n    Chicago, IL, USA\n1\nLocality\nChicago\nChicago\n    2\nAdministrative Area Level 2\nCook County\nCook County\n    3\nAdministrative Area Level 1\nIllinois\nIL\n    4\nCountry\nUnited States\nUS\n    Arlington, TX, USA\n1\nLocality\nArlington\nArlington\n    2\nAdministrative Area Level 2\nTarrant County\nTarrant County\n    3\nAdministrative Area Level 1\nTexas\nTX\n    4\nCountry\nUnited States\nUS\n    Arlington, VA, USA\n1\nLocality\nArlington\nArlington\n    2\nAdministrative Area Level 2\nArlington County\nArlington County\n    3\nAdministrative Area Level 1\nVirginia\nVA\n    4\nCountry\nUnited States\nUS\n  \n  \n  \n\n\nFigure 1: The components of addresses in gmaps_cities"
  },
  {
    "objectID": "Chapter24.html#question-1-2",
    "href": "Chapter24.html#question-1-2",
    "title": "Chapter 24",
    "section": "Question 1",
    "text": "Question 1\nRectangle the df_col and df_row below. They represent the two ways of encoding a data frame in JSON.\n\njson_col &lt;- parse_json('\n  {\n    \"x\": [\"a\", \"x\", \"z\"],\n    \"y\": [10, null, 3]\n  }\n')\n\n\njson_row &lt;- parse_json('\n  [\n    {\"x\": \"a\", \"y\": 10},\n    {\"x\": \"x\", \"y\": null},\n    {\"x\": \"z\", \"y\": 3}\n  ]\n')\n\ndf_col &lt;- tibble(json = list(json_col)) \n\ndf_row &lt;- tibble(json = json_row)\n\n# Rectangling df_col\ndf_col |&gt;\n  unnest_wider(json) |&gt;\n  unnest_longer(c(x, y))\n\n# A tibble: 3 × 2\n  x         y\n  &lt;chr&gt; &lt;int&gt;\n1 a        10\n2 x        NA\n3 z         3\n\n# Reactangling df_row\ndf_row |&gt;\n  unnest_wider(json)\n\n# A tibble: 3 × 2\n  x         y\n  &lt;chr&gt; &lt;int&gt;\n1 a        10\n2 x        NA\n3 z         3"
  },
  {
    "objectID": "Chapter25.html",
    "href": "Chapter25.html",
    "title": "Chapter 25",
    "section": "",
    "text": "In the realm of aviation history and military technology, fighter aircraft hold a pivotal role, representing the pinnacle of technological prowess and strategic significance. To delve into the comprehensive landscape of fighter aircraft, this assignment embarks on a journey of data acquisition and analysis, leveraging the versatile capabilities of the R programming language, specifically the rvest package(Wickham 2022) within the tidyverse framework(Wickham et al. 2019).\nOur primary data source, the Wikipedia page titled “List of Fighter Aircraft,” serves as a repository of valuable information encompassing various facets of fighter aircraft production, categorization, and operational status.\nThis page aims to meticulously extract, process, and analyze the data from this source, shedding light on the countries that have contributed significantly to fighter aircraft production, the prevailing types of aircraft that have dominated the skies, and the intriguing disparities between operational and retired aircraft, including the number of prototypes produced. Through this endeavor, we endeavor to gain profound insights into the historical and contemporary dynamics of fighter aircraft, uncovering patterns and trends that illuminate their pivotal role in military aviation and defense strategies.\nExperiment scraping the Wikipedia page of fighter aircraft\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(scales)\n\n\n# The web-address to harvest\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_fighter_aircraft\"\n\n# Read in the html file from the URL\nurl_html &lt;- read_html(url)\n\n# Create a nice looking tibble from the data\ndf &lt;- url_html |&gt;\n  html_element(\"table\") |&gt;\n  html_table() |&gt;\n  \n  # house-keeping tasks: cleaning names etc.\n  janitor::clean_names() |&gt;\n  rename(\n    numbers = no,\n    year = date\n  ) |&gt;\n  select(-notes) |&gt;\n  mutate(\n    numbers = parse_number(numbers),\n    class = str_replace_all(class, \n                            pattern = \"\\\"\", \n                            replacement = \"\"),\n    class = na_if(class, \"\"),\n    status = fct(status, levels = c(\"Abandoned\",\n                                    \"Prototype\",\n                                    \"In Development\",\n                                    \"Retired\",\n                                    \"Operational\"))\n  )\n\nDisplaying the complete data, in a nicely formatted table at Figure 1 : –\n\ndf |&gt;\n  gt() |&gt;\n  cols_label_with(fn = ~ janitor::make_clean_names(., \"title\")) |&gt;\n  opt_interactive(page_size_default = 5,\n                  use_highlight = TRUE,\n                  use_resizers = TRUE)\n\n\n\n\n\n\n\n\nFigure 1: The complete data on fighter aircraft produced by different countries\n\n\n\nUsing the data for some analysis\nIn order to visually convey the the total number of fighter aircraft manufactured by each nation, we employ the powerful visualization tool provided by Datawrapper, and create a pie chart: –\n\nNow, we will visualize the production of fighter aircraft over different decades, categorized by their design years and attributed to various countries, using a stacked bar chart in Figure 2 .\n\n# A stacked bar chart for country-wise aircraft made in each decade\n\ndf |&gt;\n  mutate(\n    country = fct(country),\n    decade = fct(paste0(floor(year / 10) * 10, \"s\"), \n                 levels = paste0(seq(1910, 2020, by = 10), \"s\"))\n  ) |&gt;\n  group_by(decade, country) |&gt;\n  count(wt = numbers, \n        name = \"nos_planes\") |&gt;\n  \n  # Ungroup so that factor lumping works\n  ungroup() |&gt;\n  \n  # Remove 2020s as very few observations; and irrelevant to the message\n  filter(decade != \"2020s\") |&gt;\n  \n  # Lump into 5 factors for easy display of coloured stacked bar chart\n  mutate(country = fct_lump_n(country, n = 5, w = nos_planes)) |&gt; \n  \n  # Start plotting\n  ggplot(aes(x = decade,\n             y = nos_planes,\n             fill = country,\n             label = country,\n             text = paste0(\"Aircrafts produced in the decade: \", nos_planes))\n         ) +\n  geom_bar(stat = \"identity\",\n           position = \"stack\") +\n  scale_y_continuous(labels = label_number_si()) +\n  labs(x = NULL, \n       y = \"Number of aircraft\",\n       title = \"The World War decades saw maximum aircraft production\",\n       subtitle = \"Germany dominated the prodcution in the pre WW-II 1930s\\nBut, therafter, US and USSR dominated production right upto the end of cold war in 1980s\",\n       fill = NULL) +\n  scale_fill_manual(values = c(\"#bbbfc9\",\n                               \"red\",\n                               \"#8491b5\",\n                               \"#5c6580\",\n                               \"#576cad\",\n                               \"#f5f114\")) +\n  guides(fill = guide_legend(nrow = 1)) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        panel.grid = element_blank(),\n        plot.title.position = \"plot\")\n\n\n\n\nFigure 2: A stacked bar chart for country-wise aircraft made in each decade\n\n\n\n\nHere in Figure 3 are the most produced operational aircraft still operational as of today.\n\ndf |&gt;\n  filter(status == \"Operational\") |&gt;\n  arrange(desc(numbers)) |&gt;\n  slice_head(n = 10) |&gt;\n  mutate(type = if_else(type == \"McDonnell Douglas F/A-18 Hornet\",\n                        \"F/A-18 Hornet\",\n                        type)) |&gt;\n  ggplot(aes(x = numbers,\n             y = reorder(type, numbers),\n             fill = country)) +\n  geom_bar(stat = \"identity\") +\n  theme_void() +\n  geom_text(aes(x = 0,\n                label = type),\n            hjust = \"left\") +\n  geom_text(aes(label = numbers),\n            hjust = -0.3) +\n  labs(fill = NULL,\n       title = \"The most produced, and still operational, fighter planes belong to US or USSR\",\n       subtitle = \"MiG-21 is most produced craft, outnumbering the next highest by more than twice!\")  +\n  scale_fill_manual(values = c(\"#f5f584\", \"lightblue\", \"#828afa\", \"#f28374\")) +\n  theme(plot.title.position = \"panel\",\n        legend.position = \"bottom\")\n\n\n\n\nFigure 3: Most produced, and currently operational aircraft\n\n\n\n\nBut, if we look at the most mass-produced ever made, all of them are retired as shown below in Figure 4 .\n\ndf |&gt;\n  arrange(desc(numbers)) |&gt;\n  slice_head(n = 10) |&gt;\n  ggplot(aes(x = numbers,\n             y = reorder(type, numbers),\n             fill = country)) +\n  geom_bar(stat = \"identity\") +\n  theme_void() +\n  geom_text(aes(x = 0,\n                label = type),\n            hjust = \"left\") +\n  geom_text(aes(label = scales::comma(numbers)),\n            hjust = -0.3) +\n  labs(fill = NULL,\n       title = \"However, the most produced fighter planes ever, are all retired!\",\n       subtitle = \"Historically, Germany and UK, mass-produced the planes they no longer use.\")  +\n  scale_fill_manual(values = c(\"#97979c\", \"#54b8ff\", \"#828afa\", \"#f28374\")) +\n  theme(plot.title.position = \"panel\",\n        legend.position = \"bottom\")\n\n\n\n\nFigure 4: Most produced aircraft of all time: all of them are retired\n\n\n\n\nNow, looking at the class of aircraft, the most produced type of aircraft, all time are shown in Figure 5 .\n\ndf |&gt;\n  mutate(class = if_else(class == \"fighter-bomber\",\n                         \"Fighter-bomber\",\n                         class)) |&gt;\n  count(class, sort = TRUE, wt = numbers) |&gt;\n  drop_na() |&gt;\n  mutate(\n    class = fct(class),\n    class = fct_lump_n(class, n = 9, w = n)\n  ) |&gt;\n  count(class, sort = TRUE, wt = n) |&gt;\n  ggplot(aes(x = n,\n             y = fct_rev(class))) +\n  geom_bar(stat = \"identity\", fill = \"grey\") +\n  theme_void() +\n  geom_text(aes(x = 0,\n                label = class),\n            hjust = \"left\") +\n  geom_text(aes(label = scales::comma(n)),\n            hjust = -0.1) +\n  labs(fill = NULL,\n       title = \"Fighter-bombers were the most common type of aircraft ever produced!\") +\n  theme(plot.title.position = \"panel\",\n        legend.position = \"bottom\")\n\n\n\n\nFigure 5: Number of aircraft, ever produced, by the type or class of aircraft\n\n\n\n\nLaslty, the Table 1 provides the top 10 fighter aircraft types that predominantly remained in the prototype stage, offering valuable insights into classes which remained experimental only in military aviation development.\n\ndf |&gt;\n  group_by(class, status) |&gt;\n  count(wt = numbers) |&gt;\n  group_by(class) |&gt;\n  mutate(total = sum(n)) |&gt;\n  filter(status == \"Prototype\") |&gt;\n  mutate(prop = n/total) |&gt;\n  filter(total &gt;= 10) |&gt;\n  arrange(desc(prop)) |&gt;\n  drop_na() |&gt;\n  select(-status) |&gt;\n  ungroup() |&gt;\n  rename(\n    type_of_aircraft = class,\n    number_of_prototypes = n,\n    total_aircraft_produced = total,\n    percentage_of_prototypes = prop\n  ) |&gt;\n  slice_head(n = 10) |&gt;\n  gt() |&gt;\n  cols_label_with(fn = ~ janitor::make_clean_names(., \"title\")) |&gt;\n  fmt_percent(columns = percentage_of_prototypes, decimals = 1) |&gt;\n  gt_theme_538()\n\n\n\n\n\nTable 1:  Aircraft which had highest percentage of total produced planes\nremaining as prototypes only \n  \n    \n    \n      Type of Aircraft\n      Number of Prototypes\n      Total Aircraft Produced\n      Percentage of Prototypes\n    \n  \n  \n    High-altitude fighter\n12\n12\n100.0%\n    Mixed power\n13\n13\n100.0%\n    VTO rocket interceptor\n36\n36\n100.0%\n    VTOL fighter\n22\n22\n100.0%\n    Parasite fighter\n11\n18\n61.1%\n    Escort fighter\n57\n799\n7.1%\n    Shipboard fighter\n9\n157\n5.7%\n    Cannon fighter\n5\n115\n4.3%\n    Interceptor rocket\n12\n382\n3.1%\n    High-altitude interceptor\n3\n127\n2.4%\n  \n  \n  \n\n\n\n\n\n\n\n\n\nReferences\n\nWickham, Hadley. 2022. “Rvest: Easily Harvest (Scrape) Web Pages.” https://CRAN.R-project.org/package=rvest.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse” 4: 1686. https://doi.org/10.21105/joss.01686."
  },
  {
    "objectID": "Chapter26.html",
    "href": "Chapter26.html",
    "title": "Chapter 26",
    "section": "",
    "text": "library(tidyverse)\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(nycflights13)\ndata(\"flights\")\nImportant stuff from Chapter 26, R for Data Science (2e)"
  },
  {
    "objectID": "Chapter26.html#question-1",
    "href": "Chapter26.html#question-1",
    "title": "Chapter 26",
    "section": "Question 1",
    "text": "Question 1\nPractice turning the following code snippets into functions. Think about what each function does. What would you call it? How many arguments does it need?\nmean(is.na(x))\nmean(is.na(y))\nmean(is.na(z))\nThe following function computes the proportion of missing values in a vector. It needs one argument.\n\nprop_missing &lt;- function(x){\n  mean(is.na(x))\n}\n\n_____________\nx / sum(x, na.rm = TRUE)\ny / sum(y, na.rm = TRUE)\nz / sum(z, na.rm = TRUE)\nThe following function computes the proportion of each element of a vector to the sum of the vector. It needs one argument.\n\nprop_element &lt;- function(x){\n  x / sum(x, na.rm = TRUE)\n}\n\n_____________\nround(x / sum(x, na.rm = TRUE) * 100, 1)\nround(y / sum(y, na.rm = TRUE) * 100, 1)\nround(z / sum(z, na.rm = TRUE) * 100, 1)\nThis function below computes the percentage of each value (as compared to the sum of the vector of values) within one decimal place.\n\nperc_element &lt;- function(x){\n  round(x / sum(x, na.rm = TRUE) * 100, 1)\n}"
  },
  {
    "objectID": "Chapter26.html#question-2",
    "href": "Chapter26.html#question-2",
    "title": "Chapter 26",
    "section": "Question 2",
    "text": "Question 2\nIn the second variant of rescale01(), infinite values are left unchanged. Can you rewrite rescale01() so that -Inf is mapped to 0, and Inf is mapped to 1?\n\nrescale01 &lt;- function(x) {\n  \n  # Replace -Inf with the minimum number (other than -Inf)\n  min_value &lt;- min(x[is.finite(x)])\n  x[x == -Inf] &lt;- min_value\n\n  # Replace +Inf with the maximum number (other than +Inf)\n  max_value &lt;- max(x[is.finite(x)])\n  x[x == Inf] &lt;- max_value\n\n  rng &lt;- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}"
  },
  {
    "objectID": "Chapter26.html#question-3",
    "href": "Chapter26.html#question-3",
    "title": "Chapter 26",
    "section": "Question 3",
    "text": "Question 3\nGiven a vector of birth-dates, write a function to compute the age in years.\n\n# Function to compute age in years from birth dates\n\ncompute_age &lt;- function(birth_dates) {\n  \n  # Convert the birth dates to Date objects\n  birth_dates &lt;- as.Date(birth_dates)\n  \n  # Calculate the current date\n  current_date &lt;- Sys.Date()\n  \n  # Calculate age in years using lubridate\n  ages &lt;- interval(birth_dates, current_date) %/% years(1)\n  \n  # Return the ages as a numeric vector\n  return(ages)\n}"
  },
  {
    "objectID": "Chapter26.html#question-4",
    "href": "Chapter26.html#question-4",
    "title": "Chapter 26",
    "section": "Question 4",
    "text": "Question 4\nWrite your own functions to compute the variance and skewness of a numeric vector. You can look up the definitions on Wikipedia or elsewhere.\nTo compute the variance and skewness of a numeric vector, we can define custom functions for each: –\n\nVariance Calculation using the formula:\n\n# Function to compute the variance\ncompute_variance &lt;- function(x) {\n  n &lt;- length(x)\n  x_bar &lt;- mean(x)\n  sum_squared_diff &lt;- sum((x - x_bar)^2)\n  variance &lt;- sum_squared_diff / (n - 1)\n  return(variance)\n}\n\nSkewness Calculation using the formula:\n\n# Function to compute the skewness\ncompute_skewness &lt;- function(x) {\n  n &lt;- length(x)\n  x_bar &lt;- mean(x)\n  std_dev &lt;- sqrt(compute_variance(x))  # Using previously defined variance function\n  skewness &lt;- sum((x - x_bar)^3) / ((n - 1) * std_dev^3)\n  return(skewness)\n}\nThese custom functions compute_variance and compute_skewness will calculate the variance and skewness of a numeric vector based on the provided formulas, without relying on external packages."
  },
  {
    "objectID": "Chapter26.html#question-5",
    "href": "Chapter26.html#question-5",
    "title": "Chapter 26",
    "section": "Question 5",
    "text": "Question 5\nWrite both_na(), a summary function that takes two vectors of the same length and returns the number of positions that have an NA in both vectors.\n\n# Method 1\nboth_na &lt;- function(x,y){\n  \n  x_na &lt;- which(is.na(x))\n  y_na &lt;- which(is.na(y))\n  \n  # values of x which are also present in y\n  common &lt;- x_na %in% y_na\n  return(common)\n}\n\n# Method 2\nboth_na &lt;- function(vector1, vector2) {\n  if (length(vector1) != length(vector2)) {\n    stop(\"Both vectors must have the same length.\")\n  }\n  \n  # Find the indices where both vectors have NA values\n  na_indices &lt;- which(is.na(vector1) & is.na(vector2))\n  \n  return(na_indices)\n}"
  },
  {
    "objectID": "Chapter26.html#question-6",
    "href": "Chapter26.html#question-6",
    "title": "Chapter 26",
    "section": "Question 6",
    "text": "Question 6\nRead the documentation to figure out what the following functions do. Why are they useful even though they are so short?\nis_directory &lt;- function(x) {\n  file.info(x)$isdir\n }\nThis function tells whether an object is a file or a directory. It is useful because it will return a logical value, i.e., either TRUE or FALSE and thus can be used inside other operations, rather than using the $ operator; and is easy to remember in English.\n_________\nis_readable &lt;- function(x) {\n  file.access(x, 4) == 0\n}\nThis function tells us whether the file has a read permission or not, i.e, whether there is access to the file or not. It is useful because one does not have to remember the mode argument values for file.access() function, and is easy to remember in English for further use in for loops etc."
  },
  {
    "objectID": "Chapter26.html#question-1-1",
    "href": "Chapter26.html#question-1-1",
    "title": "Chapter 26",
    "section": "Question 1",
    "text": "Question 1\nUsing the datasets from nycflights13, write a function that:\n\n# Creating a function to display the results in a nice way\n\ndisplay_results &lt;- function(data){\n  data |&gt;\n  slice_head(n = 5) |&gt; \n  gt() |&gt; \n  cols_label_with(fn = ~ janitor::make_clean_names(., \"title\")) |&gt;\n  gt_theme_538()\n}\n\n\nFinds all flights that were cancelled (i.e. is.na(arr_time)) or delayed by more than an hour.\nflights |&gt; filter_severe()\n\nfilter_severe &lt;- function(data, arrival_delay) {\n  data |&gt;\n    filter(is.na({{arrival_delay}}) | ({{arrival_delay}} &gt; 60))\n}\n\n# Runinng an example to show it works\nflights |&gt;\n  filter_severe(arr_delay) |&gt;\n  select(1:10) |&gt;\n  display_results()\n\n\n\n\n\n  \n\n    \n      Year\n      Month\n      Day\n      Dep Time\n      Sched Dep Time\n      Dep Delay\n      Arr Time\n      Sched Arr Time\n      Arr Delay\n      Carrier\n    \n  \n  \n    2013\n1\n1\n811\n630\n101\n1047\n830\n137\nMQ\n    2013\n1\n1\n848\n1835\n853\n1001\n1950\n851\nMQ\n    2013\n1\n1\n957\n733\n144\n1056\n853\n123\nUA\n    2013\n1\n1\n1114\n900\n134\n1447\n1222\n145\nUA\n    2013\n1\n1\n1120\n944\n96\n1331\n1213\n78\nEV\n  \n\n\n\n\n\n\nCounts the number of cancelled flights and the number of flights delayed by more than an hour.\nflights |&gt; group_by(dest) |&gt; summarize_severe()\n\nsummarize_severe &lt;- function(data, arrival_delay){\n  data |&gt;\n    summarize(\n      cancelled_flights = sum(is.na({{arrival_delay}})),\n      delayed_flights = sum({{arrival_delay}} &gt; 60, na.rm = TRUE)\n    )\n}\n\n# Runinng an example to show it works\nflights |&gt;\n  group_by(dest) |&gt;\n  summarize_severe(arr_delay) |&gt;\n  display_results()\n\n\n\n\n\n  \n\n    \n      Dest\n      Cancelled Flights\n      Delayed Flights\n    \n  \n  \n    ABQ\n0\n25\n    ACK\n1\n12\n    ALB\n21\n59\n    ANC\n0\n0\n    ATL\n378\n1433\n  \n\n\n\n\n\n\nFinds all flights that were cancelled or delayed by more than a user supplied number of hours:\nflights |&gt; filter_severe(hours = 2)\n\nfilter_severe &lt;- function(data, hours){\n  data |&gt;\n    filter(arr_delay &gt; (hours*60))\n}\n\n# Running an example to show that it works\nflights |&gt;\n  filter_severe(hours = 4) |&gt;\n  select(1:10) |&gt;\n  display_results()\n\n\n\n\n\n  \n\n    \n      Year\n      Month\n      Day\n      Dep Time\n      Sched Dep Time\n      Dep Delay\n      Arr Time\n      Sched Arr Time\n      Arr Delay\n      Carrier\n    \n  \n  \n    2013\n1\n1\n848\n1835\n853\n1001\n1950\n851\nMQ\n    2013\n1\n1\n1815\n1325\n290\n2120\n1542\n338\nEV\n    2013\n1\n1\n1842\n1422\n260\n1958\n1535\n263\nEV\n    2013\n1\n1\n2115\n1700\n255\n2330\n1920\n250\n9E\n    2013\n1\n1\n2205\n1720\n285\n46\n2040\n246\nAA\n  \n\n\n\n\n\n\nSummarizes the weather to compute the minimum, mean, and maximum, of a user supplied variable:\nweather |&gt; summarize_weather(temp)\n\nsummarize_weather &lt;- function(data, variable){\n  data |&gt;\n    summarize(\n      mean = mean({{variable}}, na.rm = TRUE),\n      minimum = min({{variable}}, na.rm = TRUE),\n      maximum = max({{variable}}, na.rm = TRUE)\n    )\n}\n# Runinng an example to show it works\nweather |&gt;\n  group_by(origin) |&gt;\n  summarize_weather(temp) |&gt;\n  display_results() |&gt; fmt_number(decimals = 2)\n\n\n\n\n\n  \n\n    \n      Origin\n      Mean\n      Minimum\n      Maximum\n    \n  \n  \n    EWR\n55.55\n10.94\n100.04\n    JFK\n54.47\n12.02\n98.06\n    LGA\n55.76\n12.02\n98.96\n  \n\n\n\n\n\n\nConverts the user supplied variable that uses clock time (e.g., dep_time, arr_time, etc.) into a decimal time (i.e. hours + (minutes / 60)).\n\nflights |&gt; standardize_time(sched_dep_time)\n\n# Method 1\nstandardize_time &lt;- function(data, variable){\n  data |&gt;\n    mutate(std_time = round({{variable}} %/% 100) + (({{variable}} %% 100)/60), 2) |&gt;\n    relocate(std_time, .after = {{variable}})\n}\n\n# Method 2 (after learning use of \":=\" in Section 26.4.2)\nstandardize_time &lt;- function(data, variable){\n  data |&gt;\n    mutate({{variable}} := round({{variable}} %/% 100) + (({{variable}} %% 100)/60), 2) \n}\n\n# Runinng an example to show it works\nflights |&gt;\n  standardize_time(sched_dep_time) |&gt;\n  select(1:10) |&gt;\n  display_results()\n\n\n\n\n\n  \n    \n    \n      Year\n      Month\n      Day\n      Dep Time\n      Sched Dep Time\n      Dep Delay\n      Arr Time\n      Sched Arr Time\n      Arr Delay\n      Carrier\n    \n  \n  \n    2013\n1\n1\n517\n5.250000\n2\n830\n819\n11\nUA\n    2013\n1\n1\n533\n5.483333\n4\n850\n830\n20\nUA\n    2013\n1\n1\n542\n5.666667\n2\n923\n850\n33\nAA\n    2013\n1\n1\n544\n5.750000\n-1\n1004\n1022\n-18\nB6\n    2013\n1\n1\n554\n6.000000\n-6\n812\n837\n-25\nDL"
  },
  {
    "objectID": "Chapter26.html#question-2-1",
    "href": "Chapter26.html#question-2-1",
    "title": "Chapter 26",
    "section": "Question 2",
    "text": "Question 2\nFor each of the following functions list all arguments that use tidy evaluation and describe whether they use data-masking or tidy-selection: distinct(), count(), group_by(), rename_with(), slice_min(), slice_sample().\nIn the context of the dplyr package in R, tidy evaluation refers to a method of non-standard evaluation where you can use variables as arguments to functions and refer to those variables within the context of a data frame. It has two parts: –\n\nData Masking: Data masking involves creating a temporary environment within a function where external variables from the global environment are hidden or masked. This means that variables from the global environment are temporarily unavailable inside the function, preventing unintended interactions or conflicts. For example:\n\ndata &lt;- data.frame(x = 1:5, y = 6:10)\n\ncustom_function &lt;- function(df) {\n\n  # Using data masking to prevent access to global 'data' variable\n  df %&gt;%\n    mutate(z = x + y)\n}\n\ncustom_function(data)\n\n  x  y  z\n1 1  6  7\n2 2  7  9\n3 3  8 11\n4 4  9 13\n5 5 10 15\n\n\nIn this example, the custom_function takes a data frame df as an argument and uses the mutate() function from dplyr to create a new variable z. Inside the function, data masking ensures that x and y refer to the columns within the df argument, not the global data data frame.\nTidy Selection: Tidy selection involves selecting or manipulating columns within a data frame using non-standard evaluation (NSE). It allows you to refer to column names as symbols or expressions, facilitating dynamic and programmatic column selection. For example: –\n\ndata &lt;- data.frame(x = 1:5, y = 6:10)\n\nselected_columns &lt;- c(\"x\", \"y\")\n\ndata %&gt;%\n  select(all_of(selected_columns))\n\n  x  y\n1 1  6\n2 2  7\n3 3  8\n4 4  9\n5 5 10\n\n\nIn this example, the select() function uses tidy selection to select columns specified in the selected_columns vector.\n\nThus, Data masking primarily deals with the environment of the function, while tidy selection focuses on column selection and manipulation within a data frame.\nAll of the following functions use Tidy Evaluation in the following way: –\n\n\n\n\n\n\n\n\n\nFunction\nDescription\nTidy Selection\nArguments\n\n\n\n\ndistinct()\nSelect distinct rows based on specified columns\nYes\n\nThe data argument does not involve Data Masking or Tidy Selection.\nThe ... argument supports Tidy Selection methods for specifying columns. It also employs data masking to prevent global environment variables from interfering.\nThe .keep_all argument does not involve Data Masking or Tidy Selection.\n\n\n\ncount()\nCount the number of rows in groups defined by specific variables\nYes\n\nThe data argument does not involve Data Masking or Tidy Selection.\nThe ... argument supports Tidy Selection methods for specifying columns. It also employs data masking to prevent global environment variables from interfering.\nThe .name argument does not involve Data Masking or Tidy Selection.\n\n\n\ngroup_by()\nGroup data by specific columns\nYes\n\nThe data argument does not involve Data Masking or Tidy Selection.\nThe ... argument supports data masking to prevent global environment variables from interfering. It does not use tidy selection.\nThe .add argument does not involve Data Masking or Tidy Selection.\n\n\n\nrename_with()\nRename columns based on a function or expression\nYes\n\nThe data argument does not involve Data Masking or Tidy Selection.\nThe .fn and ... arguments support Tidy Selection methods for specifying columns and renaming logic.\nThe .cols argument supports Tidy Selection methods for specifying the columns to apply renaming to. It also employs data masking to prevent global environment variables from interfering.\n\n\n\nslice_min()\nFilter rows corresponding to the minimum value of a column\nYes\n\nThe data and n arguments do not involve Data Masking or Tidy Selection.\nThe order_by argument supports Tidy Selection methods for specifying the column to determine the order for finding minimum values. It also employs data masking to prevent global environment variables from interfering.\n\n\n\nslice_sample()\nRandomly sample rows from a data frame\nYes\n\nThe data, n, replace, and .groups arguments do not involve Data Masking or Tidy Selection.\nThe weight argument supports Tidy Selection methods for specifying the sampling weight variable.\n\n\n\n\nIn summary, all of the functions mentioned above support tidy evaluation for specifying arguments related to column selection or manipulation."
  },
  {
    "objectID": "Chapter26.html#question-3-1",
    "href": "Chapter26.html#question-3-1",
    "title": "Chapter 26",
    "section": "Question 3",
    "text": "Question 3\nGeneralize the following function so that you can supply any number of variables to count.\ncount_prop &lt;- function(df, var, sort = FALSE) {\n  df |&gt;\n    count({{ var }}, sort = sort) |&gt;\n    mutate(prop = n / sum(n))\n}\nHere is the modified function to allow user to supply any number of variables to count: –\n\ncount_prop &lt;- function(df, vars, sort = FALSE) {\n  df |&gt;\n    count(pick({{ vars }}), sort = sort) |&gt;\n    mutate(prop = n / sum(n))\n}\n\n# Testing the results\nflights |&gt;\n  count_prop(c(dest, origin)) |&gt;\n  slice_head(n = 10) |&gt; \n  gt() |&gt; gt_theme_538() |&gt; fmt_number(prop, decimals = 4)\n\n\n\n\n\n  \n    \n    \n      dest\n      origin\n      n\n      prop\n    \n  \n  \n    ABQ\nJFK\n254\n0.0008\n    ACK\nJFK\n265\n0.0008\n    ALB\nEWR\n439\n0.0013\n    ANC\nEWR\n8\n0.0000\n    ATL\nEWR\n5022\n0.0149\n    ATL\nJFK\n1930\n0.0057\n    ATL\nLGA\n10263\n0.0305\n    AUS\nEWR\n968\n0.0029\n    AUS\nJFK\n1471\n0.0044\n    AVL\nEWR\n265\n0.0008"
  },
  {
    "objectID": "Chapter26.html#question-1-2",
    "href": "Chapter26.html#question-1-2",
    "title": "Chapter 26",
    "section": "Question 1",
    "text": "Question 1\nDraw a scatter-plot given dataset and x and y variables.\n\nscatterplot &lt;- function(data, x, y){\n  data |&gt;\n    ggplot(aes(x = {{x}},\n               y = {{y}})) +\n    geom_point() +\n    theme_classic()\n}"
  },
  {
    "objectID": "Chapter26.html#question-2-2",
    "href": "Chapter26.html#question-2-2",
    "title": "Chapter 26",
    "section": "Question 2",
    "text": "Question 2\nAdd a line of best fit (i.e. a linear model with no standard errors).\n\nscatterplot &lt;- function(data, x, y){\n  data |&gt;\n    ggplot(aes(x = {{x}},\n               y = {{y}})) +\n    geom_point() +\n    geom_smooth(method = \"lm\",\n                formula = {{y}} ~ {{x}},\n                se = FALSE) +\n    theme_classic()\n}"
  },
  {
    "objectID": "Chapter26.html#question-3-2",
    "href": "Chapter26.html#question-3-2",
    "title": "Chapter 26",
    "section": "Question 3",
    "text": "Question 3\nAdd a title.\n\nscatterplot &lt;- function(data, x, y){\n    ggplot(data, aes(x = {{x}},\n               y = {{y}})) +\n    geom_point() +\n    geom_smooth(method = \"lm\",\n                formula = {{y}} ~ {{x}},\n                se = FALSE) +\n    labs(title = rlang::englue(\"A scatter plot of {{y}} vs. {{x}}\")) +\n    theme_classic()\n}"
  },
  {
    "objectID": "Chapter26.html#question-1-3",
    "href": "Chapter26.html#question-1-3",
    "title": "Chapter 26",
    "section": "Question 1",
    "text": "Question 1\nRead the source code for each of the following two functions, puzzle out what they do, and then brainstorm better names.\nis_prefix()\n# is_prefix\nf1 &lt;- function(string, prefix) {\n  str_sub(string, 1, str_length(prefix)) == prefix\n}\nmatch_length()\nf3 &lt;- function(x, y) {\n  rep(y, length.out = length(x))\n}"
  },
  {
    "objectID": "Chapter26.html#question-2-3",
    "href": "Chapter26.html#question-2-3",
    "title": "Chapter 26",
    "section": "Question 2",
    "text": "Question 2\nTake a function that you’ve written recently and spend 5 minutes brainstorming a better name for it and its arguments.\nHere are some suggestions for renaming html_nodes() and its arguments (from the package rvest):\n\nOriginal Function: html_nodes()\n\nhtml_extract() - This name suggests the action of extracting nodes from an HTML document.\nselect_html_nodes() - A more descriptive name that clarifies the purpose of the function.\n\nArgument css:\n\nselector - A more explicit name to indicate that this argument takes CSS selectors.\nelement_selector - If the purpose of the argument is to select HTML elements, this name makes it clear.\n\nArgument xpath:\n\nxpath_expression - A more descriptive name that specifies the type of input expected.\nxml_path - A concise alternative that still hints at the purpose.\n\nArgument trim:\n\nremove_whitespace - A name that better reflects the action performed when trim is set to TRUE.\nclean_whitespace - Another option to indicate the removal of whitespace."
  },
  {
    "objectID": "Chapter26.html#question-3-3",
    "href": "Chapter26.html#question-3-3",
    "title": "Chapter 26",
    "section": "Question 3",
    "text": "Question 3\nMake a case for why norm_r(), norm_d() etc. would be better than rnorm(), dnorm(). Make a case for the opposite. How could you make the names even clearer?\nFunction naming conventions in R often follow a pattern where functions are named with a verb followed by a noun, which makes the code more readable and easier to understand.\nArguments for norm_r() and norm_d() (Custom Names):\n\nVerb-First Naming: Putting the action (e.g., “normalize”) before the distribution name (e.g., “norm”) makes the function name more descriptive and aligns with R’s naming conventions.\nConsistency: These names maintain consistency with other functions in R that follow a similar pattern (e.g., read_csv(), plot_hist()).\nClarity: These names emphasize the action being performed (normalizing) over the distribution being used (normal distribution), which might be more intuitive to some users.\nReadability: These names read like a sentence: “Normalize a random sample from a normal distribution.”\n\nArguments for rnorm() and dnorm() (Current Names):\n\nHistorical Convention: Functions like rnorm() and dnorm() have been part of the base R distribution for a long time. Changing their names might lead to confusion for users who are already familiar with these functions.\nAlignment with Mathematical Notation: The rnorm() and dnorm() naming closely resembles mathematical notation, making it easier for users with a strong mathematical background to recognize and use these functions.\nPackage Consistency: Maintaining the original names ensures consistency with other functions in the stats package (e.g., pnorm(), qnorm()), which also follow this convention.\n\nEven Clearer Names: If we want to make the function names even clearer, we can consider longer, more descriptive names, such as:\n\ngenerate_random_samples_from_normal_distribution() for rnorm(): This explicitly states what the function does.\nprobability_density_function_of_normal_distribution() for dnorm(): This specifies the exact purpose of the function.\n\nWhile these longer names are more descriptive, they may become unwieldy in code. Striking a balance between clarity and brevity is important."
  },
  {
    "objectID": "Chapter27.html",
    "href": "Chapter27.html",
    "title": "Chapter 27",
    "section": "",
    "text": "library(tidyverse)\nlibrary(gt)\nlibrary(gtExtras)"
  },
  {
    "objectID": "Chapter27.html#question-1",
    "href": "Chapter27.html#question-1",
    "title": "Chapter 27",
    "section": "Question 1",
    "text": "Question 1\nPractice your across() skills by:\n\nComputing the number of unique values in each column of palmerpenguins::penguins.\n\npalmerpenguins::penguins |&gt;\n  summarise(across(.cols = everything(),\n                   .fns = n_distinct))\n\n# A tibble: 1 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n    &lt;int&gt;  &lt;int&gt;          &lt;int&gt;         &lt;int&gt;             &lt;int&gt;       &lt;int&gt;\n1       3      3            165            81                56          95\n# ℹ 2 more variables: sex &lt;int&gt;, year &lt;int&gt;\n\n\nComputing the mean of every column in mtcars.\n\nmtcars |&gt;\n  summarise(across(everything(), mean))\n\n       mpg    cyl     disp       hp     drat      wt     qsec     vs      am\n1 20.09062 6.1875 230.7219 146.6875 3.596563 3.21725 17.84875 0.4375 0.40625\n    gear   carb\n1 3.6875 2.8125\n\n\nGrouping diamonds by cut, clarity, and color then counting the number of observations and computing the mean of each numeric column.\n\nggplot2::diamonds |&gt;\n  group_by(cut, clarity, color) |&gt;\n  summarise(\n    across(\n    .cols = where(is.numeric),\n    .fns = function(x) mean(x, na.rm = TRUE)\n    ),\n    n = n()\n    )\n\n# A tibble: 276 × 11\n# Groups:   cut, clarity [40]\n   cut   clarity color carat depth table price     x     y     z     n\n   &lt;ord&gt; &lt;ord&gt;   &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n 1 Fair  I1      D     1.88   65.6  56.8 7383   7.52  7.42  4.90     4\n 2 Fair  I1      E     0.969  65.6  58.1 2095.  6.17  6.06  4.01     9\n 3 Fair  I1      F     1.02   65.7  58.4 2544.  6.14  6.04  4.00    35\n 4 Fair  I1      G     1.23   65.3  57.7 3187.  6.52  6.43  4.23    53\n 5 Fair  I1      H     1.50   65.8  58.4 4213.  6.96  6.86  4.55    52\n 6 Fair  I1      I     1.32   65.7  58.4 3501   6.76  6.65  4.41    34\n 7 Fair  I1      J     1.99   66.5  57.9 5795.  7.55  7.46  4.99    23\n 8 Fair  SI2     D     1.02   64.7  58.6 4355.  6.24  6.17  4.01    56\n 9 Fair  SI2     E     1.02   63.4  59.5 4172.  6.28  6.22  3.96    78\n10 Fair  SI2     F     1.08   63.8  59.5 4520.  6.36  6.30  4.04    89\n# ℹ 266 more rows"
  },
  {
    "objectID": "Chapter27.html#question-2",
    "href": "Chapter27.html#question-2",
    "title": "Chapter 27",
    "section": "Question 2",
    "text": "Question 2\nWhat happens if you use a list of functions in across(), but don’t name them? How is the output named?\nUsing a list of functions in across() without naming them will result in automatically generated names for the output columns, following the default convention of\n\n\"{.col}\" for the single function case and\n\"{.col}_{.fn}\" for the case where a named function list is used for .fns.\n\"{.col}_1\" , \"{.col}_2\" etc. for the case where unnamed list is used for .fns.\n\nThus, when we use a list, resulting columns will be named like x_mean when name of original column is x and new named function used across is mean.\n\n# Create a sample data frame\ndata &lt;- data.frame(\n  A = c(1, 2, 3, NA, 4),\n  B = c(4, NA, 5, 6, 7)\n)\n\n# Use across() without naming the functions\ndata %&gt;%\n  mutate(across(everything(), list(sqrt, log)))\n\n   A  B      A_1       A_2      B_1      B_2\n1  1  4 1.000000 0.0000000 2.000000 1.386294\n2  2 NA 1.414214 0.6931472       NA       NA\n3  3  5 1.732051 1.0986123 2.236068 1.609438\n4 NA  6       NA        NA 2.449490 1.791759\n5  4  7 2.000000 1.3862944 2.645751 1.945910\n\n# Use across() with named new functions\ndata %&gt;%\n  mutate(\n    across(\n      everything(), \n      list(sqrt = sqrt, \n           log = log,\n           mean = \\(x) mean(x, na.rm = TRUE)\n           )\n      )\n    )\n\n   A  B   A_sqrt     A_log A_mean   B_sqrt    B_log B_mean\n1  1  4 1.000000 0.0000000    2.5 2.000000 1.386294    5.5\n2  2 NA 1.414214 0.6931472    2.5       NA       NA    5.5\n3  3  5 1.732051 1.0986123    2.5 2.236068 1.609438    5.5\n4 NA  6       NA        NA    2.5 2.449490 1.791759    5.5\n5  4  7 2.000000 1.3862944    2.5 2.645751 1.945910    5.5"
  },
  {
    "objectID": "Chapter27.html#question-3",
    "href": "Chapter27.html#question-3",
    "title": "Chapter 27",
    "section": "Question 3",
    "text": "Question 3\nAdjust expand_dates() to automatically remove the date columns after they’ve been expanded. Do you need to embrace any arguments?\nTo modify the expand_dates() function to automatically remove the date columns after they’ve been expanded, we can achieve this by adding the select() function to drop the original date columns.\nWe don’t need to embrace any additional arguments.\nHere’s the adjusted expand_dates() function:\n\nexpand_dates &lt;- function(df) {\n  df |&gt; \n    mutate(\n      across(where(is.Date), list(year = year, month = month, day = mday))\n    ) |&gt;\n    select(!where(is.Date))\n  }\n\ndf_date &lt;- tibble(\n  name = c(\"Amy\", \"Bob\", \"Charlie\", \"David\", \"Eva\"),\n  date = ymd(c(\"2009-08-03\", \"2010-01-16\", \"2012-05-20\", \"2013-11-30\", \"2015-07-12\"))\n  )\n\ndf_date |&gt; \n  expand_dates()\n\n# A tibble: 5 × 4\n  name    date_year date_month date_day\n  &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;    &lt;int&gt;\n1 Amy          2009          8        3\n2 Bob          2010          1       16\n3 Charlie      2012          5       20\n4 David        2013         11       30\n5 Eva          2015          7       12"
  },
  {
    "objectID": "Chapter27.html#question-4",
    "href": "Chapter27.html#question-4",
    "title": "Chapter 27",
    "section": "Question 4",
    "text": "Question 4\nExplain what each step of the pipeline in this function does. What special feature of where() are we taking advantage of?\nshow_missing &lt;- function(df, group_vars, summary_vars = everything()) {\n  df |&gt; \n    group_by(pick({{ group_vars }})) |&gt; \n    summarize(\n      across({{ summary_vars }}, \\(x) sum(is.na(x))),\n      .groups = \"drop\"\n    ) |&gt;\n    select(where(\\(x) any(x &gt; 0)))\n}\nnycflights13::flights |&gt; show_missing(c(year, month, day))\nThe show_missing() function appears to be designed to display the number of missing values in a data-frame.\nThe each step of the pipeline is explained as follows: –\n\ndf |&gt; group_by(pick({{ group_vars }})):\n\nThe pipe operator (|&gt;) is used to pass the dataframe df to the next operation.\ngroup_by() is applied to the dataframe, and it groups the data by the variables specified by group_vars. The group_vars parameter is enclosed in double curly braces ({{ }}), which is a tidy evaluation feature used to work with non-standard evaluation. It allows you to pass column names as unquoted arguments to the function.\n\nsummarize(across({{ summary_vars }}, \\(x) sum(is.na(x))), .groups = \"drop\"):\n\nAfter grouping the data, summarize() is used to calculate summary statistics.\nacross() is applied to the variables specified by summary_vars. Similar to group_vars, summary_vars is also enclosed in double curly braces to use tidyeval. This means you can pass a list of column names to summary_vars as unquoted arguments.\nInside the across() function, a lambda function (i.e. an anonymous function) \\(x) sum(is.na(x)) is used to count the number of missing values in each column. This is done by applying the is.na() function to each column x and then summing the logical values.\nThe .groups = \"drop\" argument is used to drop the grouping structure after summarizing. This means the resulting dataframe will not have grouped rows.\n\nselect(where(\\(x) any(x &gt; 0))):\n\nFinally, the select() function is used to choose the columns for display.\nThe where() function is applied to select columns based on a condition.\nThe condition used here is \\(x) any(x &gt; 0), which checks if there is any value greater than 0 in each column x. If there are any non-zero values in a column, it means there are missing values in that column, so it’s selected.\nThis step ensures that only columns with missing values (i.e., columns with at least one non-zero value in the summary) are included in the final output.\n\n\nSo, the special feature of where() being taken advantage of is its ability to select columns based on a condition, and in this case, it’s used to select columns with missing values for display. The function is designed to help identify which columns have missing data after grouping by specified variables.\n\n# Recreating the function\nshow_missing &lt;- function(df, group_vars, summary_vars = everything()) {\n  df |&gt; \n    group_by(pick({{ group_vars }})) |&gt; \n    summarize(\n      across({{ summary_vars }}, \\(x) sum(is.na(x))),\n      .groups = \"drop\"\n    ) |&gt;\n    select(where(\\(x) any(x &gt; 0)))\n}\n\n# Running an example\nnycflights13::flights |&gt; \n  show_missing(c(year, month, day)) |&gt;\n  slice_head(n = 5) |&gt; \n  gt() |&gt; gt_theme_538()\n\n\n\n\n\n  \n    \n    \n      year\n      month\n      day\n      dep_time\n      dep_delay\n      arr_time\n      arr_delay\n      tailnum\n      air_time\n    \n  \n  \n    2013\n1\n1\n4\n4\n5\n11\n0\n11\n    2013\n1\n2\n8\n8\n10\n15\n2\n15\n    2013\n1\n3\n10\n10\n10\n14\n2\n14\n    2013\n1\n4\n6\n6\n6\n7\n2\n7\n    2013\n1\n5\n3\n3\n3\n3\n1\n3"
  },
  {
    "objectID": "index.html#author-aditya-dahiya",
    "href": "index.html#author-aditya-dahiya",
    "title": "Solutions Manual: R for Data Science (2e)",
    "section": "",
    "text": "Welcome to the Solutions Manual for the second edition of R for Data Science by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund. This manual is your indispensable companion on the path to mastering data science with R.\nInside these pages, you’ll find a rich trove of techniques and best practices that underpin the very essence of data science. I emphasize not just the “how,” but also the “why” behind each step, ensuring a deep understanding of the principles that drive data science.\nThis website is committed to fostering a collaborative learning environment and offers the Solutions Manual for free. Happy learning!\n\n\n."
  },
  {
    "objectID": "Chapter28.html",
    "href": "Chapter28.html",
    "title": "Chapter 28",
    "section": "",
    "text": "Loading libraries\nlibrary(tidyverse)"
  },
  {
    "objectID": "Chapter28.html#question-1",
    "href": "Chapter28.html#question-1",
    "title": "Chapter 28",
    "section": "Question 1",
    "text": "Question 1\nCreate functions that take a vector as input and return:\n\nThe elements at even-numbered positions.\n\n# Creating a function called even positions\neven_positions &lt;- function(x){\n  x[seq.int(from = 0,\n            to = length(x),\n            by = 2)]\n}\n\n# Demonstration\n# Create a vector with alternating \"odd\" and \"even\" based on the sequence\nnums &lt;- rep(1:10, each = 2)\nchars &lt;- c(\"odd\", \"even\")\nx &lt;- paste0(chars, nums)\nnames(x) &lt;- 1:length(x)\nx\n\n       1        2        3        4        5        6        7        8 \n  \"odd1\"  \"even1\"   \"odd2\"  \"even2\"   \"odd3\"  \"even3\"   \"odd4\"  \"even4\" \n       9       10       11       12       13       14       15       16 \n  \"odd5\"  \"even5\"   \"odd6\"  \"even6\"   \"odd7\"  \"even7\"   \"odd8\"  \"even8\" \n      17       18       19       20 \n  \"odd9\"  \"even9\"  \"odd10\" \"even10\" \n\n# Demonstrating an example with our created function\neven_positions(x)\n\n       2        4        6        8       10       12       14       16 \n \"even1\"  \"even2\"  \"even3\"  \"even4\"  \"even5\"  \"even6\"  \"even7\"  \"even8\" \n      18       20 \n \"even9\" \"even10\" \n\n# Method 2: simpler method using TRUE and FALSE\neven_positions &lt;- function(input_vector) {\n  even_elements &lt;- input_vector[c(FALSE, TRUE)]\n  return(even_elements)\n}\n\neven_positions(x)\n\n       2        4        6        8       10       12       14       16 \n \"even1\"  \"even2\"  \"even3\"  \"even4\"  \"even5\"  \"even6\"  \"even7\"  \"even8\" \n      18       20 \n \"even9\" \"even10\" \n\n\nEvery element except the last value.\n\ndrop_last &lt;- function(x){\n  return(x[-length(x)])\n}\n\n# Demonstrating an example with our created function\nx &lt;- letters[1:5]\ndrop_last(x)\n\n[1] \"a\" \"b\" \"c\" \"d\"\n\n\nOnly even values (and no missing values).\n\ndisplay_even &lt;- function(x){\n  return(x[(x %% 2 == 0) & (!is.na(x))])\n}\n\n# Demonstrating an example with our created function\nx &lt;- sample(c(1:10, NA), size = 10, replace = FALSE)\nx\n\n [1]  3  6 10  7  1  2  4  9  8 NA\n\ndisplay_even(x)\n\n[1]  6 10  2  4  8\n\n# Better function with an error message facility\ndisplay_even &lt;- function(input_vector) {\n  if (is.vector(input_vector) && is.numeric(input_vector)) {\n    even_values &lt;- input_vector[!is.na(input_vector) & input_vector %% 2 == 0]\n    return(even_values)\n  } else {\n    stop(\"Input must be a numeric vector.\")\n  }\n}\n\n# Example usage:\ndisplay_even(x)\n\n[1]  6 10  2  4  8"
  },
  {
    "objectID": "Chapter28.html#question-2",
    "href": "Chapter28.html#question-2",
    "title": "Chapter 28",
    "section": "Question 2",
    "text": "Question 2\nWhy is x[-which(x &gt; 0)] not the same as x[x &lt;= 0]? Read the documentation for which() and do some experiments to figure it out.\nIn R, x[-which(x &gt; 0)] and x[x &lt;= 0] are not exactly the same because they handle missing values (NAs) and non-real numbers (NaNs) differently. The difference arises from how the which() function handles missing values.\n\nThe which() function returns the indices of elements that are TRUE. When applied wiht !, it will return all those elements that did not fulfil that logical condition. Thus, it returns the indices of elements that do not satisfy that condition.\nThe [x &lt;= 0] operator also finds similar answer. However, note the important difference shown in the code below, it converts NaN comparison with zero into NAs.\n\nHere’s an example to illustrate the difference:\n\n# Example vector of +ve, -ve, NA and NaN values\nx &lt;- c(NA, NaN, -Inf, -5:5, Inf, NaN, NA)\nx\n\n [1]   NA  NaN -Inf   -5   -4   -3   -2   -1    0    1    2    3    4    5  Inf\n[16]  NaN   NA\n\n# which() finds all the conditions that are true\nx[which(x &gt; 0)]\n\n[1]   1   2   3   4   5 Inf\n\n# Thus, -which() finds all the elements that are not present above\nx[-which(x &gt; 0)]\n\n [1]   NA  NaN -Inf   -5   -4   -3   -2   -1    0  NaN   NA\n\n# On the other hand, logical condition also finds similar answer\n# however, note the important difference, it converts NaN comparison\n# with zero into NAs\nx[x &lt;= 0]\n\n [1]   NA   NA -Inf   -5   -4   -3   -2   -1    0   NA   NA"
  },
  {
    "objectID": "Chapter28.html#question-1-1",
    "href": "Chapter28.html#question-1-1",
    "title": "Chapter 28",
    "section": "Question 1",
    "text": "Question 1\nWhat happens when you use [[ with a positive integer that’s bigger than the length of the vector? What happens when you subset with a name that doesn’t exist?\nWhen we use double square brackets [[ with a positive integer that’s bigger than the length of the vector, or when we subset with a name that doesn’t exist, we’ll get different outcomes depending on the situation:\n\nUsing [[ with an index larger than the vector length: If we use [[ with an index that is greater than the length of the vector, R will return an error. This is because we are trying to access an element that is out of the vector’s bounds, which is not allowed in R.\nExample:\n\nx &lt;- c(1, 2, 3, 4, 5)\nx\n\n[1] 1 2 3 4 5\n\nx[[6]]\n\nError in x[[6]]: subscript out of bounds\n\n\nSubsetting with a name that doesn’t exist: When we use [[ with a name that doesn’t exist in the list or data frame, R will return NULL. It doesn’t throw an error, but it tells us that the name we specified doesn’t match any existing names in the list or data frame.\nExample:\n\nmy_list &lt;- list(a = 1, b = 2, c = 3)\nmy_list\n\n$a\n[1] 1\n\n$b\n[1] 2\n\n$c\n[1] 3\n\nmy_list[[\"d\"]]\n\nNULL\n\n\n\nIn summary, attempting to access an index or name that is out of bounds or doesn’t exist will either result in an error (for index out of bounds) or a NULL value (for nonexistent names), depending on the specific situation."
  },
  {
    "objectID": "Chapter28.html#question-2-1",
    "href": "Chapter28.html#question-2-1",
    "title": "Chapter 28",
    "section": "Question 2",
    "text": "Question 2\nWhat would pepper[[1]][1] be? What about pepper[[1]][[1]]?\nIn R, when we are working with lists and we want to subset elements, the use of single square brackets [] and double square brackets [[]] has different implications:\n\nSingle Square Brackets []: When we use single square brackets to subset a list, we get a new list containing the selected elements. The result is always a list, regardless of how many elements you select.\nDouble Square Brackets [[]]: Double square brackets, on the other hand, are used to directly access the individual elements within the list. When we use [[]], we extract the element itself rather than a list containing that element.\n\n\n# Example list\nlist1 &lt;- list(\n  a = 1:10,\n  b = letters[1:5],\n  c = list(\n    c1  = 11:15,\n    c2 = letters[6:10]\n  )\n)\n# Printing the list\nlist1\n\n$a\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$b\n[1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\n$c\n$c$c1\n[1] 11 12 13 14 15\n\n$c$c2\n[1] \"f\" \"g\" \"h\" \"i\" \"j\"\n\n# [[ ]] pulls out a vector, i.e. it produces element of the class of first element of the list\nlist1[[1]]\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nlist1[[1]] |&gt; class()\n\n[1] \"integer\"\n\n# [1] pulls out a list that only contains the first element\nlist1[1]\n\n$a\n [1]  1  2  3  4  5  6  7  8  9 10\n\nlist1[1] |&gt; class()\n\n[1] \"list\"\n\n\nThus, pepper[[1]][1] will access the first grain of pepper inside the satchet of pepper. The grain of pepper will still be inside the sachet of pepper.\nOn the other hand, pepper[[1]][[1]] will be the first grain of pepper. This grain of pepper will be lying on the table, not inside the pepper sachet. Thus, the grain of pepper will be accessed directly."
  }
]